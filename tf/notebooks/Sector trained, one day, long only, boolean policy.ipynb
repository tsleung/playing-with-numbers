{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "\n",
    "Creates a neural network which evaluates a time series and produce a set of predicted values for the time series\n",
    "Predicted values may be used in a policy to make a trade. This policy may be modeled by simple multiple regression or a neural network.\n",
    "\n",
    "## Data\n",
    "Test data is taken as most recent to avoid lookahead bias. Train data is split into a validation and training set during fitting.\n",
    "\n",
    "\n",
    "## TODO\n",
    "- Convert feature percentages to stdev\n",
    "- Adding VIX as a signal\n",
    "- Adding High/Low as signals\n",
    "- Multiple securities/ aggregate samples\n",
    "- Policy network\n",
    "- Regularization (l2)\n",
    "- Dilated convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "NUM_INPUT_NEURONS = 64\n",
    "NUM_OUTPUT_NEURONS = 1\n",
    "DAY_OFFSET = 5\n",
    "\n",
    "def from_network(symbol):\n",
    "    return pdr.get_data_yahoo(symbols=symbol, start=datetime(1900, 1, 1))\n",
    "\n",
    "def from_file(symbol):\n",
    "    dataset_path = keras.utils.get_file(\"{}.csv\".format(symbol), \"http://localhost:8000/data/daily/{}.csv\".format(symbol))\n",
    "    column_names = ['Date','Open','High','Low','Close','Adj Close','Volume'] \n",
    "    return pd.read_csv(dataset_path, \n",
    "                              names=column_names, \n",
    "                              dtype={'Close': np.float64,'Open': np.float64,'High': np.float64,'Adj Close': np.float64, 'Volume': np.float64},\n",
    "                              header=0,\n",
    "                              na_values = \"?\", \n",
    "                              comment='\\t',\n",
    "                              sep=\",\",\n",
    "                              skipinitialspace=True)\n",
    "\n",
    "\n",
    "# Create features (only close price for now)\n",
    "def convert_to_percentage(old, new):\n",
    "    return (old - new) / old\n",
    "\n",
    "def convert_labels_to_category(labels): \n",
    "    # Simplification - If positive return, 1, else 0\n",
    "    # return map(lambda arr: 1 if arr[0] > 1 else 0, labels)\n",
    "    # rounding simpliciation\n",
    "    return map(lambda arr: map(lambda val: round(val,4),arr), labels)\n",
    "\n",
    "def convert_to_train(raw_dataset, offset=5):\n",
    "    dataset = raw_dataset.copy()\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(offset, len(dataset) - NUM_INPUT_NEURONS):\n",
    "\n",
    "        feature_dataset = dataset[i:i+NUM_INPUT_NEURONS].copy()\n",
    "        latest_close = feature_dataset['Close'].iloc[0]\n",
    "        \n",
    "        features.append(\n",
    "            feature_dataset['Close']\n",
    "                .map(lambda current: convert_to_percentage(latest_close, current))\n",
    "                .tolist()\n",
    "        )\n",
    "        labels.append([\n",
    "            dataset['Close'].iloc[i-1] / latest_close, # 1 day trade\n",
    "#             dataset['Close'].iloc[i-2] / latest_close, # 2 day trade\n",
    "#             dataset['Close'].iloc[i-3] / latest_close, # 3 day trade\n",
    "#             dataset['Close'].iloc[i-4] / latest_close, # 4 day trade\n",
    "#             dataset['Close'].iloc[i-5] / latest_close, # 5 day trade\n",
    "        ])\n",
    "        \n",
    "    # Without converting labels the precision is hard to determine accuracy. \n",
    "    # Rather than crude 0/1, maybe this can be more sophisticated\n",
    "    labels = convert_labels_to_category(labels)\n",
    "    \n",
    "    return [features,labels]\n",
    "def split_data(symbol):\n",
    "    fetched = from_network(symbol).sort_values(by=['Date'],ascending=False)\n",
    "    converted = convert_to_train(fetched)\n",
    "    features = converted[0]\n",
    "    labels = converted[1]\n",
    "    training = [ # since we decide a model, use all data for training\n",
    "        features[1:],\n",
    "        labels[1:]\n",
    "    ]\n",
    "    validation = [\n",
    "        features[5:1000],\n",
    "        labels[5:1000]\n",
    "    ]\n",
    "    prediction = [\n",
    "        features[:5],\n",
    "        labels[:5],\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'prediction': prediction,\n",
    "        'validation': validation,\n",
    "        'training': training,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 48)                3120      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 24)                888       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 14,405\n",
      "Trainable params: 14,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.nn.relu, input_shape=[NUM_INPUT_NEURONS]),\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dense(48, activation=tf.nn.relu),\n",
    "    layers.Dense(36, activation=tf.nn.relu),\n",
    "    layers.Dense(24, activation=tf.nn.relu),\n",
    "    layers.Dense(12, activation=tf.nn.relu),\n",
    "\n",
    "      \n",
    "#     layers.Dense(64, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(48, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(36, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(24, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(12, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "    layers.Dense(NUM_OUTPUT_NEURONS)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_squared_logarithmic_error',\n",
    "                optimizer='sgd',\n",
    "#                 metrics=[\n",
    "#                     'mae',\n",
    "#                 ]\n",
    "               )\n",
    "  return model\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset = raw_dataset.copy()\n",
    "#dataset = from_network('SPY').sort_values(by=['Date'],ascending=False)\n",
    "# add function to cache fetch\n",
    "QQQ = from_network('QQQ')\n",
    "SPY = from_network('SPY')\n",
    "XLK = from_network('XLK')\n",
    "XLF = from_network('XLF')\n",
    "XLE = from_network('XLE')\n",
    "XLP = from_network('XLP')\n",
    "XLV = from_network('XLV')\n",
    "XLY = from_network('XLY')\n",
    "XLI = from_network('XLI')\n",
    "XLU = from_network('XLU')\n",
    "\n",
    "#dataset = pd.concat([QQQ,SPY,XLK,XLF,XLE,XLP,XLV,XLY,XLI,XLU]).sort_values(by=['Date'],ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepped_data = map(split_data, [\n",
    "    'QQQ',\n",
    "    'SPY',\n",
    "    'IWM',\n",
    "    'XLK',\n",
    "    'XLF',\n",
    "    'XLE',\n",
    "    'XLP',\n",
    "    'XLV',\n",
    "    'XLY',\n",
    "    'XLI',\n",
    "    'XLU',\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQQ\n",
      "SPY\n",
      "IWM\n",
      "XLK\n",
      "XLF\n",
      "XLE\n",
      "XLP\n",
      "XLV\n",
      "XLY\n",
      "XLI\n",
      "XLU\n",
      "0\n",
      "4933\n",
      "11408\n",
      "16033\n",
      "21018\n",
      "26003\n",
      "30988\n",
      "35973\n",
      "40958\n",
      "45943\n",
      "50928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(prepped_data)):\n",
    "    print prepped_data[i]['symbol']\n",
    "    \n",
    "def combine_all(accum, prep):\n",
    "    \n",
    "    print len(accum['training'][0])\n",
    "    return {   \n",
    "        'prediction':[\n",
    "            accum['prediction'][0] + prep['prediction'][0],\n",
    "            accum['prediction'][1] + prep['prediction'][1],\n",
    "        ],\n",
    "        'validation':[\n",
    "            accum['validation'][0] + prep['validation'][0],\n",
    "            accum['validation'][1] + prep['validation'][1],\n",
    "        ],\n",
    "        'training':[\n",
    "            accum['training'][0] + prep['training'][0],\n",
    "            accum['training'][1] + prep['training'][1],\n",
    "        ],\n",
    "    }\n",
    "combined = reduce(combine_all, prepped_data,{\n",
    "    'prediction':[[],[]],\n",
    "    'validation':[[],[]],\n",
    "    'training':[[],[]],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55913"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "len(combined['training'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55913\n",
      "10945\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "#len(converted_feature_set[0][0])\n",
    "print len(combined['training'][0])\n",
    "train_data = np.array(combined['training'][0])\n",
    "train_labels = np.array(combined['training'][1])\n",
    "\n",
    "print len(combined['validation'][0])\n",
    "test_data = np.array(combined['validation'][0])\n",
    "test_labels = np.array(combined['validation'][1])\n",
    "\n",
    "print len(combined['prediction'][0])\n",
    "prediction_data = np.array(combined['prediction'][0])\n",
    "prediction_labels = np.array(combined['prediction'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.00018478  0.01897137 ... -0.09202332 -0.09159221\n",
      "  -0.06073299]\n",
      " [ 0.          0.01915261  0.0104077  ... -0.09139054 -0.06053702\n",
      "  -0.07353124]\n",
      " [ 0.         -0.00891567 -0.01262004 ... -0.0812457  -0.09449364\n",
      "  -0.06485843]\n",
      " ...\n",
      " [ 0.         -0.01635514 -0.02161122 ... -0.13434394 -0.14368973\n",
      "  -0.12499815]\n",
      " [ 0.         -0.0051715  -0.0183908  ... -0.12528553 -0.10689473\n",
      "  -0.11264368]\n",
      " [ 0.         -0.01315129 -0.01658001 ... -0.10119987 -0.10691924\n",
      "  -0.08690688]]\n",
      "[[1.0079]\n",
      " [0.9998]\n",
      " [1.0195]\n",
      " ...\n",
      " [0.9977]\n",
      " [0.9839]\n",
      " [0.9949]]\n"
     ]
    }
   ],
   "source": [
    "print train_data\n",
    "print train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55633 samples, validate on 280 samples\n",
      "Epoch 1/5\n",
      "55633/55633 [==============================] - 3s 53us/step - loss: 0.0097 - val_loss: 4.8635e-05\n",
      "Epoch 2/5\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 6.0254e-05 - val_loss: 4.6718e-05\n",
      "Epoch 3/5\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.8243e-05 - val_loss: 4.6286e-05\n",
      "Epoch 4/5\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.7259e-05 - val_loss: 4.6038e-05\n",
      "Epoch 5/5\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.6572e-05 - val_loss: 4.5544e-05\n",
      "Train on 55633 samples, validate on 280 samples\n",
      "Epoch 1/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.6010e-05 - val_loss: 4.5266e-05\n",
      "Epoch 2/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.5537e-05 - val_loss: 4.5018e-05\n",
      "Epoch 3/400\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.5172e-05 - val_loss: 4.4839e-05\n",
      "Epoch 4/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.4901e-05 - val_loss: 4.4751e-05\n",
      "Epoch 5/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.4697e-05 - val_loss: 4.4683e-05\n",
      "Epoch 6/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.4535e-05 - val_loss: 4.4596e-05\n",
      "Epoch 7/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.4413e-05 - val_loss: 4.4552e-05\n",
      "Epoch 8/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.4299e-05 - val_loss: 4.4510e-05\n",
      "Epoch 9/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.4213e-05 - val_loss: 4.4414e-05\n",
      "Epoch 10/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.4122e-05 - val_loss: 4.4370e-05\n",
      "Epoch 11/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.4047e-05 - val_loss: 4.4329e-05\n",
      "Epoch 12/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.3976e-05 - val_loss: 4.4296e-05\n",
      "Epoch 13/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.3908e-05 - val_loss: 4.4252e-05\n",
      "Epoch 14/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.3848e-05 - val_loss: 4.4240e-05\n",
      "Epoch 15/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.3785e-05 - val_loss: 4.4216e-05\n",
      "Epoch 16/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.3726e-05 - val_loss: 4.4147e-05\n",
      "Epoch 17/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.3678e-05 - val_loss: 4.4116e-05\n",
      "Epoch 18/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.3622e-05 - val_loss: 4.4097e-05\n",
      "Epoch 19/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.3565e-05 - val_loss: 4.4065e-05\n",
      "Epoch 20/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.3522e-05 - val_loss: 4.4052e-05\n",
      "Epoch 21/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.3472e-05 - val_loss: 4.4050e-05\n",
      "Epoch 22/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.3423e-05 - val_loss: 4.4034e-05\n",
      "Epoch 23/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.3381e-05 - val_loss: 4.3979e-05\n",
      "Epoch 24/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.3341e-05 - val_loss: 4.3960e-05\n",
      "Epoch 25/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.3299e-05 - val_loss: 4.3945e-05\n",
      "Epoch 26/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.3256e-05 - val_loss: 4.3943e-05\n",
      "Epoch 27/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.3213e-05 - val_loss: 4.3923e-05\n",
      "Epoch 28/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.3179e-05 - val_loss: 4.3914e-05\n",
      "Epoch 29/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.3141e-05 - val_loss: 4.3896e-05\n",
      "Epoch 30/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.3104e-05 - val_loss: 4.3875e-05\n",
      "Epoch 31/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.3071e-05 - val_loss: 4.3864e-05\n",
      "Epoch 32/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.3036e-05 - val_loss: 4.3853e-05\n",
      "Epoch 33/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.3002e-05 - val_loss: 4.3853e-05\n",
      "Epoch 34/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.2972e-05 - val_loss: 4.3831e-05\n",
      "Epoch 35/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2948e-05 - val_loss: 4.3832e-05\n",
      "Epoch 36/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.2916e-05 - val_loss: 4.3826e-05\n",
      "Epoch 37/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2891e-05 - val_loss: 4.3816e-05\n",
      "Epoch 38/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2859e-05 - val_loss: 4.3809e-05\n",
      "Epoch 39/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.2829e-05 - val_loss: 4.3803e-05\n",
      "Epoch 40/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.2801e-05 - val_loss: 4.3799e-05\n",
      "Epoch 41/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2781e-05 - val_loss: 4.3797e-05\n",
      "Epoch 42/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.2755e-05 - val_loss: 4.3796e-05\n",
      "Epoch 43/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.2735e-05 - val_loss: 4.3786e-05\n",
      "Epoch 44/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.2713e-05 - val_loss: 4.3780e-05\n",
      "Epoch 45/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2687e-05 - val_loss: 4.3780e-05\n",
      "Epoch 46/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2667e-05 - val_loss: 4.3779e-05\n",
      "Epoch 47/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2643e-05 - val_loss: 4.3793e-05\n",
      "Epoch 48/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.2622e-05 - val_loss: 4.3765e-05\n",
      "Epoch 49/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2604e-05 - val_loss: 4.3759e-05\n",
      "Epoch 50/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.2579e-05 - val_loss: 4.3757e-05\n",
      "Epoch 51/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2560e-05 - val_loss: 4.3780e-05\n",
      "Epoch 52/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2542e-05 - val_loss: 4.3777e-05\n",
      "Epoch 53/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2518e-05 - val_loss: 4.3764e-05\n",
      "Epoch 54/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2503e-05 - val_loss: 4.3777e-05\n",
      "Epoch 55/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.2486e-05 - val_loss: 4.3761e-05\n",
      "Epoch 56/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2469e-05 - val_loss: 4.3758e-05\n",
      "Epoch 57/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2451e-05 - val_loss: 4.3770e-05\n",
      "Epoch 58/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.2436e-05 - val_loss: 4.3760e-05\n",
      "Epoch 59/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2418e-05 - val_loss: 4.3774e-05\n",
      "Epoch 60/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2397e-05 - val_loss: 4.3768e-05\n",
      "Epoch 61/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.2385e-05 - val_loss: 4.3789e-05\n",
      "Epoch 62/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.2367e-05 - val_loss: 4.3777e-05\n",
      "Epoch 63/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.2349e-05 - val_loss: 4.3781e-05\n",
      "Epoch 64/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2340e-05 - val_loss: 4.3778e-05\n",
      "Epoch 65/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2322e-05 - val_loss: 4.3779e-05\n",
      "Epoch 66/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2307e-05 - val_loss: 4.3789e-05\n",
      "Epoch 67/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2292e-05 - val_loss: 4.3815e-05\n",
      "Epoch 68/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.2280e-05 - val_loss: 4.3782e-05\n",
      "Epoch 69/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2267e-05 - val_loss: 4.3792e-05\n",
      "Epoch 70/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.2253e-05 - val_loss: 4.3782e-05\n",
      "Epoch 71/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.2234e-05 - val_loss: 4.3796e-05\n",
      "Epoch 72/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2224e-05 - val_loss: 4.3792e-05\n",
      "Epoch 73/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2211e-05 - val_loss: 4.3805e-05\n",
      "Epoch 74/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2201e-05 - val_loss: 4.3817e-05\n",
      "Epoch 75/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2181e-05 - val_loss: 4.3896e-05\n",
      "Epoch 76/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2168e-05 - val_loss: 4.3796e-05\n",
      "Epoch 77/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2162e-05 - val_loss: 4.3814e-05\n",
      "Epoch 78/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.2153e-05 - val_loss: 4.3796e-05A: 0s - loss:\n",
      "Epoch 79/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2141e-05 - val_loss: 4.3793e-05\n",
      "Epoch 80/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.2128e-05 - val_loss: 4.3810e-05\n",
      "Epoch 81/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2117e-05 - val_loss: 4.3805e-05\n",
      "Epoch 82/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.2110e-05 - val_loss: 4.3801e-05\n",
      "Epoch 83/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.2099e-05 - val_loss: 4.3808e-05\n",
      "Epoch 84/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2088e-05 - val_loss: 4.3814e-05\n",
      "Epoch 85/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2075e-05 - val_loss: 4.3805e-05\n",
      "Epoch 86/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.2063e-05 - val_loss: 4.3818e-05\n",
      "Epoch 87/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.2057e-05 - val_loss: 4.3822e-05\n",
      "Epoch 88/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2041e-05 - val_loss: 4.3811e-05\n",
      "Epoch 89/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2036e-05 - val_loss: 4.3808e-05\n",
      "Epoch 90/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.2024e-05 - val_loss: 4.3836e-05\n",
      "Epoch 91/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.2015e-05 - val_loss: 4.3834e-05\n",
      "Epoch 92/400\n",
      "55633/55633 [==============================] - 3s 54us/step - loss: 5.2004e-05 - val_loss: 4.3809e-05\n",
      "Epoch 93/400\n",
      "55633/55633 [==============================] - 3s 56us/step - loss: 5.1997e-05 - val_loss: 4.3813e-05\n",
      "Epoch 94/400\n",
      "55633/55633 [==============================] - 3s 53us/step - loss: 5.1983e-05 - val_loss: 4.3809e-05\n",
      "Epoch 95/400\n",
      "55633/55633 [==============================] - 4s 67us/step - loss: 5.1975e-05 - val_loss: 4.3809e-05\n",
      "Epoch 96/400\n",
      "55633/55633 [==============================] - 4s 75us/step - loss: 5.1963e-05 - val_loss: 4.3807e-05\n",
      "Epoch 97/400\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.1953e-05 - val_loss: 4.3828e-05\n",
      "Epoch 98/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1948e-05 - val_loss: 4.3803e-05\n",
      "Epoch 99/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1937e-05 - val_loss: 4.3806e-05\n",
      "Epoch 100/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1927e-05 - val_loss: 4.3809e-05\n",
      "Epoch 101/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1917e-05 - val_loss: 4.3814e-05\n",
      "Epoch 102/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1910e-05 - val_loss: 4.3815e-05\n",
      "Epoch 103/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1900e-05 - val_loss: 4.3807e-05\n",
      "Epoch 104/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1888e-05 - val_loss: 4.3811e-05\n",
      "Epoch 105/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1880e-05 - val_loss: 4.3808e-05\n",
      "Epoch 106/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1872e-05 - val_loss: 4.3806e-05\n",
      "Epoch 107/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1865e-05 - val_loss: 4.3802e-05\n",
      "Epoch 108/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1856e-05 - val_loss: 4.3824e-05\n",
      "Epoch 109/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1846e-05 - val_loss: 4.3799e-05\n",
      "Epoch 110/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1837e-05 - val_loss: 4.3798e-05\n",
      "Epoch 111/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1830e-05 - val_loss: 4.3803e-05\n",
      "Epoch 112/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1824e-05 - val_loss: 4.3806e-05\n",
      "Epoch 113/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1812e-05 - val_loss: 4.3816e-05\n",
      "Epoch 114/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1809e-05 - val_loss: 4.3839e-05\n",
      "Epoch 115/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1800e-05 - val_loss: 4.3806e-05\n",
      "Epoch 116/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1793e-05 - val_loss: 4.3805e-05\n",
      "Epoch 117/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1780e-05 - val_loss: 4.3811e-05\n",
      "Epoch 118/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1776e-05 - val_loss: 4.3807e-05\n",
      "Epoch 119/400\n",
      "55633/55633 [==============================] - 3s 49us/step - loss: 5.1768e-05 - val_loss: 4.3808e-05\n",
      "Epoch 120/400\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.1763e-05 - val_loss: 4.3813e-05\n",
      "Epoch 121/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1756e-05 - val_loss: 4.3823e-05\n",
      "Epoch 122/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1747e-05 - val_loss: 4.3850e-05\n",
      "Epoch 123/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1742e-05 - val_loss: 4.3823e-05\n",
      "Epoch 124/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1724e-05 - val_loss: 4.3828e-05\n",
      "Epoch 125/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1724e-05 - val_loss: 4.3816e-05\n",
      "Epoch 126/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1718e-05 - val_loss: 4.3849e-05\n",
      "Epoch 127/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1712e-05 - val_loss: 4.3832e-05\n",
      "Epoch 128/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1708e-05 - val_loss: 4.3852e-05\n",
      "Epoch 129/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1699e-05 - val_loss: 4.3856e-05\n",
      "Epoch 130/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1696e-05 - val_loss: 4.3869e-05\n",
      "Epoch 131/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1687e-05 - val_loss: 4.3870e-05\n",
      "Epoch 132/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1678e-05 - val_loss: 4.3877e-05\n",
      "Epoch 133/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1674e-05 - val_loss: 4.3862e-05\n",
      "Epoch 134/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1668e-05 - val_loss: 4.3871e-05\n",
      "Epoch 135/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1660e-05 - val_loss: 4.3875e-05\n",
      "Epoch 136/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1656e-05 - val_loss: 4.3866e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1649e-05 - val_loss: 4.3877e-05\n",
      "Epoch 138/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1644e-05 - val_loss: 4.3886e-05\n",
      "Epoch 139/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1639e-05 - val_loss: 4.3886e-05\n",
      "Epoch 140/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1633e-05 - val_loss: 4.3908e-05\n",
      "Epoch 141/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1625e-05 - val_loss: 4.3903e-05\n",
      "Epoch 142/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1620e-05 - val_loss: 4.3904e-05\n",
      "Epoch 143/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1614e-05 - val_loss: 4.3924e-05\n",
      "Epoch 144/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1610e-05 - val_loss: 4.3907e-05\n",
      "Epoch 145/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1604e-05 - val_loss: 4.3920e-050s - \n",
      "Epoch 146/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1595e-05 - val_loss: 4.3909e-05\n",
      "Epoch 147/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1593e-05 - val_loss: 4.3914e-05\n",
      "Epoch 148/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1588e-05 - val_loss: 4.3927e-05\n",
      "Epoch 149/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1578e-05 - val_loss: 4.3927e-05\n",
      "Epoch 150/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1577e-05 - val_loss: 4.3946e-05\n",
      "Epoch 151/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1574e-05 - val_loss: 4.3941e-05\n",
      "Epoch 152/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1566e-05 - val_loss: 4.3945e-05\n",
      "Epoch 153/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1556e-05 - val_loss: 4.3949e-05\n",
      "Epoch 154/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1558e-05 - val_loss: 4.3959e-05\n",
      "Epoch 155/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1548e-05 - val_loss: 4.3958e-05\n",
      "Epoch 156/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1541e-05 - val_loss: 4.3964e-05\n",
      "Epoch 157/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1538e-05 - val_loss: 4.3972e-05\n",
      "Epoch 158/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1534e-05 - val_loss: 4.3985e-05\n",
      "Epoch 159/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1530e-05 - val_loss: 4.3987e-05\n",
      "Epoch 160/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1525e-05 - val_loss: 4.3997e-05\n",
      "Epoch 161/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1510e-05 - val_loss: 4.4000e-05\n",
      "Epoch 162/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1515e-05 - val_loss: 4.3994e-05\n",
      "Epoch 163/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1511e-05 - val_loss: 4.4003e-05\n",
      "Epoch 164/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1507e-05 - val_loss: 4.4018e-05\n",
      "Epoch 165/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1502e-05 - val_loss: 4.4019e-05\n",
      "Epoch 166/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1497e-05 - val_loss: 4.4020e-05\n",
      "Epoch 167/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1495e-05 - val_loss: 4.4025e-05\n",
      "Epoch 168/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1488e-05 - val_loss: 4.4048e-05\n",
      "Epoch 169/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1485e-05 - val_loss: 4.4031e-05\n",
      "Epoch 170/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1479e-05 - val_loss: 4.4028e-05\n",
      "Epoch 171/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1475e-05 - val_loss: 4.4045e-05\n",
      "Epoch 172/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1468e-05 - val_loss: 4.4034e-05\n",
      "Epoch 173/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1465e-05 - val_loss: 4.4033e-05\n",
      "Epoch 174/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1464e-05 - val_loss: 4.4050e-05\n",
      "Epoch 175/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1453e-05 - val_loss: 4.4041e-05\n",
      "Epoch 176/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1452e-05 - val_loss: 4.4061e-05\n",
      "Epoch 177/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1451e-05 - val_loss: 4.4037e-05\n",
      "Epoch 178/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1447e-05 - val_loss: 4.4038e-05\n",
      "Epoch 179/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1437e-05 - val_loss: 4.4092e-05\n",
      "Epoch 180/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1437e-05 - val_loss: 4.4049e-05\n",
      "Epoch 181/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1437e-05 - val_loss: 4.4052e-05\n",
      "Epoch 182/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1424e-05 - val_loss: 4.4082e-05\n",
      "Epoch 183/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1426e-05 - val_loss: 4.4055e-05\n",
      "Epoch 184/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1418e-05 - val_loss: 4.4072e-05\n",
      "Epoch 185/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1416e-05 - val_loss: 4.4074e-05\n",
      "Epoch 186/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1416e-05 - val_loss: 4.4072e-05\n",
      "Epoch 187/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1410e-05 - val_loss: 4.4068e-05\n",
      "Epoch 188/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1408e-05 - val_loss: 4.4071e-05\n",
      "Epoch 189/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1396e-05 - val_loss: 4.4087e-05\n",
      "Epoch 190/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1398e-05 - val_loss: 4.4073e-05\n",
      "Epoch 191/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1394e-05 - val_loss: 4.4080e-05\n",
      "Epoch 192/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1391e-05 - val_loss: 4.4069e-05\n",
      "Epoch 193/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1390e-05 - val_loss: 4.4073e-05\n",
      "Epoch 194/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1382e-05 - val_loss: 4.4079e-05\n",
      "Epoch 195/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1372e-05 - val_loss: 4.4081e-05\n",
      "Epoch 196/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1378e-05 - val_loss: 4.4076e-05\n",
      "Epoch 197/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1377e-05 - val_loss: 4.4070e-05\n",
      "Epoch 198/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1369e-05 - val_loss: 4.4076e-05\n",
      "Epoch 199/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1368e-05 - val_loss: 4.4090e-05\n",
      "Epoch 200/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1358e-05 - val_loss: 4.4157e-05\n",
      "Epoch 201/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1362e-05 - val_loss: 4.4079e-05\n",
      "Epoch 202/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1358e-05 - val_loss: 4.4110e-05\n",
      "Epoch 203/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1356e-05 - val_loss: 4.4077e-05\n",
      "Epoch 204/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1351e-05 - val_loss: 4.4083e-05\n",
      "Epoch 205/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1350e-05 - val_loss: 4.4078e-05\n",
      "Epoch 206/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1344e-05 - val_loss: 4.4075e-05\n",
      "Epoch 207/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1344e-05 - val_loss: 4.4072e-05\n",
      "Epoch 208/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1340e-05 - val_loss: 4.4076e-05\n",
      "Epoch 209/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1333e-05 - val_loss: 4.4077e-05\n",
      "Epoch 210/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1329e-05 - val_loss: 4.4091e-05\n",
      "Epoch 211/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1330e-05 - val_loss: 4.4079e-05\n",
      "Epoch 212/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1324e-05 - val_loss: 4.4079e-05\n",
      "Epoch 213/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1324e-05 - val_loss: 4.4074e-05\n",
      "Epoch 214/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1317e-05 - val_loss: 4.4068e-05\n",
      "Epoch 215/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1316e-05 - val_loss: 4.4088e-05\n",
      "Epoch 216/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1316e-05 - val_loss: 4.4074e-05\n",
      "Epoch 217/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1314e-05 - val_loss: 4.4068e-05\n",
      "Epoch 218/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1311e-05 - val_loss: 4.4068e-05\n",
      "Epoch 219/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1304e-05 - val_loss: 4.4098e-05\n",
      "Epoch 220/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1304e-05 - val_loss: 4.4070e-05\n",
      "Epoch 221/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1300e-05 - val_loss: 4.4083e-05\n",
      "Epoch 222/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1299e-05 - val_loss: 4.4068e-05\n",
      "Epoch 223/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1295e-05 - val_loss: 4.4066e-05\n",
      "Epoch 224/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1288e-05 - val_loss: 4.4082e-05\n",
      "Epoch 225/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1289e-05 - val_loss: 4.4076e-05\n",
      "Epoch 226/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1285e-05 - val_loss: 4.4073e-05\n",
      "Epoch 227/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1279e-05 - val_loss: 4.4068e-05\n",
      "Epoch 228/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1277e-05 - val_loss: 4.4075e-05\n",
      "Epoch 229/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1277e-05 - val_loss: 4.4067e-05\n",
      "Epoch 230/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1275e-05 - val_loss: 4.4067e-05\n",
      "Epoch 231/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1274e-05 - val_loss: 4.4092e-05\n",
      "Epoch 232/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1271e-05 - val_loss: 4.4075e-05\n",
      "Epoch 233/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1264e-05 - val_loss: 4.4070e-05\n",
      "Epoch 234/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1264e-05 - val_loss: 4.4077e-05\n",
      "Epoch 235/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1260e-05 - val_loss: 4.4080e-05\n",
      "Epoch 236/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1259e-05 - val_loss: 4.4078e-05\n",
      "Epoch 237/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1251e-05 - val_loss: 4.4079e-05\n",
      "Epoch 238/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1250e-05 - val_loss: 4.4090e-05\n",
      "Epoch 239/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1248e-05 - val_loss: 4.4075e-05\n",
      "Epoch 240/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1249e-05 - val_loss: 4.4074e-05\n",
      "Epoch 241/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1243e-05 - val_loss: 4.4074e-05\n",
      "Epoch 242/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1237e-05 - val_loss: 4.4080e-05\n",
      "Epoch 243/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1235e-05 - val_loss: 4.4074e-05\n",
      "Epoch 244/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1238e-05 - val_loss: 4.4076e-05\n",
      "Epoch 245/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1230e-05 - val_loss: 4.4080e-05\n",
      "Epoch 246/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1232e-05 - val_loss: 4.4073e-05\n",
      "Epoch 247/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1230e-05 - val_loss: 4.4070e-05\n",
      "Epoch 248/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1226e-05 - val_loss: 4.4075e-05\n",
      "Epoch 249/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1226e-05 - val_loss: 4.4068e-05\n",
      "Epoch 250/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1222e-05 - val_loss: 4.4066e-05\n",
      "Epoch 251/400\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.1216e-05 - val_loss: 4.4065e-05\n",
      "Epoch 252/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1215e-05 - val_loss: 4.4064e-05\n",
      "Epoch 253/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1215e-05 - val_loss: 4.4064e-05\n",
      "Epoch 254/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1211e-05 - val_loss: 4.4064e-05\n",
      "Epoch 255/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1211e-05 - val_loss: 4.4068e-05\n",
      "Epoch 256/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1206e-05 - val_loss: 4.4072e-05\n",
      "Epoch 257/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1199e-05 - val_loss: 4.4060e-05\n",
      "Epoch 258/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1202e-05 - val_loss: 4.4062e-05\n",
      "Epoch 259/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1197e-05 - val_loss: 4.4068e-05\n",
      "Epoch 260/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1196e-05 - val_loss: 4.4064e-05\n",
      "Epoch 261/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1193e-05 - val_loss: 4.4068e-05\n",
      "Epoch 262/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1193e-05 - val_loss: 4.4060e-05\n",
      "Epoch 263/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1189e-05 - val_loss: 4.4061e-05\n",
      "Epoch 264/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1185e-05 - val_loss: 4.4058e-05\n",
      "Epoch 265/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1178e-05 - val_loss: 4.4114e-05\n",
      "Epoch 266/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1183e-05 - val_loss: 4.4065e-05\n",
      "Epoch 267/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1179e-05 - val_loss: 4.4064e-05\n",
      "Epoch 268/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1177e-05 - val_loss: 4.4056e-05\n",
      "Epoch 269/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1167e-05 - val_loss: 4.4063e-05\n",
      "Epoch 270/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1172e-05 - val_loss: 4.4055e-05\n",
      "Epoch 271/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1168e-05 - val_loss: 4.4055e-05\n",
      "Epoch 272/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1169e-05 - val_loss: 4.4058e-05\n",
      "Epoch 273/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1160e-05 - val_loss: 4.4064e-05\n",
      "Epoch 274/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1164e-05 - val_loss: 4.4058e-05\n",
      "Epoch 275/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1151e-05 - val_loss: 4.4065e-05\n",
      "Epoch 276/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1156e-05 - val_loss: 4.4067e-05\n",
      "Epoch 277/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1155e-05 - val_loss: 4.4057e-05\n",
      "Epoch 278/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1143e-05 - val_loss: 4.4059e-05\n",
      "Epoch 279/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1150e-05 - val_loss: 4.4072e-05\n",
      "Epoch 280/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1146e-05 - val_loss: 4.4068e-05\n",
      "Epoch 281/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1146e-05 - val_loss: 4.4060e-05\n",
      "Epoch 282/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1139e-05 - val_loss: 4.4073e-05\n",
      "Epoch 283/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1142e-05 - val_loss: 4.4067e-05\n",
      "Epoch 284/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1137e-05 - val_loss: 4.4065e-05\n",
      "Epoch 285/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1138e-05 - val_loss: 4.4052e-05\n",
      "Epoch 286/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1136e-05 - val_loss: 4.4053e-05\n",
      "Epoch 287/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1132e-05 - val_loss: 4.4054e-05\n",
      "Epoch 288/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1128e-05 - val_loss: 4.4051e-05\n",
      "Epoch 289/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1129e-05 - val_loss: 4.4056e-05\n",
      "Epoch 290/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1120e-05 - val_loss: 4.4067e-05\n",
      "Epoch 291/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1124e-05 - val_loss: 4.4055e-05\n",
      "Epoch 292/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1123e-05 - val_loss: 4.4053e-05\n",
      "Epoch 293/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1114e-05 - val_loss: 4.4049e-05: 5.0908e-0\n",
      "Epoch 294/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1115e-05 - val_loss: 4.4059e-05\n",
      "Epoch 295/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1114e-05 - val_loss: 4.4056e-05\n",
      "Epoch 296/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1108e-05 - val_loss: 4.4055e-05\n",
      "Epoch 297/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1110e-05 - val_loss: 4.4063e-05\n",
      "Epoch 298/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1108e-05 - val_loss: 4.4055e-05\n",
      "Epoch 299/400\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.1107e-05 - val_loss: 4.4055e-05\n",
      "Epoch 300/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1099e-05 - val_loss: 4.4069e-05\n",
      "Epoch 301/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1100e-05 - val_loss: 4.4057e-05\n",
      "Epoch 302/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1101e-05 - val_loss: 4.4060e-05\n",
      "Epoch 303/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1095e-05 - val_loss: 4.4057e-05\n",
      "Epoch 304/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1094e-05 - val_loss: 4.4056e-05\n",
      "Epoch 305/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1089e-05 - val_loss: 4.4058e-05\n",
      "Epoch 306/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1091e-05 - val_loss: 4.4059e-05\n",
      "Epoch 307/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1087e-05 - val_loss: 4.4064e-050s - loss: 5.1292e\n",
      "Epoch 308/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1088e-05 - val_loss: 4.4059e-05\n",
      "Epoch 309/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1082e-05 - val_loss: 4.4061e-05\n",
      "Epoch 310/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1077e-05 - val_loss: 4.4074e-05\n",
      "Epoch 311/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1079e-05 - val_loss: 4.4064e-05\n",
      "Epoch 312/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1075e-05 - val_loss: 4.4068e-05\n",
      "Epoch 313/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1075e-05 - val_loss: 4.4076e-05\n",
      "Epoch 314/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1071e-05 - val_loss: 4.4068e-05TA: 0s - loss: 5.08\n",
      "Epoch 315/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1065e-05 - val_loss: 4.4070e-05\n",
      "Epoch 316/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1068e-05 - val_loss: 4.4070e-05\n",
      "Epoch 317/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1062e-05 - val_loss: 4.4071e-05\n",
      "Epoch 318/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1060e-05 - val_loss: 4.4071e-05\n",
      "Epoch 319/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1061e-05 - val_loss: 4.4071e-05\n",
      "Epoch 320/400\n",
      "55633/55633 [==============================] - 3s 49us/step - loss: 5.1058e-05 - val_loss: 4.4070e-05\n",
      "Epoch 321/400\n",
      "55633/55633 [==============================] - 3s 50us/step - loss: 5.1057e-05 - val_loss: 4.4072e-05\n",
      "Epoch 322/400\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.1055e-05 - val_loss: 4.4073e-05\n",
      "Epoch 323/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1055e-05 - val_loss: 4.4081e-05\n",
      "Epoch 324/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1051e-05 - val_loss: 4.4077e-05\n",
      "Epoch 325/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1049e-05 - val_loss: 4.4077e-05\n",
      "Epoch 326/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1047e-05 - val_loss: 4.4090e-05\n",
      "Epoch 327/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.1045e-05 - val_loss: 4.4092e-05\n",
      "Epoch 328/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1045e-05 - val_loss: 4.4094e-05\n",
      "Epoch 329/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1041e-05 - val_loss: 4.4094e-05\n",
      "Epoch 330/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1041e-05 - val_loss: 4.4077e-05\n",
      "Epoch 331/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1038e-05 - val_loss: 4.4086e-05\n",
      "Epoch 332/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1033e-05 - val_loss: 4.4087e-05\n",
      "Epoch 333/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1032e-05 - val_loss: 4.4082e-05\n",
      "Epoch 334/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1034e-05 - val_loss: 4.4085e-05\n",
      "Epoch 335/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1027e-05 - val_loss: 4.4089e-05\n",
      "Epoch 336/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1031e-05 - val_loss: 4.4092e-05\n",
      "Epoch 337/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1029e-05 - val_loss: 4.4093e-05\n",
      "Epoch 338/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1025e-05 - val_loss: 4.4112e-05\n",
      "Epoch 339/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1020e-05 - val_loss: 4.4098e-05\n",
      "Epoch 340/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.1024e-05 - val_loss: 4.4095e-05\n",
      "Epoch 341/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1020e-05 - val_loss: 4.4089e-05\n",
      "Epoch 342/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1007e-05 - val_loss: 4.4090e-05\n",
      "Epoch 343/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1017e-05 - val_loss: 4.4097e-05\n",
      "Epoch 344/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1013e-05 - val_loss: 4.4093e-05\n",
      "Epoch 345/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1014e-05 - val_loss: 4.4105e-05\n",
      "Epoch 346/400\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.1013e-05 - val_loss: 4.4092e-05\n",
      "Epoch 347/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1009e-05 - val_loss: 4.4108e-05\n",
      "Epoch 348/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1009e-05 - val_loss: 4.4094e-05\n",
      "Epoch 349/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1006e-05 - val_loss: 4.4095e-05\n",
      "Epoch 350/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1003e-05 - val_loss: 4.4097e-05\n",
      "Epoch 351/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.1003e-05 - val_loss: 4.4107e-05\n",
      "Epoch 352/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.0998e-05 - val_loss: 4.4100e-05\n",
      "Epoch 353/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.1000e-05 - val_loss: 4.4100e-05\n",
      "Epoch 354/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.1000e-05 - val_loss: 4.4106e-05\n",
      "Epoch 355/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.0997e-05 - val_loss: 4.4101e-05\n",
      "Epoch 356/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.0992e-05 - val_loss: 4.4118e-05\n",
      "Epoch 357/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.0991e-05 - val_loss: 4.4103e-05\n",
      "Epoch 358/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.0993e-05 - val_loss: 4.4118e-05\n",
      "Epoch 359/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.0990e-05 - val_loss: 4.4109e-05\n",
      "Epoch 360/400\n",
      "55633/55633 [==============================] - 2s 44us/step - loss: 5.0987e-05 - val_loss: 4.4111e-05\n",
      "Epoch 361/400\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.0984e-05 - val_loss: 4.4106e-05\n",
      "Epoch 362/400\n",
      "55633/55633 [==============================] - 2s 40us/step - loss: 5.0985e-05 - val_loss: 4.4108e-05\n",
      "Epoch 363/400\n",
      "55633/55633 [==============================] - 2s 40us/step - loss: 5.0983e-05 - val_loss: 4.4104e-05\n",
      "Epoch 364/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0984e-05 - val_loss: 4.4106e-05\n",
      "Epoch 365/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0980e-05 - val_loss: 4.4105e-05\n",
      "Epoch 366/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0978e-05 - val_loss: 4.4104e-05\n",
      "Epoch 367/400\n",
      "55633/55633 [==============================] - 2s 38us/step - loss: 5.0975e-05 - val_loss: 4.4102e-05\n",
      "Epoch 368/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0974e-05 - val_loss: 4.4107e-05\n",
      "Epoch 369/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0974e-05 - val_loss: 4.4111e-05\n",
      "Epoch 370/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0968e-05 - val_loss: 4.4111e-05\n",
      "Epoch 371/400\n",
      "55633/55633 [==============================] - 2s 41us/step - loss: 5.0970e-05 - val_loss: 4.4110e-05\n",
      "Epoch 372/400\n",
      "55633/55633 [==============================] - 2s 41us/step - loss: 5.0959e-05 - val_loss: 4.4138e-05\n",
      "Epoch 373/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0964e-05 - val_loss: 4.4106e-05\n",
      "Epoch 374/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0962e-05 - val_loss: 4.4112e-05\n",
      "Epoch 375/400\n",
      "55633/55633 [==============================] - 2s 38us/step - loss: 5.0962e-05 - val_loss: 4.4112e-05\n",
      "Epoch 376/400\n",
      "55633/55633 [==============================] - 2s 38us/step - loss: 5.0960e-05 - val_loss: 4.4122e-05\n",
      "Epoch 377/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0960e-05 - val_loss: 4.4113e-05\n",
      "Epoch 378/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0959e-05 - val_loss: 4.4112e-05\n",
      "Epoch 379/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0958e-05 - val_loss: 4.4118e-05\n",
      "Epoch 380/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0951e-05 - val_loss: 4.4110e-05\n",
      "Epoch 381/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0954e-05 - val_loss: 4.4139e-05\n",
      "Epoch 382/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0952e-05 - val_loss: 4.4109e-05\n",
      "Epoch 383/400\n",
      "55633/55633 [==============================] - 2s 38us/step - loss: 5.0951e-05 - val_loss: 4.4112e-05\n",
      "Epoch 384/400\n",
      "55633/55633 [==============================] - 2s 39us/step - loss: 5.0947e-05 - val_loss: 4.4119e-05\n",
      "Epoch 385/400\n",
      "55633/55633 [==============================] - 2s 40us/step - loss: 5.0943e-05 - val_loss: 4.4113e-05\n",
      "Epoch 386/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.0945e-05 - val_loss: 4.4113e-05\n",
      "Epoch 387/400\n",
      "55633/55633 [==============================] - 3s 48us/step - loss: 5.0937e-05 - val_loss: 4.4119e-05\n",
      "Epoch 388/400\n",
      "55633/55633 [==============================] - 3s 49us/step - loss: 5.0936e-05 - val_loss: 4.4110e-05TA: 0s - loss: 5.11\n",
      "Epoch 389/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.0938e-05 - val_loss: 4.4108e-05\n",
      "Epoch 390/400\n",
      "55633/55633 [==============================] - 2s 45us/step - loss: 5.0938e-05 - val_loss: 4.4106e-05\n",
      "Epoch 391/400\n",
      "55633/55633 [==============================] - 3s 47us/step - loss: 5.0931e-05 - val_loss: 4.4130e-05\n",
      "Epoch 392/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.0935e-05 - val_loss: 4.4135e-05\n",
      "Epoch 393/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.0933e-05 - val_loss: 4.4113e-05\n",
      "Epoch 394/400\n",
      "55633/55633 [==============================] - 3s 45us/step - loss: 5.0931e-05 - val_loss: 4.4105e-05\n",
      "Epoch 395/400\n",
      "55633/55633 [==============================] - 3s 46us/step - loss: 5.0928e-05 - val_loss: 4.4110e-050\n",
      "Epoch 396/400\n",
      "37888/55633 [===================>..........] - ETA: 0s - loss: 5.0697e-05"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=5, validation_split = 0.005, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=400, validation_split = 0.005, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00285213  0.01091266  0.01977928  0.03131202  0.0710565\n",
      "  0.0396825   0.04352681  0.05152528  0.05102924  0.05468745  0.11024305\n",
      "  0.08500739  0.0557416   0.04185268  0.01773314  0.02387157  0.00124006\n",
      " -0.02368556 -0.02337552 -0.01444694 -0.01109876 -0.00062008 -0.03478423\n",
      " -0.02752978 -0.0685144  -0.05016119 -0.0425967  -0.04600693 -0.01339288\n",
      " -0.00998264  0.01283477  0.00564238  0.01314481 -0.0048363  -0.03856648\n",
      " -0.04222469 -0.02430554 -0.03218007 -0.03131202 -0.0634921  -0.08172128\n",
      " -0.08866569 -0.0555556  -0.0476191  -0.05022325 -0.06684027 -0.05295144\n",
      " -0.02876984 -0.01209075 -0.03335817 -0.06063987 -0.0251736  -0.07434279\n",
      " -0.07831104 -0.07279269 -0.07378468 -0.09926832 -0.09883434 -0.06777039\n",
      " -0.08085323 -0.05158735 -0.06479413 -0.11377732]\n",
      "[0.9963]\n",
      "[1.0030931]\n"
     ]
    }
   ],
   "source": [
    "print test_data[0]\n",
    "print test_labels[0]\n",
    "print outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primitive policy temporarily in place of a RL policy network\n",
    "\n",
    "# signal step for our policy network\n",
    "signals = map(lambda arr: sum(arr) / len(arr),outputs)\n",
    "# signals = map(lambda arr: arr[0],outputs)\n",
    "trades = map(lambda signal: 1 if round(signal,4) > 1 else 0,signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one day return\n",
    "actual = map(lambda arr: arr[0],test_labels)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'signal':signals,\n",
    "    'actual': actual,\n",
    "    'trade':trades,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['entry_success'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['entry_failure'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['avoid_success'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 0) else 0,axis=1)\n",
    "df['avoid_failure'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 0) else 0,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>signal</th>\n",
       "      <th>trade</th>\n",
       "      <th>entry_success</th>\n",
       "      <th>entry_failure</th>\n",
       "      <th>avoid_success</th>\n",
       "      <th>avoid_failure</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9963</td>\n",
       "      <td>1.003093</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0029</td>\n",
       "      <td>1.000933</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0081</td>\n",
       "      <td>0.995592</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0090</td>\n",
       "      <td>1.002120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0119</td>\n",
       "      <td>1.004062</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0428</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9673</td>\n",
       "      <td>1.003048</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0040</td>\n",
       "      <td>1.003702</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0084</td>\n",
       "      <td>1.002775</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9995</td>\n",
       "      <td>1.001102</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0039</td>\n",
       "      <td>0.995918</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0624</td>\n",
       "      <td>0.997113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.998410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.9690</td>\n",
       "      <td>1.000476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9855</td>\n",
       "      <td>0.993554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.991166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0063</td>\n",
       "      <td>0.993477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9773</td>\n",
       "      <td>0.997145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.9757</td>\n",
       "      <td>1.004075</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0003</td>\n",
       "      <td>1.003633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0088</td>\n",
       "      <td>1.006135</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0033</td>\n",
       "      <td>1.001007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0105</td>\n",
       "      <td>1.002629</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.9670</td>\n",
       "      <td>1.006022</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0071</td>\n",
       "      <td>1.004193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9616</td>\n",
       "      <td>1.002617</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0175</td>\n",
       "      <td>1.001713</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0073</td>\n",
       "      <td>1.003085</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.9967</td>\n",
       "      <td>1.005095</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0322</td>\n",
       "      <td>0.999399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10915</th>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.996719</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10916</th>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.999205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10917</th>\n",
       "      <td>0.9935</td>\n",
       "      <td>1.000329</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10918</th>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.998431</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10919</th>\n",
       "      <td>1.0018</td>\n",
       "      <td>0.999622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10920</th>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.999491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10921</th>\n",
       "      <td>1.0079</td>\n",
       "      <td>0.997699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10922</th>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.998985</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10923</th>\n",
       "      <td>1.0061</td>\n",
       "      <td>1.001795</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10924</th>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.999192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>0.9989</td>\n",
       "      <td>1.000058</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10926</th>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.998784</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10927</th>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.998504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10928</th>\n",
       "      <td>1.0074</td>\n",
       "      <td>0.997405</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10929</th>\n",
       "      <td>1.0068</td>\n",
       "      <td>0.999606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10930</th>\n",
       "      <td>1.0002</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10931</th>\n",
       "      <td>0.9887</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10932</th>\n",
       "      <td>1.0237</td>\n",
       "      <td>1.002733</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10933</th>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.000678</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10934</th>\n",
       "      <td>0.9847</td>\n",
       "      <td>1.002540</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10935</th>\n",
       "      <td>0.9954</td>\n",
       "      <td>1.002226</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10936</th>\n",
       "      <td>0.9783</td>\n",
       "      <td>1.002827</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>1.0206</td>\n",
       "      <td>1.003049</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10938</th>\n",
       "      <td>0.9912</td>\n",
       "      <td>1.004967</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10939</th>\n",
       "      <td>0.9588</td>\n",
       "      <td>1.001597</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10940</th>\n",
       "      <td>1.0091</td>\n",
       "      <td>1.000190</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10941</th>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.999444</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10942</th>\n",
       "      <td>1.0037</td>\n",
       "      <td>0.999022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10943</th>\n",
       "      <td>1.0043</td>\n",
       "      <td>1.000498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944</th>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10945 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual    signal  trade  entry_success  entry_failure  avoid_success  \\\n",
       "0      0.9963  1.003093      1              0              1              0   \n",
       "1      1.0029  1.000933      1              1              0              0   \n",
       "2      1.0081  0.995592      0              0              0              0   \n",
       "3      1.0090  1.002120      1              1              0              0   \n",
       "4      1.0119  1.004062      1              1              0              0   \n",
       "5      1.0428  0.999151      0              0              0              0   \n",
       "6      0.9673  1.003048      1              0              1              0   \n",
       "7      1.0040  1.003702      1              1              0              0   \n",
       "8      1.0084  1.002775      1              1              0              0   \n",
       "9      0.9995  1.001102      1              0              1              0   \n",
       "10     1.0039  0.995918      0              0              0              0   \n",
       "11     1.0624  0.997113      0              0              0              0   \n",
       "12     0.9724  0.998410      0              0              0              1   \n",
       "13     0.9690  1.000476      1              0              1              0   \n",
       "14     0.9855  0.993554      0              0              0              1   \n",
       "15     0.9754  0.991166      0              0              0              1   \n",
       "16     1.0063  0.993477      0              0              0              0   \n",
       "17     0.9773  0.997145      0              0              0              1   \n",
       "18     0.9757  1.004075      1              0              1              0   \n",
       "19     1.0003  1.003633      1              1              0              0   \n",
       "20     1.0088  1.006135      1              1              0              0   \n",
       "21     1.0033  1.001007      1              1              0              0   \n",
       "22     1.0105  1.002629      1              1              0              0   \n",
       "23     0.9670  1.006022      1              0              1              0   \n",
       "24     1.0071  1.004193      1              1              0              0   \n",
       "25     0.9616  1.002617      1              0              1              0   \n",
       "26     1.0175  1.001713      1              1              0              0   \n",
       "27     1.0073  1.003085      1              1              0              0   \n",
       "28     0.9967  1.005095      1              0              1              0   \n",
       "29     1.0322  0.999399      0              0              0              0   \n",
       "...       ...       ...    ...            ...            ...            ...   \n",
       "10915  0.9898  0.996719      0              0              0              1   \n",
       "10916  1.0190  0.999205      0              0              0              0   \n",
       "10917  0.9935  1.000329      1              0              1              0   \n",
       "10918  0.9982  0.998431      0              0              0              1   \n",
       "10919  1.0018  0.999622      0              0              0              0   \n",
       "10920  0.9700  0.999491      0              0              0              1   \n",
       "10921  1.0079  0.997699      0              0              0              0   \n",
       "10922  0.9944  0.998985      0              0              0              1   \n",
       "10923  1.0061  1.001795      1              1              0              0   \n",
       "10924  0.9808  0.999192      0              0              0              1   \n",
       "10925  0.9989  1.000058      1              0              1              0   \n",
       "10926  0.9934  0.998784      0              0              0              1   \n",
       "10927  0.9836  0.998504      0              0              0              1   \n",
       "10928  1.0074  0.997405      0              0              0              0   \n",
       "10929  1.0068  0.999606      0              0              0              0   \n",
       "10930  1.0002  0.998857      0              0              0              0   \n",
       "10931  0.9887  0.999596      0              0              0              1   \n",
       "10932  1.0237  1.002733      1              1              0              0   \n",
       "10933  0.9991  1.000678      1              0              1              0   \n",
       "10934  0.9847  1.002540      1              0              1              0   \n",
       "10935  0.9954  1.002226      1              0              1              0   \n",
       "10936  0.9783  1.002827      1              0              1              0   \n",
       "10937  1.0206  1.003049      1              1              0              0   \n",
       "10938  0.9912  1.004967      1              0              1              0   \n",
       "10939  0.9588  1.001597      1              0              1              0   \n",
       "10940  1.0091  1.000190      1              1              0              0   \n",
       "10941  0.9879  0.999444      0              0              0              1   \n",
       "10942  1.0037  0.999022      0              0              0              0   \n",
       "10943  1.0043  1.000498      1              1              0              0   \n",
       "10944  0.9779  0.999911      0              0              0              1   \n",
       "\n",
       "       avoid_failure  success  \n",
       "0                  0        0  \n",
       "1                  0        1  \n",
       "2                  1        1  \n",
       "3                  0        1  \n",
       "4                  0        1  \n",
       "5                  1        1  \n",
       "6                  0        0  \n",
       "7                  0        1  \n",
       "8                  0        1  \n",
       "9                  0        0  \n",
       "10                 1        1  \n",
       "11                 1        1  \n",
       "12                 0        1  \n",
       "13                 0        0  \n",
       "14                 0        1  \n",
       "15                 0        1  \n",
       "16                 1        1  \n",
       "17                 0        1  \n",
       "18                 0        0  \n",
       "19                 0        1  \n",
       "20                 0        1  \n",
       "21                 0        1  \n",
       "22                 0        1  \n",
       "23                 0        0  \n",
       "24                 0        1  \n",
       "25                 0        0  \n",
       "26                 0        1  \n",
       "27                 0        1  \n",
       "28                 0        0  \n",
       "29                 1        1  \n",
       "...              ...      ...  \n",
       "10915              0        1  \n",
       "10916              1        1  \n",
       "10917              0        0  \n",
       "10918              0        1  \n",
       "10919              1        1  \n",
       "10920              0        1  \n",
       "10921              1        1  \n",
       "10922              0        1  \n",
       "10923              0        1  \n",
       "10924              0        1  \n",
       "10925              0        0  \n",
       "10926              0        1  \n",
       "10927              0        1  \n",
       "10928              1        1  \n",
       "10929              1        1  \n",
       "10930              1        1  \n",
       "10931              0        1  \n",
       "10932              0        1  \n",
       "10933              0        0  \n",
       "10934              0        0  \n",
       "10935              0        0  \n",
       "10936              0        0  \n",
       "10937              0        1  \n",
       "10938              0        0  \n",
       "10939              0        0  \n",
       "10940              0        1  \n",
       "10941              0        1  \n",
       "10942              1        1  \n",
       "10943              0        1  \n",
       "10944              0        1  \n",
       "\n",
       "[10945 rows x 8 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primitive policy - replace with a policy network which maximizes reward\n",
    "def label_success (row):\n",
    "    return 0 if (row['entry_failure'] == 1 or row['entry_failure'] == 1) else 1\n",
    "\n",
    "success = df.apply (lambda row: label_success (row),axis=1)\n",
    "df['success'] = success;\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04760136716262708\n",
      "0.030630579073428504\n",
      "0.4156178309956048\n",
      "0.40777279092926183\n"
     ]
    }
   ],
   "source": [
    "print df['actual'].corr(df['signal'])\n",
    "print df['actual'].corr(df['trade'])\n",
    "print df['actual'].corr(df['entry_success'])\n",
    "print df['actual'].corr(df['success'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10945.000000\n",
       "mean         1.000269\n",
       "std          0.010294\n",
       "min          0.946400\n",
       "25%          0.995600\n",
       "50%          1.000600\n",
       "75%          1.005600\n",
       "max          1.062400\n",
       "Name: actual, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['actual'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10945.000000\n",
       "mean         1.000021\n",
       "std          0.001873\n",
       "min          0.988116\n",
       "25%          0.999025\n",
       "50%          1.000008\n",
       "75%          1.001038\n",
       "max          1.013663\n",
       "Name: signal, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['signal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10945\n",
      "\n",
      "Precision\n",
      "0.550169747265\n",
      "\n",
      "Recall\n",
      "0.501979005335\n",
      "\n",
      "Accuracy\n",
      "0.266514390132\n",
      "\n",
      "Non-loss events\n",
      "8560\n",
      "0.78209227958\n",
      "\n",
      "Lose trades\n",
      "2385\n",
      "0.21790772042\n",
      "\n",
      "Win trades\n",
      "2917\n",
      "0.266514390132\n",
      "\n",
      "Missed opportunities\n",
      "2894\n",
      "0.264412973961\n",
      "\n",
      "Bullets dodged\n",
      "2657\n",
      "0.242759250799\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST_SAMPLES = len(test_data)\n",
    "print NUM_TEST_SAMPLES\n",
    "\n",
    "print '\\nPrecision' # optimize for this since we can increase discovery, so long as we find enough trades\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['entry_failure'])) \n",
    "\n",
    "print '\\nRecall'\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['avoid_failure']))\n",
    "\n",
    "print '\\nAccuracy'\n",
    "print sum(df['entry_success']) * 1.00 / (NUM_TEST_SAMPLES)\n",
    "\n",
    "print '\\nNon-loss events'\n",
    "print sum(df['success'])\n",
    "print sum(df['success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nLose trades'\n",
    "print sum(df['entry_failure'])\n",
    "print sum(df['entry_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nWin trades'\n",
    "print sum(df['entry_success'])\n",
    "print sum(df['entry_success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nMissed opportunities'\n",
    "print sum(df['avoid_failure'])\n",
    "print sum(df['avoid_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nBullets dodged'\n",
    "print sum(df['avoid_success'])\n",
    "print sum(df['avoid_success']) / (NUM_TEST_SAMPLES * 1.00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    ['SPY', model.predict(np.array(convert_to_train(SPY.copy(), 0)[0][:1]))],\n",
    "    ['QQQ', model.predict(np.array(convert_to_train(QQQ.copy(), 0)[0][:1]))],\n",
    "    ['XLE', model.predict(np.array(convert_to_train(XLE.copy(), 0)[0][:1]))],\n",
    "    ['XLF', model.predict(np.array(convert_to_train(XLF.copy(), 0)[0][:1]))],\n",
    "    ['XLK', model.predict(np.array(convert_to_train(XLK.copy(), 0)[0][:1]))],\n",
    "    ['XLP', model.predict(np.array(convert_to_train(XLP.copy(), 0)[0][:1]))],\n",
    "    ['XLV', model.predict(np.array(convert_to_train(XLV.copy(), 0)[0][:1]))],\n",
    "    ['XLU', model.predict(np.array(convert_to_train(XLU.copy(), 0)[0][:1]))],\n",
    "    ['XLY', model.predict(np.array(convert_to_train(XLY.copy(), 0)[0][:1]))],\n",
    "    ['XLI', model.predict(np.array(convert_to_train(XLI.copy(), 0)[0][:1]))],\n",
    "]\n",
    "\n",
    "predictions.sort(key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLP [[1.0019561]]\n",
      "XLE [[1.0004708]]\n",
      "QQQ [[1.0004005]]\n",
      "SPY [[0.99986374]]\n",
      "XLV [[0.9984318]]\n",
      "XLU [[0.9982821]]\n",
      "XLY [[0.99771714]]\n",
      "XLI [[0.99717605]]\n",
      "XLK [[0.9965079]]\n",
      "XLF [[0.995003]]\n",
      "[None, None, None, None, None, None, None, None, None, None]\n",
      "['XLP', 'XLE', 'QQQ', 'SPY', 'XLV', 'XLU', 'XLY', 'XLI', 'XLK', 'XLF']\n"
     ]
    }
   ],
   "source": [
    "def print_arg(arr):\n",
    "    print '{} {}'.format(arr[0],str(arr[1]))\n",
    "print map(print_arg, predictions)\n",
    "#print map(lambda arr: str(arr[0]+''+str(arr[1])+'\\\\n'),predictions)\n",
    "#print predictions\n",
    "print map(lambda arr: arr[0],predictions)\n",
    "#200['QQQ', 'XLK', 'XLY', 'XLE', 'XLV', 'XLU', 'SPY', 'XLP', 'XLF', 'XLI']\n",
    "#200['XLP', 'XLU', 'XLF', 'XLI', 'XLE', 'SPY', 'XLV', 'XLY', 'XLK', 'QQQ']\n",
    "#200['SPY', 'XLP', 'XLE', 'QQQ', 'XLY', 'XLV', 'XLU', 'XLK', 'XLI', 'XLF']\n",
    "#400['XLP', 'XLE', 'QQQ', 'SPY', 'XLV', 'XLU', 'XLY', 'XLI', 'XLK', 'XLF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[1.0035602] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-08f7756b8644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mXLY\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0035602\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mXLP\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0021014\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mXLF\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0024977\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mXLV\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0002216\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leunte/git/playing-with-numbers/tf/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leunte/git/playing-with-numbers/tf/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leunte/git/playing-with-numbers/tf/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[1.0035602] not in index'"
     ]
    }
   ],
   "source": [
    "\n",
    "XLY [[1.0035602]]\n",
    "XLP [[1.0021014]]\n",
    "XLF [[1.0024977]]\n",
    "XLV [[1.0002216]]\n",
    "XLU [[0.999964]]\n",
    "XLI [[0.9998115]]\n",
    "SPY [[0.99781907]]\n",
    "XLK [[0.99753517]]\n",
    "QQQ [[0.99573797]]\n",
    "XLE [[0.99497455]]\n",
    "\n",
    "\n",
    "XLY, XLP, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
