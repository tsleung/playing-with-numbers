{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade\n",
    "Loads valuation model(s) and applies error handling to rank sectors and trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "import itertools\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_INPUT_NEURONS = 64\n",
    "NUM_OUTPUT_NEURONS = 1\n",
    "DAY_OFFSET = 5\n",
    "\n",
    "def from_network(symbol):\n",
    "    return pdr.get_data_yahoo(symbols=symbol, start=datetime(1900, 1, 1)).sort_values(by=['Date'],ascending=False)\n",
    "\n",
    "# Create features (only close price for now)\n",
    "def convert_to_percentage(old, new):\n",
    "    return (old - new) / old\n",
    "\n",
    "def convert_labels_to_category(labels): \n",
    "    # Simplification - If positive return, 1, else 0\n",
    "    # return map(lambda arr: 1 if arr[0] > 1 else 0, labels)\n",
    "    # rounding simpliciation\n",
    "    return map(lambda arr: map(lambda val: round(val,4),arr), labels)\n",
    "\n",
    "def convert_to_train(raw_dataset, offset=5):\n",
    "    dataset = raw_dataset.copy()\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(offset, len(dataset) - NUM_INPUT_NEURONS):\n",
    "\n",
    "        feature_dataset = dataset[i:i+NUM_INPUT_NEURONS].copy()\n",
    "        latest_close = feature_dataset['Close'].iloc[0]\n",
    "        \n",
    "        features.append(\n",
    "            feature_dataset['Close']\n",
    "                .map(lambda current: convert_to_percentage(latest_close, current))\n",
    "                .tolist()\n",
    "        )\n",
    "        labels.append([\n",
    "            dataset['Close'].iloc[i-1] / latest_close, # 1 day trade\n",
    "        ])\n",
    "        \n",
    "    # Without converting labels the precision is hard to determine accuracy. \n",
    "    # Rather than crude 0/1, maybe this can be more sophisticated\n",
    "    labels = convert_labels_to_category(labels)\n",
    "    \n",
    "    return [features,labels]\n",
    "\n",
    "def split_data(symbol):\n",
    "    fetched = from_network(symbol)\n",
    "    converted = convert_to_train(fetched)\n",
    "    features = converted[0]\n",
    "    labels = converted[1]\n",
    "    training = [ # since we decide a model, use all data for training\n",
    "        features[1:],\n",
    "        labels[1:]\n",
    "    ]\n",
    "    validation = [\n",
    "        features[5:1000],\n",
    "        labels[5:1000]\n",
    "    ]\n",
    "    prediction = [\n",
    "        features[:5],\n",
    "        labels[:5],\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'prediction': prediction,\n",
    "        'validation': validation,\n",
    "        'training': training,\n",
    "    }\n",
    "\n",
    "    \n",
    "def combine_all(accum, prep):\n",
    "    return {   \n",
    "        'prediction':[\n",
    "            accum['prediction'][0] + prep['prediction'][0],\n",
    "            accum['prediction'][1] + prep['prediction'][1],\n",
    "        ],\n",
    "        'validation':[\n",
    "            accum['validation'][0] + prep['validation'][0],\n",
    "            accum['validation'][1] + prep['validation'][1],\n",
    "        ],\n",
    "        'training':[\n",
    "            accum['training'][0] + prep['training'][0],\n",
    "            accum['training'][1] + prep['training'][1],\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "    error_model = load_model('sector_model_error.h5')\n",
    "    model = load_model('sector_model.h5')\n",
    "    model_0 = load_model('sector_model_0.h5')\n",
    "    model_1 = load_model('sector_model_1.h5')\n",
    "    model_2 = load_model('sector_model_2.h5')\n",
    "    model_3 = load_model('sector_model_3.h5')\n",
    "    model_4 = load_model('sector_model_4.h5')\n",
    "    model_5 = load_model('sector_model_5.h5')\n",
    "    model_6 = load_model('sector_model_6.h5')\n",
    "    model_7 = load_model('sector_model_7.h5')\n",
    "    model_8 = load_model('sector_model_8.h5')\n",
    "    model_9 = load_model('sector_model_9.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QQQ = from_network('QQQ')\n",
    "SPY = from_network('SPY')\n",
    "XLK = from_network('XLK')\n",
    "XLF = from_network('XLF')\n",
    "XLE = from_network('XLE')\n",
    "XLP = from_network('XLP')\n",
    "XLV = from_network('XLV')\n",
    "XLY = from_network('XLY')\n",
    "XLI = from_network('XLI')\n",
    "XLU = from_network('XLU')\n",
    "DIA = from_network('DIA')\n",
    "IWM = from_network('IWM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "securities_to_predict = [\n",
    "#     ['SPY', np.array(convert_to_train(SPY.copy(), 0)[0][:1])],\n",
    "#     ['QQQ', np.array(convert_to_train(QQQ.copy(), 0)[0][:1])],\n",
    "    ['XLE', np.array(convert_to_train(XLE.copy(), 0)[0][:1])],\n",
    "    ['XLF', np.array(convert_to_train(XLF.copy(), 0)[0][:1])],\n",
    "    ['XLK', np.array(convert_to_train(XLK.copy(), 0)[0][:1])],\n",
    "    ['XLP', np.array(convert_to_train(XLP.copy(), 0)[0][:1])],\n",
    "    ['XLV', np.array(convert_to_train(XLV.copy(), 0)[0][:1])],\n",
    "    ['XLU', np.array(convert_to_train(XLU.copy(), 0)[0][:1])],\n",
    "    ['XLY', np.array(convert_to_train(XLY.copy(), 0)[0][:1])],\n",
    "    ['XLI', np.array(convert_to_train(XLI.copy(), 0)[0][:1])],\n",
    "#     ['IWM', np.array(convert_to_train(IWM.copy(), 0)[0][:1])],\n",
    "#     ['DIA', np.array(convert_to_train(DIA.copy(), 0)[0][:1])],\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_arg(arr):\n",
    "    print '{} {}'.format(arr[0],str(arr[1]))\n",
    "    \n",
    "    \n",
    "def predict_and_correct(model, inputs):\n",
    "    prediction = model.predict(inputs)\n",
    "    return prediction\n",
    "\n",
    "def generate_model_predictions(model):\n",
    "    predictions = map(lambda arr: [arr[0], predict_and_correct(model,arr[1])], securities_to_predict)\n",
    "    predictions.sort(key=lambda x: x[1],reverse=True)\n",
    "#     print map(print_arg, predictions)\n",
    "    print map(lambda arr: arr[0],predictions)\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XLY', 'XLK', 'XLE', 'XLP', 'XLI', 'XLF', 'XLU', 'XLV']\n",
      "['XLE', 'XLF', 'XLI', 'XLV', 'XLY', 'XLU', 'XLP', 'XLK']\n",
      "['XLP', 'XLI', 'XLK', 'XLU', 'XLY', 'XLE', 'XLV', 'XLF']\n",
      "['XLK', 'XLF', 'XLP', 'XLV', 'XLE', 'XLY', 'XLI', 'XLU']\n",
      "['XLE', 'XLU', 'XLV', 'XLK', 'XLY', 'XLP', 'XLI', 'XLF']\n",
      "['XLI', 'XLY', 'XLK', 'XLP', 'XLU', 'XLE', 'XLV', 'XLF']\n",
      "['XLE', 'XLV', 'XLY', 'XLK', 'XLP', 'XLF', 'XLI', 'XLU']\n",
      "['XLU', 'XLF', 'XLP', 'XLV', 'XLY', 'XLI', 'XLE', 'XLK']\n",
      "['XLE', 'XLI', 'XLK', 'XLF', 'XLV', 'XLY', 'XLU', 'XLP']\n",
      "['XLV', 'XLY', 'XLK', 'XLF', 'XLP', 'XLU', 'XLE', 'XLI']\n",
      "['XLP', 'XLU', 'XLV', 'XLF', 'XLI', 'XLK', 'XLY', 'XLE']\n"
     ]
    }
   ],
   "source": [
    "all_model_predictions = map(generate_model_predictions,[model, model_1, model_2, model_3, model_4,model_5,model_6,model_7,model_8,model_9,model_0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLK 2.0 [1, 7, 2, 0, 3, 2, 3, 7, 2, 2, 5]\n",
      "XLF 3.0 [5, 1, 7, 1, 7, 7, 5, 1, 3, 3, 3]\n",
      "XLP 3.0 [3, 6, 0, 2, 5, 3, 4, 2, 7, 4, 0]\n",
      "XLV 3.0 [7, 3, 6, 3, 2, 6, 1, 3, 4, 0, 2]\n",
      "XLE 4.0 [2, 0, 5, 4, 0, 5, 0, 6, 0, 6, 7]\n",
      "XLY 4.0 [0, 4, 4, 5, 4, 1, 2, 4, 5, 1, 6]\n",
      "XLI 4.0 [4, 2, 1, 6, 6, 0, 6, 5, 1, 7, 4]\n",
      "XLU 5.0 [6, 5, 3, 7, 1, 4, 7, 0, 6, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ranking_for_symbol(symbol):\n",
    "    return map(lambda predictions: map(lambda arr: arr[0],predictions).index(symbol),all_model_predictions)\n",
    "\n",
    "\n",
    "securities_to_predict.sort(key=lambda x: np.median(ranking_for_symbol(x[0])))\n",
    "\n",
    "for security in securities_to_predict:\n",
    "    print security[0],np.median(ranking_for_symbol(security[0])),ranking_for_symbol(security[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Trade policy\n",
    "\n",
    "1. Long rank <=2\n",
    "2. Short rank >=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
