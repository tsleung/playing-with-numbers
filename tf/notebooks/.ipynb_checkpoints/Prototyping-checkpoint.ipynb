{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6445\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 7,489\n",
      "Trainable params: 7,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "dataset_path = keras.utils.get_file(\"SPY.csv\", \"http://localhost:8000/data/daily/SPY.csv\")\n",
    "\n",
    "column_names = ['Date','Open','High','Low','Close','Adj Close','Volume'] \n",
    "raw_dataset = pd.read_csv(dataset_path, \n",
    "#                           names=column_names, \n",
    "                          dtype={'Close': np.float64,'Open': np.float64,'High': np.float64,'Adj Close': np.float64, 'Volume': np.float64},\n",
    "                          header=0,\n",
    "                          na_values = \"?\", \n",
    "                          comment='\\t',\n",
    "                          sep=\",\",\n",
    "                          skipinitialspace=True)\n",
    "\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.sort_values(by=['Date'],ascending=False)\n",
    "\n",
    "print len(dataset)\n",
    "\n",
    "dataset.head(5)\n",
    "\n",
    "dataset_stats = dataset.describe()\n",
    "dataset_stats = dataset_stats.transpose()\n",
    "dataset_stats\n",
    "\n",
    "# Create features (only close price for now)\n",
    "def convert_to_percentage(old, new):\n",
    "    return (old - new) / old\n",
    "\n",
    "# Simplification - If positive return, 1, else 0\n",
    "def convert_labels_to_category(labels): \n",
    "    return map(lambda arr: 1 if arr[0] > 1 else 0, labels)\n",
    "\n",
    "def convert_to_train(raw_dataset):\n",
    "    dataset = raw_dataset.copy()\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(5, len(dataset) - 50):\n",
    "\n",
    "        feature_dataset = dataset[i:i+50].copy()\n",
    "        latest_close = feature_dataset['Close'].iloc[0]\n",
    "        \n",
    "        features.append(\n",
    "            feature_dataset['Close']\n",
    "                .map(lambda current: convert_to_percentage(latest_close, current))\n",
    "                .tolist()\n",
    "        )\n",
    "        labels.append([\n",
    "            #dataset['Close'].iloc[i-1] / latest_close, # 1 day trade\n",
    "            #dataset['Close'].iloc[i-3] / latest_close, # 3 day trade\n",
    "            dataset['Close'].iloc[i-5] / latest_close, # 5 day trade\n",
    "        ])\n",
    "        \n",
    "    # Without converting labels the precision is hard to determine accuracy. \n",
    "    # Rather than crude 0/1, maybe this can be more sophisticated\n",
    "    labels = convert_labels_to_category(labels)\n",
    "    \n",
    "    return [features,labels]\n",
    "converted_feature_set = convert_to_train(dataset)\n",
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.nn.relu, input_shape=[len(converted_feature_set[0][0])]),\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer='sgd',\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#len(converted_feature_set[0][0])\n",
    "train_data = np.array(converted_feature_set[0][900:])\n",
    "train_labels = np.array(converted_feature_set[1][900:])\n",
    "\n",
    "test_data = np.array(converted_feature_set[0][:900])\n",
    "test_labels = np.array(converted_feature_set[1][:900])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.01299094 -0.02653142 ... -0.0270311  -0.02113519\n",
      "  -0.02048569]\n",
      " [ 0.         -0.01336683 -0.01099929 ... -0.0080398  -0.00739864\n",
      "  -0.007152  ]\n",
      " [ 0.          0.00233632 -0.00316383 ...  0.00588947  0.00613285\n",
      "   0.00725232]\n",
      " ...\n",
      " [ 0.          0.         -0.00139082 ... -0.0006943  -0.00139082\n",
      "   0.00278164]\n",
      " [ 0.         -0.00139082  0.00069652 ... -0.00139082  0.00278164\n",
      "   0.01321391]\n",
      " [ 0.          0.00208444  0.01597333 ...  0.00416667  0.01458444\n",
      "   0.01666667]]\n",
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print train_data\n",
    "print train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4392 samples, validate on 1098 samples\n",
      "Epoch 1/90\n",
      "4392/4392 [==============================] - 0s 96us/step - loss: 0.2777 - acc: 0.4898 - val_loss: 0.2511 - val_acc: 0.5128\n",
      "Epoch 2/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2506 - acc: 0.5244 - val_loss: 0.2498 - val_acc: 0.5100\n",
      "Epoch 3/90\n",
      "4392/4392 [==============================] - 0s 44us/step - loss: 0.2496 - acc: 0.5257 - val_loss: 0.2494 - val_acc: 0.5273\n",
      "Epoch 4/90\n",
      "4392/4392 [==============================] - 0s 44us/step - loss: 0.2492 - acc: 0.5289 - val_loss: 0.2490 - val_acc: 0.5273\n",
      "Epoch 5/90\n",
      "4392/4392 [==============================] - 0s 44us/step - loss: 0.2490 - acc: 0.5316 - val_loss: 0.2493 - val_acc: 0.5273\n",
      "Epoch 6/90\n",
      "4392/4392 [==============================] - 0s 43us/step - loss: 0.2489 - acc: 0.5394 - val_loss: 0.2486 - val_acc: 0.5273\n",
      "Epoch 7/90\n",
      "4392/4392 [==============================] - 0s 44us/step - loss: 0.2488 - acc: 0.5328 - val_loss: 0.2485 - val_acc: 0.5373\n",
      "Epoch 8/90\n",
      "4392/4392 [==============================] - 0s 44us/step - loss: 0.2487 - acc: 0.5414 - val_loss: 0.2500 - val_acc: 0.5273\n",
      "Epoch 9/90\n",
      "4392/4392 [==============================] - 0s 42us/step - loss: 0.2489 - acc: 0.5360 - val_loss: 0.2484 - val_acc: 0.5301\n",
      "Epoch 10/90\n",
      "4392/4392 [==============================] - 0s 44us/step - loss: 0.2485 - acc: 0.5376 - val_loss: 0.2490 - val_acc: 0.5273\n",
      "Epoch 11/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2486 - acc: 0.5330 - val_loss: 0.2489 - val_acc: 0.5273\n",
      "Epoch 12/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2482 - acc: 0.5346 - val_loss: 0.2483 - val_acc: 0.5383\n",
      "Epoch 13/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2484 - acc: 0.5364 - val_loss: 0.2483 - val_acc: 0.5346\n",
      "Epoch 14/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2485 - acc: 0.5371 - val_loss: 0.2483 - val_acc: 0.5346\n",
      "Epoch 15/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2483 - acc: 0.5353 - val_loss: 0.2490 - val_acc: 0.5264\n",
      "Epoch 16/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2484 - acc: 0.5376 - val_loss: 0.2484 - val_acc: 0.5355\n",
      "Epoch 17/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2484 - acc: 0.5373 - val_loss: 0.2486 - val_acc: 0.5291\n",
      "Epoch 18/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2483 - acc: 0.5367 - val_loss: 0.2483 - val_acc: 0.5392\n",
      "Epoch 19/90\n",
      "4392/4392 [==============================] - 0s 52us/step - loss: 0.2483 - acc: 0.5337 - val_loss: 0.2488 - val_acc: 0.5373\n",
      "Epoch 20/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2482 - acc: 0.5364 - val_loss: 0.2483 - val_acc: 0.5337\n",
      "Epoch 21/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2482 - acc: 0.5430 - val_loss: 0.2485 - val_acc: 0.5319\n",
      "Epoch 22/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2479 - acc: 0.5353 - val_loss: 0.2486 - val_acc: 0.5328\n",
      "Epoch 23/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2482 - acc: 0.5385 - val_loss: 0.2483 - val_acc: 0.5346\n",
      "Epoch 24/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2481 - acc: 0.5401 - val_loss: 0.2484 - val_acc: 0.5392\n",
      "Epoch 25/90\n",
      "4392/4392 [==============================] - 0s 51us/step - loss: 0.2481 - acc: 0.5417 - val_loss: 0.2484 - val_acc: 0.5346\n",
      "Epoch 26/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2478 - acc: 0.5467 - val_loss: 0.2495 - val_acc: 0.5255\n",
      "Epoch 27/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2481 - acc: 0.5417 - val_loss: 0.2483 - val_acc: 0.5373\n",
      "Epoch 28/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2482 - acc: 0.5419 - val_loss: 0.2483 - val_acc: 0.5319\n",
      "Epoch 29/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2481 - acc: 0.5419 - val_loss: 0.2484 - val_acc: 0.5346\n",
      "Epoch 30/90\n",
      "4392/4392 [==============================] - 0s 50us/step - loss: 0.2480 - acc: 0.5417 - val_loss: 0.2487 - val_acc: 0.5328\n",
      "Epoch 31/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2481 - acc: 0.5423 - val_loss: 0.2488 - val_acc: 0.5301\n",
      "Epoch 32/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2479 - acc: 0.5426 - val_loss: 0.2484 - val_acc: 0.5246\n",
      "Epoch 33/90\n",
      "4392/4392 [==============================] - 0s 57us/step - loss: 0.2480 - acc: 0.5449 - val_loss: 0.2483 - val_acc: 0.5328\n",
      "Epoch 34/90\n",
      "4392/4392 [==============================] - 0s 55us/step - loss: 0.2479 - acc: 0.5408 - val_loss: 0.2482 - val_acc: 0.5392\n",
      "Epoch 35/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2479 - acc: 0.5428 - val_loss: 0.2485 - val_acc: 0.5337\n",
      "Epoch 36/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2479 - acc: 0.5442 - val_loss: 0.2485 - val_acc: 0.5346\n",
      "Epoch 37/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2478 - acc: 0.5433 - val_loss: 0.2482 - val_acc: 0.5392\n",
      "Epoch 38/90\n",
      "4392/4392 [==============================] - 0s 50us/step - loss: 0.2479 - acc: 0.5421 - val_loss: 0.2484 - val_acc: 0.5383\n",
      "Epoch 39/90\n",
      "4392/4392 [==============================] - 0s 55us/step - loss: 0.2480 - acc: 0.5371 - val_loss: 0.2483 - val_acc: 0.5383\n",
      "Epoch 40/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2475 - acc: 0.5362 - val_loss: 0.2486 - val_acc: 0.5346\n",
      "Epoch 41/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2477 - acc: 0.5442 - val_loss: 0.2485 - val_acc: 0.5255\n",
      "Epoch 42/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2478 - acc: 0.5451 - val_loss: 0.2488 - val_acc: 0.5301\n",
      "Epoch 43/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2476 - acc: 0.5453 - val_loss: 0.2497 - val_acc: 0.5246\n",
      "Epoch 44/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2477 - acc: 0.5401 - val_loss: 0.2483 - val_acc: 0.5355\n",
      "Epoch 45/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2477 - acc: 0.5394 - val_loss: 0.2486 - val_acc: 0.5228\n",
      "Epoch 46/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2477 - acc: 0.5414 - val_loss: 0.2487 - val_acc: 0.5319\n",
      "Epoch 47/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2475 - acc: 0.5428 - val_loss: 0.2492 - val_acc: 0.5301\n",
      "Epoch 48/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2477 - acc: 0.5460 - val_loss: 0.2482 - val_acc: 0.5337\n",
      "Epoch 49/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2476 - acc: 0.5387 - val_loss: 0.2483 - val_acc: 0.5355\n",
      "Epoch 50/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2476 - acc: 0.5444 - val_loss: 0.2486 - val_acc: 0.5346\n",
      "Epoch 51/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2477 - acc: 0.5442 - val_loss: 0.2498 - val_acc: 0.5282\n",
      "Epoch 52/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2478 - acc: 0.5474 - val_loss: 0.2483 - val_acc: 0.5392\n",
      "Epoch 53/90\n",
      "4392/4392 [==============================] - 0s 54us/step - loss: 0.2476 - acc: 0.5458 - val_loss: 0.2484 - val_acc: 0.5237\n",
      "Epoch 54/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2476 - acc: 0.5496 - val_loss: 0.2483 - val_acc: 0.5337\n",
      "Epoch 55/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2476 - acc: 0.5485 - val_loss: 0.2489 - val_acc: 0.5301\n",
      "Epoch 56/90\n",
      "4392/4392 [==============================] - 0s 55us/step - loss: 0.2475 - acc: 0.5401 - val_loss: 0.2483 - val_acc: 0.5392\n",
      "Epoch 57/90\n",
      "4392/4392 [==============================] - 0s 54us/step - loss: 0.2476 - acc: 0.5442 - val_loss: 0.2483 - val_acc: 0.5328\n",
      "Epoch 58/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2476 - acc: 0.5433 - val_loss: 0.2482 - val_acc: 0.5383\n",
      "Epoch 59/90\n",
      "4392/4392 [==============================] - 0s 54us/step - loss: 0.2474 - acc: 0.5460 - val_loss: 0.2482 - val_acc: 0.5383\n",
      "Epoch 60/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2474 - acc: 0.5439 - val_loss: 0.2488 - val_acc: 0.5337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2474 - acc: 0.5460 - val_loss: 0.2487 - val_acc: 0.5355\n",
      "Epoch 62/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2473 - acc: 0.5490 - val_loss: 0.2486 - val_acc: 0.5219\n",
      "Epoch 63/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2475 - acc: 0.5492 - val_loss: 0.2488 - val_acc: 0.5337\n",
      "Epoch 64/90\n",
      "4392/4392 [==============================] - 0s 44us/step - loss: 0.2475 - acc: 0.5462 - val_loss: 0.2485 - val_acc: 0.5346\n",
      "Epoch 65/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2473 - acc: 0.5505 - val_loss: 0.2484 - val_acc: 0.5355\n",
      "Epoch 66/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2475 - acc: 0.5444 - val_loss: 0.2485 - val_acc: 0.5346\n",
      "Epoch 67/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2473 - acc: 0.5485 - val_loss: 0.2484 - val_acc: 0.5291\n",
      "Epoch 68/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2475 - acc: 0.5485 - val_loss: 0.2482 - val_acc: 0.5364\n",
      "Epoch 69/90\n",
      "4392/4392 [==============================] - 0s 44us/step - loss: 0.2473 - acc: 0.5474 - val_loss: 0.2484 - val_acc: 0.5200\n",
      "Epoch 70/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2472 - acc: 0.5508 - val_loss: 0.2482 - val_acc: 0.5383\n",
      "Epoch 71/90\n",
      "4392/4392 [==============================] - 0s 53us/step - loss: 0.2473 - acc: 0.5408 - val_loss: 0.2483 - val_acc: 0.5346\n",
      "Epoch 72/90\n",
      "4392/4392 [==============================] - 0s 51us/step - loss: 0.2472 - acc: 0.5385 - val_loss: 0.2483 - val_acc: 0.5319\n",
      "Epoch 73/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2472 - acc: 0.5446 - val_loss: 0.2495 - val_acc: 0.5337\n",
      "Epoch 74/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2473 - acc: 0.5487 - val_loss: 0.2486 - val_acc: 0.5346\n",
      "Epoch 75/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2473 - acc: 0.5453 - val_loss: 0.2490 - val_acc: 0.5337\n",
      "Epoch 76/90\n",
      "4392/4392 [==============================] - 0s 50us/step - loss: 0.2471 - acc: 0.5469 - val_loss: 0.2485 - val_acc: 0.5200\n",
      "Epoch 77/90\n",
      "4392/4392 [==============================] - 0s 50us/step - loss: 0.2472 - acc: 0.5494 - val_loss: 0.2486 - val_acc: 0.5337\n",
      "Epoch 78/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2472 - acc: 0.5483 - val_loss: 0.2483 - val_acc: 0.5328\n",
      "Epoch 79/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2473 - acc: 0.5487 - val_loss: 0.2483 - val_acc: 0.5383\n",
      "Epoch 80/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2472 - acc: 0.5460 - val_loss: 0.2483 - val_acc: 0.5364\n",
      "Epoch 81/90\n",
      "4392/4392 [==============================] - 0s 49us/step - loss: 0.2472 - acc: 0.5419 - val_loss: 0.2484 - val_acc: 0.5373\n",
      "Epoch 82/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2470 - acc: 0.5474 - val_loss: 0.2486 - val_acc: 0.5346\n",
      "Epoch 83/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2473 - acc: 0.5492 - val_loss: 0.2495 - val_acc: 0.5337\n",
      "Epoch 84/90\n",
      "4392/4392 [==============================] - 0s 43us/step - loss: 0.2473 - acc: 0.5474 - val_loss: 0.2485 - val_acc: 0.5200\n",
      "Epoch 85/90\n",
      "4392/4392 [==============================] - 0s 46us/step - loss: 0.2471 - acc: 0.5487 - val_loss: 0.2483 - val_acc: 0.5355\n",
      "Epoch 86/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2472 - acc: 0.5501 - val_loss: 0.2486 - val_acc: 0.5191\n",
      "Epoch 87/90\n",
      "4392/4392 [==============================] - 0s 47us/step - loss: 0.2471 - acc: 0.5485 - val_loss: 0.2493 - val_acc: 0.5319\n",
      "Epoch 88/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2470 - acc: 0.5449 - val_loss: 0.2490 - val_acc: 0.5346\n",
      "Epoch 89/90\n",
      "4392/4392 [==============================] - 0s 48us/step - loss: 0.2471 - acc: 0.5458 - val_loss: 0.2482 - val_acc: 0.5346\n",
      "Epoch 90/90\n",
      "4392/4392 [==============================] - 0s 45us/step - loss: 0.2471 - acc: 0.5492 - val_loss: 0.2484 - val_acc: 0.5237\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  train_data, train_labels,\n",
    "  epochs=90, validation_split = 0.2, verbose=1,\n",
    "  callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
