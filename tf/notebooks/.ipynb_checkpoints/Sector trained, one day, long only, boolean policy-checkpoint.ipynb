{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "\n",
    "Creates a neural network which evaluates a time series and produce a set of predicted values for the time series\n",
    "Predicted values may be used in a policy to make a trade. This policy may be modeled by simple multiple regression or a neural network.\n",
    "\n",
    "## Data\n",
    "Test data is taken as most recent to avoid lookahead bias. Train data is split into a validation and training set during fitting.\n",
    "\n",
    "\n",
    "## TODO\n",
    "- Convert feature percentages to stdev\n",
    "- Adding VIX as a signal\n",
    "- Adding High/Low as signals\n",
    "- Multiple securities/ aggregate samples\n",
    "- Policy network\n",
    "- Regularization (l2)\n",
    "- Dilated convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "NUM_INPUT_NEURONS = 64\n",
    "NUM_OUTPUT_NEURONS = 1\n",
    "DAY_OFFSET = 5\n",
    "\n",
    "def from_network(symbol):\n",
    "    return pdr.get_data_yahoo(symbols=symbol, start=datetime(1900, 1, 1))\n",
    "\n",
    "def from_file(symbol):\n",
    "    dataset_path = keras.utils.get_file(\"{}.csv\".format(symbol), \"http://localhost:8000/data/daily/{}.csv\".format(symbol))\n",
    "    column_names = ['Date','Open','High','Low','Close','Adj Close','Volume'] \n",
    "    return pd.read_csv(dataset_path, \n",
    "                              names=column_names, \n",
    "                              dtype={'Close': np.float64,'Open': np.float64,'High': np.float64,'Adj Close': np.float64, 'Volume': np.float64},\n",
    "                              header=0,\n",
    "                              na_values = \"?\", \n",
    "                              comment='\\t',\n",
    "                              sep=\",\",\n",
    "                              skipinitialspace=True)\n",
    "\n",
    "\n",
    "# Create features (only close price for now)\n",
    "def convert_to_percentage(old, new):\n",
    "    return (old - new) / old\n",
    "\n",
    "def convert_labels_to_category(labels): \n",
    "    # Simplification - If positive return, 1, else 0\n",
    "    # return map(lambda arr: 1 if arr[0] > 1 else 0, labels)\n",
    "    # rounding simpliciation\n",
    "    return map(lambda arr: map(lambda val: round(val,4),arr), labels)\n",
    "\n",
    "def convert_to_train(raw_dataset, offset=5):\n",
    "    dataset = raw_dataset.copy()\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(offset, len(dataset) - NUM_INPUT_NEURONS):\n",
    "\n",
    "        feature_dataset = dataset[i:i+NUM_INPUT_NEURONS].copy()\n",
    "        latest_close = feature_dataset['Close'].iloc[0]\n",
    "        \n",
    "        features.append(\n",
    "            feature_dataset['Close']\n",
    "                .map(lambda current: convert_to_percentage(latest_close, current))\n",
    "                .tolist()\n",
    "        )\n",
    "        labels.append([\n",
    "            dataset['Close'].iloc[i-1] / latest_close, # 1 day trade\n",
    "#             dataset['Close'].iloc[i-2] / latest_close, # 2 day trade\n",
    "#             dataset['Close'].iloc[i-3] / latest_close, # 3 day trade\n",
    "#             dataset['Close'].iloc[i-4] / latest_close, # 4 day trade\n",
    "#             dataset['Close'].iloc[i-5] / latest_close, # 5 day trade\n",
    "        ])\n",
    "        \n",
    "    # Without converting labels the precision is hard to determine accuracy. \n",
    "    # Rather than crude 0/1, maybe this can be more sophisticated\n",
    "    labels = convert_labels_to_category(labels)\n",
    "    \n",
    "    return [features,labels]\n",
    "def split_data(symbol):\n",
    "    fetched = from_network(symbol).sort_values(by=['Date'],ascending=False)\n",
    "    converted = convert_to_train(fetched)\n",
    "    features = converted[0]\n",
    "    labels = converted[1]\n",
    "    prediction = [\n",
    "        features[:5],\n",
    "        labels[:5],\n",
    "    ]\n",
    "    validation = [\n",
    "        features[5:1000],\n",
    "        labels[5:1000]\n",
    "    ]\n",
    "    training = [\n",
    "        features[1000:],\n",
    "        labels[1000:]\n",
    "    ]\n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'prediction': prediction,\n",
    "        'validation': validation,\n",
    "        'training': training,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 48)                3120      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                888       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 14,405\n",
      "Trainable params: 14,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.nn.relu, input_shape=[NUM_INPUT_NEURONS]),\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dense(48, activation=tf.nn.relu),\n",
    "    layers.Dense(36, activation=tf.nn.relu),\n",
    "    layers.Dense(24, activation=tf.nn.relu),\n",
    "    layers.Dense(12, activation=tf.nn.relu),\n",
    "\n",
    "      \n",
    "#     layers.Dense(64, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(48, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(36, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(24, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(12, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "    layers.Dense(NUM_OUTPUT_NEURONS)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_squared_logarithmic_error',\n",
    "                optimizer='sgd',\n",
    "#                 metrics=[\n",
    "#                     'mae',\n",
    "#                 ]\n",
    "               )\n",
    "  return model\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset = raw_dataset.copy()\n",
    "#dataset = from_network('SPY').sort_values(by=['Date'],ascending=False)\n",
    "# add function to cache fetch\n",
    "QQQ = from_network('QQQ')\n",
    "SPY = from_network('SPY')\n",
    "XLK = from_network('XLK')\n",
    "XLF = from_network('XLF')\n",
    "XLE = from_network('XLE')\n",
    "XLP = from_network('XLP')\n",
    "XLV = from_network('XLV')\n",
    "XLY = from_network('XLY')\n",
    "XLI = from_network('XLI')\n",
    "XLU = from_network('XLU')\n",
    "\n",
    "#dataset = pd.concat([QQQ,SPY,XLK,XLF,XLE,XLP,XLV,XLY,XLI,XLU]).sort_values(by=['Date'],ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepped_data = map(split_data, [\n",
    "    'QQQ',\n",
    "    'SPY',\n",
    "    'IWM',\n",
    "    'VTI',\n",
    "    'XLK',\n",
    "    'XLF',\n",
    "    'XLE',\n",
    "    'XLP',\n",
    "    'XLV',\n",
    "    'XLY',\n",
    "    'XLI',\n",
    "    'XLU',\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQQ\n",
      "SPY\n",
      "IWM\n",
      "VTI\n",
      "XLK\n",
      "XLF\n",
      "XLE\n",
      "XLP\n",
      "XLV\n",
      "XLY\n",
      "XLI\n",
      "XLU\n",
      "0\n",
      "3934\n",
      "9410\n",
      "13036\n",
      "16397\n",
      "20383\n",
      "24369\n",
      "28355\n",
      "32341\n",
      "36327\n",
      "40313\n",
      "44299\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(prepped_data)):\n",
    "    print prepped_data[i]['symbol']\n",
    "    \n",
    "def combine_all(accum, prep):\n",
    "    \n",
    "    print len(accum['training'][0])\n",
    "    return {   \n",
    "        'prediction':[\n",
    "            accum['prediction'][0] + prep['prediction'][0],\n",
    "            accum['prediction'][1] + prep['prediction'][1],\n",
    "        ],\n",
    "        'validation':[\n",
    "            accum['validation'][0] + prep['validation'][0],\n",
    "            accum['validation'][1] + prep['validation'][1],\n",
    "        ],\n",
    "        'training':[\n",
    "            accum['training'][0] + prep['training'][0],\n",
    "            accum['training'][1] + prep['training'][1],\n",
    "        ],\n",
    "    }\n",
    "combined = reduce(combine_all, prepped_data,{\n",
    "    'prediction':[[],[]],\n",
    "    'validation':[[],[]],\n",
    "    'training':[[],[]],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48285"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "len(combined['training'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48285\n",
      "11940\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "#len(converted_feature_set[0][0])\n",
    "print len(combined['training'][0])\n",
    "train_data = np.array(combined['training'][0])\n",
    "train_labels = np.array(combined['training'][1])\n",
    "\n",
    "print len(combined['validation'][0])\n",
    "test_data = np.array(combined['validation'][0])\n",
    "test_labels = np.array(combined['validation'][1])\n",
    "\n",
    "print len(combined['prediction'][0])\n",
    "prediction_data = np.array(combined['prediction'][0])\n",
    "prediction_labels = np.array(combined['prediction'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.00515264 -0.03190647 ...  0.01099882  0.00733252\n",
      "   0.02209667]\n",
      " [ 0.         -0.02661669 -0.02779968 ...  0.01242116  0.02710962\n",
      "   0.02779968]\n",
      " [ 0.         -0.00115232  0.00105628 ...  0.05233337  0.05300554\n",
      "   0.06068753]\n",
      " ...\n",
      " [ 0.         -0.01635514 -0.02161122 ... -0.13434394 -0.14368973\n",
      "  -0.12499815]\n",
      " [ 0.         -0.0051715  -0.0183908  ... -0.12528553 -0.10689473\n",
      "  -0.11264368]\n",
      " [ 0.         -0.01315129 -0.01658001 ... -0.10119987 -0.10691924\n",
      "  -0.08690688]]\n",
      "[[1.0096]\n",
      " [0.9949]\n",
      " [0.9741]\n",
      " ...\n",
      " [0.9977]\n",
      " [0.9839]\n",
      " [0.9949]]\n"
     ]
    }
   ],
   "source": [
    "print train_data\n",
    "print train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38628 samples, validate on 9657 samples\n",
      "Epoch 1/600\n",
      "38628/38628 [==============================] - 2s 53us/step - loss: 0.0081 - val_loss: 5.0797e-05\n",
      "Epoch 2/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.5056e-05 - val_loss: 4.9238e-05\n",
      "Epoch 3/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.4384e-05 - val_loss: 4.8927e-05\n",
      "Epoch 4/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.4118e-05 - val_loss: 4.8772e-05\n",
      "Epoch 5/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.3919e-05 - val_loss: 4.8579e-05\n",
      "Epoch 6/600\n",
      "38628/38628 [==============================] - 2s 52us/step - loss: 6.3736e-05 - val_loss: 4.8447e-05\n",
      "Epoch 7/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.3590e-05 - val_loss: 4.8328e-05\n",
      "Epoch 8/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 6.3436e-05 - val_loss: 4.8254e-05\n",
      "Epoch 9/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 6.3300e-05 - val_loss: 4.8140e-05\n",
      "Epoch 10/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.3189e-05 - val_loss: 4.8044e-05\n",
      "Epoch 11/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 6.3065e-05 - val_loss: 4.8072e-05\n",
      "Epoch 12/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.2957e-05 - val_loss: 4.7888e-05\n",
      "Epoch 13/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.2855e-05 - val_loss: 4.7830e-05\n",
      "Epoch 14/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.2753e-05 - val_loss: 4.7779e-05\n",
      "Epoch 15/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.2664e-05 - val_loss: 4.7713e-05\n",
      "Epoch 16/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.2570e-05 - val_loss: 4.7656e-05\n",
      "Epoch 17/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.2490e-05 - val_loss: 4.7617e-05\n",
      "Epoch 18/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.2413e-05 - val_loss: 4.7589e-05\n",
      "Epoch 19/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.2335e-05 - val_loss: 4.7520e-05\n",
      "Epoch 20/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.2273e-05 - val_loss: 4.7492e-05\n",
      "Epoch 21/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.2198e-05 - val_loss: 4.7466e-05\n",
      "Epoch 22/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.2139e-05 - val_loss: 4.7442e-05\n",
      "Epoch 23/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.2078e-05 - val_loss: 4.7390e-05\n",
      "Epoch 24/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 6.2022e-05 - val_loss: 4.7374e-05\n",
      "Epoch 25/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.1964e-05 - val_loss: 4.7313e-05\n",
      "Epoch 26/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1905e-05 - val_loss: 4.7286e-05\n",
      "Epoch 27/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.1852e-05 - val_loss: 4.7255e-05\n",
      "Epoch 28/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1799e-05 - val_loss: 4.7226e-05\n",
      "Epoch 29/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1752e-05 - val_loss: 4.7199e-05\n",
      "Epoch 30/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1701e-05 - val_loss: 4.7174e-05\n",
      "Epoch 31/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.1650e-05 - val_loss: 4.7159e-05\n",
      "Epoch 32/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.1605e-05 - val_loss: 4.7134e-05\n",
      "Epoch 33/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1558e-05 - val_loss: 4.7134e-05\n",
      "Epoch 34/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1511e-05 - val_loss: 4.7089e-05\n",
      "Epoch 35/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1468e-05 - val_loss: 4.7062e-05\n",
      "Epoch 36/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1425e-05 - val_loss: 4.7071e-05\n",
      "Epoch 37/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.1388e-05 - val_loss: 4.6997e-05\n",
      "Epoch 38/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.1345e-05 - val_loss: 4.6973e-05\n",
      "Epoch 39/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1290e-05 - val_loss: 4.6950e-05\n",
      "Epoch 40/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.1260e-05 - val_loss: 4.6944e-05\n",
      "Epoch 41/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.1221e-05 - val_loss: 4.6979e-05\n",
      "Epoch 42/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.1188e-05 - val_loss: 4.6916e-05\n",
      "Epoch 43/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.1142e-05 - val_loss: 4.6877e-05\n",
      "Epoch 44/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.1115e-05 - val_loss: 4.6856e-05\n",
      "Epoch 45/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.1069e-05 - val_loss: 4.6833e-05\n",
      "Epoch 46/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.1027e-05 - val_loss: 4.7005e-05\n",
      "Epoch 47/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.1007e-05 - val_loss: 4.6800e-05\n",
      "Epoch 48/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.0974e-05 - val_loss: 4.6812e-05\n",
      "Epoch 49/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.0933e-05 - val_loss: 4.6770e-05\n",
      "Epoch 50/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0911e-05 - val_loss: 4.6753e-05\n",
      "Epoch 51/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0874e-05 - val_loss: 4.6742e-05\n",
      "Epoch 52/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.0837e-05 - val_loss: 4.6767e-05\n",
      "Epoch 53/600\n",
      "38628/38628 [==============================] - 2s 51us/step - loss: 6.0808e-05 - val_loss: 4.6705e-05\n",
      "Epoch 54/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 6.0774e-05 - val_loss: 4.6690e-05\n",
      "Epoch 55/600\n",
      "38628/38628 [==============================] - 2s 50us/step - loss: 6.0739e-05 - val_loss: 4.6687e-05\n",
      "Epoch 56/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 6.0718e-05 - val_loss: 4.6660e-05\n",
      "Epoch 57/600\n",
      "38628/38628 [==============================] - 2s 53us/step - loss: 6.0679e-05 - val_loss: 4.6781e-05\n",
      "Epoch 58/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.0644e-05 - val_loss: 4.6654e-05\n",
      "Epoch 59/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 6.0630e-05 - val_loss: 4.6629e-05\n",
      "Epoch 60/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 6.0593e-05 - val_loss: 4.6672e-05\n",
      "Epoch 61/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 6.0557e-05 - val_loss: 4.6593e-05\n",
      "Epoch 62/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 6.0540e-05 - val_loss: 4.6579e-05\n",
      "Epoch 63/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0515e-05 - val_loss: 4.6573e-05\n",
      "Epoch 64/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.0485e-05 - val_loss: 4.6554e-05\n",
      "Epoch 65/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0459e-05 - val_loss: 4.6602e-05\n",
      "Epoch 66/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0441e-05 - val_loss: 4.6530e-05\n",
      "Epoch 67/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.0411e-05 - val_loss: 4.6542e-05\n",
      "Epoch 68/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0390e-05 - val_loss: 4.6529e-05\n",
      "Epoch 69/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0365e-05 - val_loss: 4.6504e-05\n",
      "Epoch 70/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.0342e-05 - val_loss: 4.6576e-05\n",
      "Epoch 71/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0331e-05 - val_loss: 4.6490e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0306e-05 - val_loss: 4.6544e-05\n",
      "Epoch 73/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.0288e-05 - val_loss: 4.6518e-05\n",
      "Epoch 74/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.0259e-05 - val_loss: 4.6444e-05\n",
      "Epoch 75/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 6.0228e-05 - val_loss: 4.6457e-05\n",
      "Epoch 76/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0218e-05 - val_loss: 4.6450e-05\n",
      "Epoch 77/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 6.0194e-05 - val_loss: 4.6417e-05\n",
      "Epoch 78/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0180e-05 - val_loss: 4.6409e-05\n",
      "Epoch 79/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 6.0155e-05 - val_loss: 4.6417e-05\n",
      "Epoch 80/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.0138e-05 - val_loss: 4.6392e-05\n",
      "Epoch 81/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0124e-05 - val_loss: 4.6382e-05\n",
      "Epoch 82/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 6.0092e-05 - val_loss: 4.6414e-05\n",
      "Epoch 83/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 6.0083e-05 - val_loss: 4.6367e-05\n",
      "Epoch 84/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 6.0062e-05 - val_loss: 4.6399e-05\n",
      "Epoch 85/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 6.0054e-05 - val_loss: 4.6352e-05\n",
      "Epoch 86/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 6.0027e-05 - val_loss: 4.6365e-05\n",
      "Epoch 87/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 6.0005e-05 - val_loss: 4.6340e-05\n",
      "Epoch 88/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9983e-05 - val_loss: 4.6333e-05\n",
      "Epoch 89/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9976e-05 - val_loss: 4.6341e-05\n",
      "Epoch 90/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9946e-05 - val_loss: 4.6387e-05\n",
      "Epoch 91/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9949e-05 - val_loss: 4.6332e-05\n",
      "Epoch 92/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9923e-05 - val_loss: 4.6302e-05\n",
      "Epoch 93/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9906e-05 - val_loss: 4.6426e-05\n",
      "Epoch 94/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9901e-05 - val_loss: 4.6288e-05\n",
      "Epoch 95/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9887e-05 - val_loss: 4.6282e-05\n",
      "Epoch 96/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9866e-05 - val_loss: 4.6298e-05\n",
      "Epoch 97/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9849e-05 - val_loss: 4.6298e-05\n",
      "Epoch 98/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9841e-05 - val_loss: 4.6264e-05\n",
      "Epoch 99/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9814e-05 - val_loss: 4.6258e-05\n",
      "Epoch 100/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.9790e-05 - val_loss: 4.6291e-05\n",
      "Epoch 101/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9797e-05 - val_loss: 4.6279e-05\n",
      "Epoch 102/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9774e-05 - val_loss: 4.6250e-05\n",
      "Epoch 103/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9765e-05 - val_loss: 4.6241e-05\n",
      "Epoch 104/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9735e-05 - val_loss: 4.6229e-05\n",
      "Epoch 105/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9734e-05 - val_loss: 4.6223e-05\n",
      "Epoch 106/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9722e-05 - val_loss: 4.6237e-05\n",
      "Epoch 107/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9705e-05 - val_loss: 4.6222e-05\n",
      "Epoch 108/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9696e-05 - val_loss: 4.6265e-05\n",
      "Epoch 109/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9687e-05 - val_loss: 4.6227e-05\n",
      "Epoch 110/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9672e-05 - val_loss: 4.6213e-05\n",
      "Epoch 111/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9661e-05 - val_loss: 4.6193e-05\n",
      "Epoch 112/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9641e-05 - val_loss: 4.6187e-05\n",
      "Epoch 113/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9638e-05 - val_loss: 4.6183e-05\n",
      "Epoch 114/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9626e-05 - val_loss: 4.6179e-05\n",
      "Epoch 115/600\n",
      "38628/38628 [==============================] - 2s 52us/step - loss: 5.9603e-05 - val_loss: 4.6172e-05\n",
      "Epoch 116/600\n",
      "38628/38628 [==============================] - 2s 51us/step - loss: 5.9601e-05 - val_loss: 4.6179e-05\n",
      "Epoch 117/600\n",
      "38628/38628 [==============================] - 2s 50us/step - loss: 5.9588e-05 - val_loss: 4.6203e-05\n",
      "Epoch 118/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.9569e-05 - val_loss: 4.6187e-05\n",
      "Epoch 119/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.9560e-05 - val_loss: 4.6157e-05\n",
      "Epoch 120/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9546e-05 - val_loss: 4.6242e-05\n",
      "Epoch 121/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.9547e-05 - val_loss: 4.6280e-05\n",
      "Epoch 122/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9532e-05 - val_loss: 4.6194e-05\n",
      "Epoch 123/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9521e-05 - val_loss: 4.6136e-05\n",
      "Epoch 124/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9491e-05 - val_loss: 4.6139e-05\n",
      "Epoch 125/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9489e-05 - val_loss: 4.6142e-05\n",
      "Epoch 126/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9487e-05 - val_loss: 4.6145e-05\n",
      "Epoch 127/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9479e-05 - val_loss: 4.6118e-05\n",
      "Epoch 128/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9458e-05 - val_loss: 4.6198e-05\n",
      "Epoch 129/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9461e-05 - val_loss: 4.6168e-05\n",
      "Epoch 130/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9445e-05 - val_loss: 4.6165e-05\n",
      "Epoch 131/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9431e-05 - val_loss: 4.6145e-05\n",
      "Epoch 132/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9429e-05 - val_loss: 4.6096e-05\n",
      "Epoch 133/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9412e-05 - val_loss: 4.6091e-05\n",
      "Epoch 134/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9411e-05 - val_loss: 4.6088e-05\n",
      "Epoch 135/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9395e-05 - val_loss: 4.6084e-05\n",
      "Epoch 136/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.9386e-05 - val_loss: 4.6087e-05\n",
      "Epoch 137/600\n",
      "38628/38628 [==============================] - 2s 50us/step - loss: 5.9373e-05 - val_loss: 4.6080e-05\n",
      "Epoch 138/600\n",
      "38628/38628 [==============================] - 2s 50us/step - loss: 5.9372e-05 - val_loss: 4.6076e-05\n",
      "Epoch 139/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9352e-05 - val_loss: 4.6066e-05\n",
      "Epoch 140/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9340e-05 - val_loss: 4.6102e-05\n",
      "Epoch 141/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9338e-05 - val_loss: 4.6064e-05\n",
      "Epoch 142/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9326e-05 - val_loss: 4.6063e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.9313e-05 - val_loss: 4.6049e-05\n",
      "Epoch 144/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9301e-05 - val_loss: 4.6044e-05\n",
      "Epoch 145/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9304e-05 - val_loss: 4.6046e-05\n",
      "Epoch 146/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9287e-05 - val_loss: 4.6038e-05\n",
      "Epoch 147/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9275e-05 - val_loss: 4.6078e-05\n",
      "Epoch 148/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9278e-05 - val_loss: 4.6044e-05\n",
      "Epoch 149/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9263e-05 - val_loss: 4.6045e-05\n",
      "Epoch 150/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9246e-05 - val_loss: 4.6082e-05\n",
      "Epoch 151/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9242e-05 - val_loss: 4.6023e-05\n",
      "Epoch 152/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9224e-05 - val_loss: 4.6016e-05 - loss: 5.9484e-\n",
      "Epoch 153/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9223e-05 - val_loss: 4.6016e-05\n",
      "Epoch 154/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9208e-05 - val_loss: 4.6016e-05- ETA: 1s\n",
      "Epoch 155/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9205e-05 - val_loss: 4.6137e-05\n",
      "Epoch 156/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9204e-05 - val_loss: 4.6000e-05\n",
      "Epoch 157/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9179e-05 - val_loss: 4.6045e-05\n",
      "Epoch 158/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9172e-05 - val_loss: 4.5996e-05\n",
      "Epoch 159/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.9169e-05 - val_loss: 4.6001e-05\n",
      "Epoch 160/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9160e-05 - val_loss: 4.5985e-05\n",
      "Epoch 161/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9153e-05 - val_loss: 4.5982e-05\n",
      "Epoch 162/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9134e-05 - val_loss: 4.5993e-05\n",
      "Epoch 163/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9125e-05 - val_loss: 4.5988e-05\n",
      "Epoch 164/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9119e-05 - val_loss: 4.5984e-05\n",
      "Epoch 165/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9120e-05 - val_loss: 4.5974e-05\n",
      "Epoch 166/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9100e-05 - val_loss: 4.5980e-05\n",
      "Epoch 167/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9104e-05 - val_loss: 4.5964e-05\n",
      "Epoch 168/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9091e-05 - val_loss: 4.6009e-05\n",
      "Epoch 169/600\n",
      "38628/38628 [==============================] - 2s 51us/step - loss: 5.9079e-05 - val_loss: 4.5969e-05\n",
      "Epoch 170/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.9076e-05 - val_loss: 4.5969e-05\n",
      "Epoch 171/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9066e-05 - val_loss: 4.5968e-05\n",
      "Epoch 172/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.9056e-05 - val_loss: 4.5941e-05\n",
      "Epoch 173/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9045e-05 - val_loss: 4.5943e-05\n",
      "Epoch 174/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.9035e-05 - val_loss: 4.5983e-05\n",
      "Epoch 175/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.9023e-05 - val_loss: 4.5965e-05\n",
      "Epoch 176/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.9018e-05 - val_loss: 4.5952e-05\n",
      "Epoch 177/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8997e-05 - val_loss: 4.5926e-05\n",
      "Epoch 178/600\n",
      "38628/38628 [==============================] - 2s 54us/step - loss: 5.8993e-05 - val_loss: 4.5922e-05\n",
      "Epoch 179/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.9004e-05 - val_loss: 4.5948e-05\n",
      "Epoch 180/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8993e-05 - val_loss: 4.5915e-05\n",
      "Epoch 181/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8973e-05 - val_loss: 4.6126e-05\n",
      "Epoch 182/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8977e-05 - val_loss: 4.5915e-05\n",
      "Epoch 183/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8942e-05 - val_loss: 4.5924e-05\n",
      "Epoch 184/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8956e-05 - val_loss: 4.5904e-05\n",
      "Epoch 185/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8953e-05 - val_loss: 4.5907e-05\n",
      "Epoch 186/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8928e-05 - val_loss: 4.5957e-05\n",
      "Epoch 187/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8935e-05 - val_loss: 4.5912e-05\n",
      "Epoch 188/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8925e-05 - val_loss: 4.5902e-05\n",
      "Epoch 189/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8918e-05 - val_loss: 4.5888e-05\n",
      "Epoch 190/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8914e-05 - val_loss: 4.5897e-05\n",
      "Epoch 191/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.8901e-05 - val_loss: 4.5909e-05\n",
      "Epoch 192/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8898e-05 - val_loss: 4.5891e-05\n",
      "Epoch 193/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8869e-05 - val_loss: 4.5945e-05\n",
      "Epoch 194/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8880e-05 - val_loss: 4.5884e-05\n",
      "Epoch 195/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.8869e-05 - val_loss: 4.5965e-05\n",
      "Epoch 196/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8873e-05 - val_loss: 4.5880e-05\n",
      "Epoch 197/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.8861e-05 - val_loss: 4.5865e-05\n",
      "Epoch 198/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8854e-05 - val_loss: 4.5871e-05\n",
      "Epoch 199/600\n",
      "38628/38628 [==============================] - 2s 51us/step - loss: 5.8841e-05 - val_loss: 4.5859e-05\n",
      "Epoch 200/600\n",
      "38628/38628 [==============================] - 2s 50us/step - loss: 5.8826e-05 - val_loss: 4.5857e-05\n",
      "Epoch 201/600\n",
      "38628/38628 [==============================] - 2s 54us/step - loss: 5.8833e-05 - val_loss: 4.5862e-05\n",
      "Epoch 202/600\n",
      "38628/38628 [==============================] - 2s 53us/step - loss: 5.8825e-05 - val_loss: 4.5852e-05\n",
      "Epoch 203/600\n",
      "38628/38628 [==============================] - 3s 66us/step - loss: 5.8815e-05 - val_loss: 4.5855e-05\n",
      "Epoch 204/600\n",
      "38628/38628 [==============================] - 2s 61us/step - loss: 5.8799e-05 - val_loss: 4.5852e-05\n",
      "Epoch 205/600\n",
      "38628/38628 [==============================] - 2s 50us/step - loss: 5.8801e-05 - val_loss: 4.5842e-05\n",
      "Epoch 206/600\n",
      "38628/38628 [==============================] - 2s 53us/step - loss: 5.8790e-05 - val_loss: 4.5904e-05\n",
      "Epoch 207/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.8782e-05 - val_loss: 4.5949e-05\n",
      "Epoch 208/600\n",
      "38628/38628 [==============================] - 2s 50us/step - loss: 5.8776e-05 - val_loss: 4.5837e-05\n",
      "Epoch 209/600\n",
      "38628/38628 [==============================] - 2s 55us/step - loss: 5.8776e-05 - val_loss: 4.5830e-05\n",
      "Epoch 210/600\n",
      "38628/38628 [==============================] - 2s 51us/step - loss: 5.8762e-05 - val_loss: 4.5833e-05\n",
      "Epoch 211/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8764e-05 - val_loss: 4.5826e-05\n",
      "Epoch 212/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8748e-05 - val_loss: 4.5832e-05\n",
      "Epoch 213/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8747e-05 - val_loss: 4.5825e-05\n",
      "Epoch 214/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8732e-05 - val_loss: 4.5816e-05TA:\n",
      "Epoch 215/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8733e-05 - val_loss: 4.5833e-05\n",
      "Epoch 216/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8724e-05 - val_loss: 4.5842e-05\n",
      "Epoch 217/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8718e-05 - val_loss: 4.5826e-05\n",
      "Epoch 218/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8712e-05 - val_loss: 4.5813e-05\n",
      "Epoch 219/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8701e-05 - val_loss: 4.5879e-05\n",
      "Epoch 220/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8704e-05 - val_loss: 4.5805e-05\n",
      "Epoch 221/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8694e-05 - val_loss: 4.5804e-05\n",
      "Epoch 222/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8687e-05 - val_loss: 4.5800e-05\n",
      "Epoch 223/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8680e-05 - val_loss: 4.5815e-05\n",
      "Epoch 224/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8662e-05 - val_loss: 4.5902e-05\n",
      "Epoch 225/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8667e-05 - val_loss: 4.5794e-05\n",
      "Epoch 226/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8664e-05 - val_loss: 4.5793e-05\n",
      "Epoch 227/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8653e-05 - val_loss: 4.5796e-05\n",
      "Epoch 228/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8656e-05 - val_loss: 4.5810e-05\n",
      "Epoch 229/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8645e-05 - val_loss: 4.5808e-05\n",
      "Epoch 230/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8634e-05 - val_loss: 4.5962e-05\n",
      "Epoch 231/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8638e-05 - val_loss: 4.5858e-05\n",
      "Epoch 232/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8626e-05 - val_loss: 4.5784e-05\n",
      "Epoch 233/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8624e-05 - val_loss: 4.5780e-05\n",
      "Epoch 234/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8614e-05 - val_loss: 4.5783e-05\n",
      "Epoch 235/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8611e-05 - val_loss: 4.5821e-05\n",
      "Epoch 236/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8606e-05 - val_loss: 4.5788e-05\n",
      "Epoch 237/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8595e-05 - val_loss: 4.5778e-05\n",
      "Epoch 238/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8591e-05 - val_loss: 4.5773e-05\n",
      "Epoch 239/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8579e-05 - val_loss: 4.5796e-050s - loss: 5.8\n",
      "Epoch 240/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8579e-05 - val_loss: 4.5769e-05\n",
      "Epoch 241/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8575e-05 - val_loss: 4.5783e-05\n",
      "Epoch 242/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8572e-05 - val_loss: 4.5770e-05\n",
      "Epoch 243/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8566e-05 - val_loss: 4.5767e-05\n",
      "Epoch 244/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8550e-05 - val_loss: 4.5856e-05\n",
      "Epoch 245/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8549e-05 - val_loss: 4.5771e-05\n",
      "Epoch 246/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8542e-05 - val_loss: 4.5764e-05\n",
      "Epoch 247/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8536e-05 - val_loss: 4.5788e-05\n",
      "Epoch 248/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8532e-05 - val_loss: 4.5796e-05\n",
      "Epoch 249/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8531e-05 - val_loss: 4.5762e-05\n",
      "Epoch 250/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8523e-05 - val_loss: 4.5757e-05\n",
      "Epoch 251/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8521e-05 - val_loss: 4.5800e-05\n",
      "Epoch 252/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8508e-05 - val_loss: 4.5850e-05\n",
      "Epoch 253/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8510e-05 - val_loss: 4.5765e-05\n",
      "Epoch 254/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8498e-05 - val_loss: 4.5805e-05\n",
      "Epoch 255/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8492e-05 - val_loss: 4.5765e-05\n",
      "Epoch 256/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8494e-05 - val_loss: 4.5753e-05\n",
      "Epoch 257/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8478e-05 - val_loss: 4.5762e-05\n",
      "Epoch 258/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8472e-05 - val_loss: 4.5801e-05\n",
      "Epoch 259/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8475e-05 - val_loss: 4.5748e-05\n",
      "Epoch 260/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8462e-05 - val_loss: 4.5755e-05\n",
      "Epoch 261/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8465e-05 - val_loss: 4.5758e-05\n",
      "Epoch 262/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8459e-05 - val_loss: 4.5744e-05\n",
      "Epoch 263/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8451e-05 - val_loss: 4.5787e-05\n",
      "Epoch 264/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8440e-05 - val_loss: 4.5764e-05\n",
      "Epoch 265/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8431e-05 - val_loss: 4.5823e-05\n",
      "Epoch 266/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8444e-05 - val_loss: 4.5737e-05\n",
      "Epoch 267/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8426e-05 - val_loss: 4.5737e-05\n",
      "Epoch 268/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8428e-05 - val_loss: 4.5744e-05\n",
      "Epoch 269/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8426e-05 - val_loss: 4.5735e-05\n",
      "Epoch 270/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8421e-05 - val_loss: 4.5731e-05\n",
      "Epoch 271/600\n",
      "38628/38628 [==============================] - ETA: 0s - loss: 5.8413e-05- ETA: 0s - - 2s 46us/step - loss: 5.8411e-05 - val_loss: 4.5729e-05\n",
      "Epoch 272/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8405e-05 - val_loss: 4.5731e-05\n",
      "Epoch 273/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8402e-05 - val_loss: 4.5767e-05\n",
      "Epoch 274/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8397e-05 - val_loss: 4.5724e-05\n",
      "Epoch 275/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8392e-05 - val_loss: 4.5729e-05\n",
      "Epoch 276/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8383e-05 - val_loss: 4.5740e-05\n",
      "Epoch 277/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8381e-05 - val_loss: 4.5747e-05\n",
      "Epoch 278/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8367e-05 - val_loss: 4.5718e-05\n",
      "Epoch 279/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8375e-05 - val_loss: 4.5719e-05\n",
      "Epoch 280/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8361e-05 - val_loss: 4.5727e-05\n",
      "Epoch 281/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8368e-05 - val_loss: 4.5725e-05\n",
      "Epoch 282/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8358e-05 - val_loss: 4.5712e-05\n",
      "Epoch 283/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.8356e-05 - val_loss: 4.5709e-05\n",
      "Epoch 284/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8343e-05 - val_loss: 4.5724e-05\n",
      "Epoch 285/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8346e-05 - val_loss: 4.5723e-05\n",
      "Epoch 286/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8346e-05 - val_loss: 4.5707e-05\n",
      "Epoch 287/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8342e-05 - val_loss: 4.5712e-05\n",
      "Epoch 288/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8330e-05 - val_loss: 4.5715e-05s - \n",
      "Epoch 289/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8324e-05 - val_loss: 4.5697e-05\n",
      "Epoch 290/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.8318e-05 - val_loss: 4.5718e-05\n",
      "Epoch 291/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8323e-05 - val_loss: 4.5692e-05\n",
      "Epoch 292/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8314e-05 - val_loss: 4.5691e-05\n",
      "Epoch 293/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8309e-05 - val_loss: 4.5694e-05\n",
      "Epoch 294/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8301e-05 - val_loss: 4.5728e-05\n",
      "Epoch 295/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8298e-05 - val_loss: 4.5685e-05\n",
      "Epoch 296/600\n",
      "38628/38628 [==============================] - 2s 50us/step - loss: 5.8294e-05 - val_loss: 4.5682e-05\n",
      "Epoch 297/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.8285e-05 - val_loss: 4.5716e-05\n",
      "Epoch 298/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8284e-05 - val_loss: 4.5683e-05\n",
      "Epoch 299/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8281e-05 - val_loss: 4.5680e-05\n",
      "Epoch 300/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8281e-05 - val_loss: 4.5681e-05\n",
      "Epoch 301/600\n",
      "38628/38628 [==============================] - 2s 44us/step - loss: 5.8273e-05 - val_loss: 4.5698e-05\n",
      "Epoch 302/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.8259e-05 - val_loss: 4.5675e-05\n",
      "Epoch 303/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.8257e-05 - val_loss: 4.5670e-05\n",
      "Epoch 304/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8263e-05 - val_loss: 4.5672e-05\n",
      "Epoch 305/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8253e-05 - val_loss: 4.5666e-05\n",
      "Epoch 306/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8258e-05 - val_loss: 4.5705e-05\n",
      "Epoch 307/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8246e-05 - val_loss: 4.5666e-05\n",
      "Epoch 308/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.8248e-05 - val_loss: 4.5675e-05\n",
      "Epoch 309/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8237e-05 - val_loss: 4.5661e-05\n",
      "Epoch 310/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8229e-05 - val_loss: 4.5705e-05\n",
      "Epoch 311/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8228e-05 - val_loss: 4.5656e-05\n",
      "Epoch 312/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8229e-05 - val_loss: 4.5717e-05\n",
      "Epoch 313/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8229e-05 - val_loss: 4.5659e-05\n",
      "Epoch 314/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8221e-05 - val_loss: 4.5662e-05\n",
      "Epoch 315/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8212e-05 - val_loss: 4.5681e-05\n",
      "Epoch 316/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8210e-05 - val_loss: 4.5671e-05\n",
      "Epoch 317/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8210e-05 - val_loss: 4.5646e-05\n",
      "Epoch 318/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8201e-05 - val_loss: 4.5665e-05\n",
      "Epoch 319/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8189e-05 - val_loss: 4.5680e-05\n",
      "Epoch 320/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8186e-05 - val_loss: 4.5637e-05\n",
      "Epoch 321/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8184e-05 - val_loss: 4.5635e-05\n",
      "Epoch 322/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8177e-05 - val_loss: 4.5711e-05\n",
      "Epoch 323/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8180e-05 - val_loss: 4.5678e-05\n",
      "Epoch 324/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8180e-05 - val_loss: 4.5656e-05\n",
      "Epoch 325/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8172e-05 - val_loss: 4.5633e-05\n",
      "Epoch 326/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8161e-05 - val_loss: 4.5656e-05\n",
      "Epoch 327/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.8167e-05 - val_loss: 4.5676e-05\n",
      "Epoch 328/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8154e-05 - val_loss: 4.5628e-05\n",
      "Epoch 329/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8153e-05 - val_loss: 4.5625e-05\n",
      "Epoch 330/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8144e-05 - val_loss: 4.5623e-05\n",
      "Epoch 331/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8147e-05 - val_loss: 4.5616e-05\n",
      "Epoch 332/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8142e-05 - val_loss: 4.5620e-05oss:\n",
      "Epoch 333/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8139e-05 - val_loss: 4.5683e-05\n",
      "Epoch 334/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8134e-05 - val_loss: 4.5627e-05\n",
      "Epoch 335/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8131e-05 - val_loss: 4.5629e-05\n",
      "Epoch 336/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8116e-05 - val_loss: 4.5609e-05\n",
      "Epoch 337/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8125e-05 - val_loss: 4.5682e-05\n",
      "Epoch 338/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.8122e-05 - val_loss: 4.5609e-05\n",
      "Epoch 339/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8111e-05 - val_loss: 4.5608e-05\n",
      "Epoch 340/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8109e-05 - val_loss: 4.5598e-05\n",
      "Epoch 341/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8105e-05 - val_loss: 4.5607e-05\n",
      "Epoch 342/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8108e-05 - val_loss: 4.5601e-05\n",
      "Epoch 343/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8080e-05 - val_loss: 4.5590e-05\n",
      "Epoch 344/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8093e-05 - val_loss: 4.5627e-05\n",
      "Epoch 345/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8087e-05 - val_loss: 4.5595e-05\n",
      "Epoch 346/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.8079e-05 - val_loss: 4.5666e-05\n",
      "Epoch 347/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8077e-05 - val_loss: 4.5584e-05\n",
      "Epoch 348/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8079e-05 - val_loss: 4.5584e-05\n",
      "Epoch 349/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8068e-05 - val_loss: 4.5789e-05\n",
      "Epoch 350/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8079e-05 - val_loss: 4.5592e-05\n",
      "Epoch 351/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8066e-05 - val_loss: 4.5580e-05\n",
      "Epoch 352/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.8051e-05 - val_loss: 4.5584e-05\n",
      "Epoch 353/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.8046e-05 - val_loss: 4.5579e-05\n",
      "Epoch 354/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8049e-05 - val_loss: 4.5583e-05\n",
      "Epoch 355/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8049e-05 - val_loss: 4.5573e-05\n",
      "Epoch 356/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8049e-05 - val_loss: 4.5576e-05\n",
      "Epoch 357/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8039e-05 - val_loss: 4.5566e-05\n",
      "Epoch 358/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8039e-05 - val_loss: 4.5565e-05\n",
      "Epoch 359/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8038e-05 - val_loss: 4.5570e-05\n",
      "Epoch 360/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8035e-05 - val_loss: 4.5578e-05\n",
      "Epoch 361/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8025e-05 - val_loss: 4.5590e-05\n",
      "Epoch 362/600\n",
      "38628/38628 [==============================] - 2s 46us/step - loss: 5.8024e-05 - val_loss: 4.5557e-05\n",
      "Epoch 363/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.8024e-05 - val_loss: 4.5565e-05\n",
      "Epoch 364/600\n",
      "38628/38628 [==============================] - 2s 49us/step - loss: 5.8012e-05 - val_loss: 4.5573e-05\n",
      "Epoch 365/600\n",
      "38628/38628 [==============================] - 2s 44us/step - loss: 5.8015e-05 - val_loss: 4.5573e-05\n",
      "Epoch 366/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.8012e-05 - val_loss: 4.5551e-05\n",
      "Epoch 367/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.8002e-05 - val_loss: 4.5550e-05\n",
      "Epoch 368/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.8006e-05 - val_loss: 4.5550e-05\n",
      "Epoch 369/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7988e-05 - val_loss: 4.5651e-05\n",
      "Epoch 370/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7984e-05 - val_loss: 4.5550e-05\n",
      "Epoch 371/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7981e-05 - val_loss: 4.5566e-05\n",
      "Epoch 372/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7987e-05 - val_loss: 4.5555e-05\n",
      "Epoch 373/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7992e-05 - val_loss: 4.5562e-05\n",
      "Epoch 374/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7987e-05 - val_loss: 4.5538e-05\n",
      "Epoch 375/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7979e-05 - val_loss: 4.5553e-05\n",
      "Epoch 376/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7972e-05 - val_loss: 4.5535e-05\n",
      "Epoch 377/600\n",
      "38628/38628 [==============================] - 2s 44us/step - loss: 5.7964e-05 - val_loss: 4.5535e-05\n",
      "Epoch 378/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7972e-05 - val_loss: 4.5554e-05\n",
      "Epoch 379/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7959e-05 - val_loss: 4.5532e-05\n",
      "Epoch 380/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7962e-05 - val_loss: 4.5537e-05\n",
      "Epoch 381/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7959e-05 - val_loss: 4.5533e-05\n",
      "Epoch 382/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7956e-05 - val_loss: 4.5527e-05\n",
      "Epoch 383/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7958e-05 - val_loss: 4.5528e-05\n",
      "Epoch 384/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7945e-05 - val_loss: 4.5628e-05\n",
      "Epoch 385/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7948e-05 - val_loss: 4.5528e-05\n",
      "Epoch 386/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7926e-05 - val_loss: 4.5580e-05\n",
      "Epoch 387/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7932e-05 - val_loss: 4.5566e-05\n",
      "Epoch 388/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7935e-05 - val_loss: 4.5520e-05\n",
      "Epoch 389/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7929e-05 - val_loss: 4.5518e-05\n",
      "Epoch 390/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7928e-05 - val_loss: 4.5527e-05\n",
      "Epoch 391/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7922e-05 - val_loss: 4.5536e-05\n",
      "Epoch 392/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7912e-05 - val_loss: 4.5519e-05\n",
      "Epoch 393/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7912e-05 - val_loss: 4.5520e-05: 0s - loss: 5.78\n",
      "Epoch 394/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7906e-05 - val_loss: 4.5534e-05\n",
      "Epoch 395/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7903e-05 - val_loss: 4.5512e-05\n",
      "Epoch 396/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7906e-05 - val_loss: 4.5775e-05\n",
      "Epoch 397/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7910e-05 - val_loss: 4.5511e-05\n",
      "Epoch 398/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7896e-05 - val_loss: 4.5519e-05\n",
      "Epoch 399/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7896e-05 - val_loss: 4.5508e-05\n",
      "Epoch 400/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7891e-05 - val_loss: 4.5509e-05\n",
      "Epoch 401/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7881e-05 - val_loss: 4.5521e-05\n",
      "Epoch 402/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7877e-05 - val_loss: 4.5596e-05\n",
      "Epoch 403/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7889e-05 - val_loss: 4.5503e-05TA: 0s - loss: 5.8\n",
      "Epoch 404/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7876e-05 - val_loss: 4.5512e-05\n",
      "Epoch 405/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7875e-05 - val_loss: 4.5591e-05\n",
      "Epoch 406/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7870e-05 - val_loss: 4.5516e-05\n",
      "Epoch 407/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7865e-05 - val_loss: 4.5506e-05\n",
      "Epoch 408/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7865e-05 - val_loss: 4.5531e-05\n",
      "Epoch 409/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7856e-05 - val_loss: 4.5562e-05\n",
      "Epoch 410/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7864e-05 - val_loss: 4.5537e-05\n",
      "Epoch 411/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7860e-05 - val_loss: 4.5495e-05\n",
      "Epoch 412/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7852e-05 - val_loss: 4.5491e-05\n",
      "Epoch 413/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7849e-05 - val_loss: 4.5628e-05\n",
      "Epoch 414/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.7846e-05 - val_loss: 4.5496e-05\n",
      "Epoch 415/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7837e-05 - val_loss: 4.5561e-05\n",
      "Epoch 416/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7837e-05 - val_loss: 4.5513e-05\n",
      "Epoch 417/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7839e-05 - val_loss: 4.5515e-05\n",
      "Epoch 418/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7831e-05 - val_loss: 4.5487e-05\n",
      "Epoch 419/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7835e-05 - val_loss: 4.5510e-05\n",
      "Epoch 420/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7825e-05 - val_loss: 4.5481e-05\n",
      "Epoch 421/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7823e-05 - val_loss: 4.5510e-05\n",
      "Epoch 422/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7823e-05 - val_loss: 4.5484e-05\n",
      "Epoch 423/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7820e-05 - val_loss: 4.5477e-05\n",
      "Epoch 424/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7816e-05 - val_loss: 4.5485e-05\n",
      "Epoch 425/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7813e-05 - val_loss: 4.5487e-05\n",
      "Epoch 426/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7805e-05 - val_loss: 4.5540e-05\n",
      "Epoch 427/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7805e-05 - val_loss: 4.5478e-05\n",
      "Epoch 428/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7801e-05 - val_loss: 4.5475e-05\n",
      "Epoch 429/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7798e-05 - val_loss: 4.5498e-05\n",
      "Epoch 430/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7804e-05 - val_loss: 4.5497e-05\n",
      "Epoch 431/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7801e-05 - val_loss: 4.5501e-05\n",
      "Epoch 432/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7795e-05 - val_loss: 4.5476e-05\n",
      "Epoch 433/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7774e-05 - val_loss: 4.5471e-05\n",
      "Epoch 434/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7781e-05 - val_loss: 4.5488e-05\n",
      "Epoch 435/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7779e-05 - val_loss: 4.5568e-05\n",
      "Epoch 436/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.7777e-05 - val_loss: 4.5477e-05\n",
      "Epoch 437/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7772e-05 - val_loss: 4.5533e-05\n",
      "Epoch 438/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7778e-05 - val_loss: 4.5469e-05\n",
      "Epoch 439/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7773e-05 - val_loss: 4.5468e-05\n",
      "Epoch 440/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7758e-05 - val_loss: 4.5466e-05\n",
      "Epoch 441/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7764e-05 - val_loss: 4.5449e-05\n",
      "Epoch 442/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7760e-05 - val_loss: 4.5451e-05\n",
      "Epoch 443/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7755e-05 - val_loss: 4.5447e-05\n",
      "Epoch 444/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7739e-05 - val_loss: 4.5462e-05\n",
      "Epoch 445/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7752e-05 - val_loss: 4.5460e-05\n",
      "Epoch 446/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7751e-05 - val_loss: 4.5452e-05\n",
      "Epoch 447/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7740e-05 - val_loss: 4.5483e-05\n",
      "Epoch 448/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7729e-05 - val_loss: 4.5448e-05\n",
      "Epoch 449/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7743e-05 - val_loss: 4.5478e-05\n",
      "Epoch 450/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7736e-05 - val_loss: 4.5441e-05\n",
      "Epoch 451/600\n",
      "38628/38628 [==============================] - 2s 44us/step - loss: 5.7730e-05 - val_loss: 4.5443e-05\n",
      "Epoch 452/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7730e-05 - val_loss: 4.5449e-05\n",
      "Epoch 453/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7722e-05 - val_loss: 4.5456e-05\n",
      "Epoch 454/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7723e-05 - val_loss: 4.5435e-05\n",
      "Epoch 455/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7716e-05 - val_loss: 4.5488e-05\n",
      "Epoch 456/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7724e-05 - val_loss: 4.5449e-05\n",
      "Epoch 457/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7713e-05 - val_loss: 4.5432e-05\n",
      "Epoch 458/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7712e-05 - val_loss: 4.5434e-05\n",
      "Epoch 459/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7707e-05 - val_loss: 4.5533e-05\n",
      "Epoch 460/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7705e-05 - val_loss: 4.5450e-05\n",
      "Epoch 461/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7702e-05 - val_loss: 4.5428e-05\n",
      "Epoch 462/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7706e-05 - val_loss: 4.5427e-05\n",
      "Epoch 463/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7697e-05 - val_loss: 4.5447e-05\n",
      "Epoch 464/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7696e-05 - val_loss: 4.5462e-05\n",
      "Epoch 465/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7693e-05 - val_loss: 4.5455e-05\n",
      "Epoch 466/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7689e-05 - val_loss: 4.5426e-05\n",
      "Epoch 467/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7684e-05 - val_loss: 4.5583e-05\n",
      "Epoch 468/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7685e-05 - val_loss: 4.5438e-05\n",
      "Epoch 469/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7684e-05 - val_loss: 4.5424e-05\n",
      "Epoch 470/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7673e-05 - val_loss: 4.5431e-05\n",
      "Epoch 471/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7676e-05 - val_loss: 4.5430e-05\n",
      "Epoch 472/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7674e-05 - val_loss: 4.5420e-05\n",
      "Epoch 473/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7674e-05 - val_loss: 4.5452e-05\n",
      "Epoch 474/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7672e-05 - val_loss: 4.5428e-05\n",
      "Epoch 475/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7663e-05 - val_loss: 4.5419e-05\n",
      "Epoch 476/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7661e-05 - val_loss: 4.5466e-05 \n",
      "Epoch 477/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7664e-05 - val_loss: 4.5436e-05\n",
      "Epoch 478/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7663e-05 - val_loss: 4.5420e-05\n",
      "Epoch 479/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7656e-05 - val_loss: 4.5419e-05\n",
      "Epoch 480/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7649e-05 - val_loss: 4.5480e-05\n",
      "Epoch 481/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7651e-05 - val_loss: 4.5411e-05\n",
      "Epoch 482/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7651e-05 - val_loss: 4.5474e-05\n",
      "Epoch 483/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7641e-05 - val_loss: 4.5450e-05\n",
      "Epoch 484/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7646e-05 - val_loss: 4.5408e-05\n",
      "Epoch 485/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7633e-05 - val_loss: 4.5437e-05\n",
      "Epoch 486/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7632e-05 - val_loss: 4.5412e-05\n",
      "Epoch 487/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7636e-05 - val_loss: 4.5411e-05\n",
      "Epoch 488/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7631e-05 - val_loss: 4.5413e-05\n",
      "Epoch 489/600\n",
      "38628/38628 [==============================] - 2s 44us/step - loss: 5.7630e-05 - val_loss: 4.5409e-05\n",
      "Epoch 490/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7633e-05 - val_loss: 4.5408e-05\n",
      "Epoch 491/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7627e-05 - val_loss: 4.5442e-05\n",
      "Epoch 492/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7621e-05 - val_loss: 4.5407e-05\n",
      "Epoch 493/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7623e-05 - val_loss: 4.5409e-05\n",
      "Epoch 494/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7620e-05 - val_loss: 4.5404e-05\n",
      "Epoch 495/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7609e-05 - val_loss: 4.5418e-05\n",
      "Epoch 496/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7612e-05 - val_loss: 4.5405e-05\n",
      "Epoch 497/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7612e-05 - val_loss: 4.5400e-05\n",
      "Epoch 498/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7607e-05 - val_loss: 4.5428e-05\n",
      "Epoch 499/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7604e-05 - val_loss: 4.5444e-05\n",
      "Epoch 500/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7606e-05 - val_loss: 4.5396e-05\n",
      "Epoch 501/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7591e-05 - val_loss: 4.5396e-05\n",
      "Epoch 502/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7598e-05 - val_loss: 4.5410e-05\n",
      "Epoch 503/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7596e-05 - val_loss: 4.5406e-05\n",
      "Epoch 504/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7594e-05 - val_loss: 4.5393e-05\n",
      "Epoch 505/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7595e-05 - val_loss: 4.5392e-05\n",
      "Epoch 506/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7575e-05 - val_loss: 4.5393e-05\n",
      "Epoch 507/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7585e-05 - val_loss: 4.5415e-05\n",
      "Epoch 508/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7581e-05 - val_loss: 4.5390e-05\n",
      "Epoch 509/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7581e-05 - val_loss: 4.5475e-05\n",
      "Epoch 510/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7582e-05 - val_loss: 4.5393e-05\n",
      "Epoch 511/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7573e-05 - val_loss: 4.5403e-05\n",
      "Epoch 512/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7576e-05 - val_loss: 4.5399e-05\n",
      "Epoch 513/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7572e-05 - val_loss: 4.5386e-05\n",
      "Epoch 514/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7568e-05 - val_loss: 4.5386e-05\n",
      "Epoch 515/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7563e-05 - val_loss: 4.5385e-05\n",
      "Epoch 516/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7560e-05 - val_loss: 4.5384e-05\n",
      "Epoch 517/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7554e-05 - val_loss: 4.5383e-05\n",
      "Epoch 518/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7554e-05 - val_loss: 4.5399e-05\n",
      "Epoch 519/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7553e-05 - val_loss: 4.5380e-05\n",
      "Epoch 520/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7557e-05 - val_loss: 4.5383e-05\n",
      "Epoch 521/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7547e-05 - val_loss: 4.5378e-05\n",
      "Epoch 522/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7548e-05 - val_loss: 4.5379e-05\n",
      "Epoch 523/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7546e-05 - val_loss: 4.5378e-05\n",
      "Epoch 524/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7543e-05 - val_loss: 4.5390e-05\n",
      "Epoch 525/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7542e-05 - val_loss: 4.5496e-05\n",
      "Epoch 526/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7544e-05 - val_loss: 4.5381e-05\n",
      "Epoch 527/600\n",
      "38628/38628 [==============================] - 2s 44us/step - loss: 5.7542e-05 - val_loss: 4.5383e-05\n",
      "Epoch 528/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7532e-05 - val_loss: 4.5416e-05\n",
      "Epoch 529/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7532e-05 - val_loss: 4.5386e-05\n",
      "Epoch 530/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7523e-05 - val_loss: 4.5390e-05\n",
      "Epoch 531/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7524e-05 - val_loss: 4.5374e-05\n",
      "Epoch 532/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7520e-05 - val_loss: 4.5385e-05\n",
      "Epoch 533/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7518e-05 - val_loss: 4.5387e-05\n",
      "Epoch 534/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7513e-05 - val_loss: 4.5370e-05\n",
      "Epoch 535/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7503e-05 - val_loss: 4.5370e-05\n",
      "Epoch 536/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7521e-05 - val_loss: 4.5374e-05\n",
      "Epoch 537/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7512e-05 - val_loss: 4.5385e-05\n",
      "Epoch 538/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7513e-05 - val_loss: 4.5411e-05\n",
      "Epoch 539/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7511e-05 - val_loss: 4.5403e-05\n",
      "Epoch 540/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7504e-05 - val_loss: 4.5369e-05\n",
      "Epoch 541/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7500e-05 - val_loss: 4.5449e-05\n",
      "Epoch 542/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7502e-05 - val_loss: 4.5364e-05\n",
      "Epoch 543/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7493e-05 - val_loss: 4.5376e-05\n",
      "Epoch 544/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7485e-05 - val_loss: 4.5397e-05\n",
      "Epoch 545/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7496e-05 - val_loss: 4.5366e-05\n",
      "Epoch 546/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7497e-05 - val_loss: 4.5382e-05\n",
      "Epoch 547/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7487e-05 - val_loss: 4.5404e-05\n",
      "Epoch 548/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7492e-05 - val_loss: 4.5376e-05\n",
      "Epoch 549/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7481e-05 - val_loss: 4.5386e-05\n",
      "Epoch 550/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7481e-05 - val_loss: 4.5422e-05\n",
      "Epoch 551/600\n",
      "38628/38628 [==============================] - 2s 47us/step - loss: 5.7477e-05 - val_loss: 4.5376e-05\n",
      "Epoch 552/600\n",
      "38628/38628 [==============================] - 2s 48us/step - loss: 5.7464e-05 - val_loss: 4.5360e-05\n",
      "Epoch 553/600\n",
      "38628/38628 [==============================] - 2s 45us/step - loss: 5.7466e-05 - val_loss: 4.5358e-05\n",
      "Epoch 554/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7471e-05 - val_loss: 4.5357e-05\n",
      "Epoch 555/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7471e-05 - val_loss: 4.5357e-05\n",
      "Epoch 556/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7471e-05 - val_loss: 4.5429e-05\n",
      "Epoch 557/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7452e-05 - val_loss: 4.5357e-05\n",
      "Epoch 558/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7461e-05 - val_loss: 4.5363e-05\n",
      "Epoch 559/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7454e-05 - val_loss: 4.5354e-05\n",
      "Epoch 560/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7448e-05 - val_loss: 4.5394e-05\n",
      "Epoch 561/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7454e-05 - val_loss: 4.5433e-05\n",
      "Epoch 562/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7451e-05 - val_loss: 4.5351e-05\n",
      "Epoch 563/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7455e-05 - val_loss: 4.5355e-05\n",
      "Epoch 564/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7441e-05 - val_loss: 4.5359e-05\n",
      "Epoch 565/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7441e-05 - val_loss: 4.5375e-05\n",
      "Epoch 566/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7449e-05 - val_loss: 4.5364e-05\n",
      "Epoch 567/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7444e-05 - val_loss: 4.5351e-05\n",
      "Epoch 568/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7440e-05 - val_loss: 4.5357e-05\n",
      "Epoch 569/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7439e-05 - val_loss: 4.5360e-05\n",
      "Epoch 570/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7435e-05 - val_loss: 4.5352e-05\n",
      "Epoch 571/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7425e-05 - val_loss: 4.5357e-05\n",
      "Epoch 572/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7434e-05 - val_loss: 4.5396e-05\n",
      "Epoch 573/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7426e-05 - val_loss: 4.5345e-05\n",
      "Epoch 574/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7428e-05 - val_loss: 4.5382e-05\n",
      "Epoch 575/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7416e-05 - val_loss: 4.5369e-05\n",
      "Epoch 576/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7423e-05 - val_loss: 4.5346e-05\n",
      "Epoch 577/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7413e-05 - val_loss: 4.5418e-05\n",
      "Epoch 578/600\n",
      "38628/38628 [==============================] - 2s 43us/step - loss: 5.7424e-05 - val_loss: 4.5349e-05\n",
      "Epoch 579/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7418e-05 - val_loss: 4.5352e-05\n",
      "Epoch 580/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7403e-05 - val_loss: 4.5356e-05\n",
      "Epoch 581/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7416e-05 - val_loss: 4.5341e-05\n",
      "Epoch 582/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7415e-05 - val_loss: 4.5344e-05\n",
      "Epoch 583/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7407e-05 - val_loss: 4.5407e-05\n",
      "Epoch 584/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7414e-05 - val_loss: 4.5407e-05\n",
      "Epoch 585/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7402e-05 - val_loss: 4.5341e-05\n",
      "Epoch 586/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7391e-05 - val_loss: 4.5347e-05\n",
      "Epoch 587/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7400e-05 - val_loss: 4.5340e-05\n",
      "Epoch 588/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7397e-05 - val_loss: 4.5382e-05\n",
      "Epoch 589/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7401e-05 - val_loss: 4.5338e-05\n",
      "Epoch 590/600\n",
      "38628/38628 [==============================] - 2s 42us/step - loss: 5.7398e-05 - val_loss: 4.5384e-05\n",
      "Epoch 591/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7394e-05 - val_loss: 4.5336e-05\n",
      "Epoch 592/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7395e-05 - val_loss: 4.5340e-05\n",
      "Epoch 593/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7392e-05 - val_loss: 4.5372e-05\n",
      "Epoch 594/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7384e-05 - val_loss: 4.5337e-05\n",
      "Epoch 595/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7386e-05 - val_loss: 4.5351e-05\n",
      "Epoch 596/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7377e-05 - val_loss: 4.5341e-05\n",
      "Epoch 597/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7373e-05 - val_loss: 4.5337e-05\n",
      "Epoch 598/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7380e-05 - val_loss: 4.5333e-05\n",
      "Epoch 599/600\n",
      "38628/38628 [==============================] - 2s 40us/step - loss: 5.7369e-05 - val_loss: 4.5351e-05\n",
      "Epoch 600/600\n",
      "38628/38628 [==============================] - 2s 41us/step - loss: 5.7371e-05 - val_loss: 4.5348e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=60, validation_split = 0.2, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00285213  0.01091266  0.01977928  0.03131202  0.0710565\n",
      "  0.0396825   0.04352681  0.05152528  0.05102924  0.05468745  0.11024305\n",
      "  0.08500739  0.0557416   0.04185268  0.01773314  0.02387157  0.00124006\n",
      " -0.02368556 -0.02337552 -0.01444694 -0.01109876 -0.00062008 -0.03478423\n",
      " -0.02752978 -0.0685144  -0.05016119 -0.0425967  -0.04600693 -0.01339288\n",
      " -0.00998264  0.01283477  0.00564238  0.01314481 -0.0048363  -0.03856648\n",
      " -0.04222469 -0.02430554 -0.03218007 -0.03131202 -0.0634921  -0.08172128\n",
      " -0.08866569 -0.0555556  -0.0476191  -0.05022325 -0.06684027 -0.05295144\n",
      " -0.02876984 -0.01209075 -0.03335817 -0.06063987 -0.0251736  -0.07434279\n",
      " -0.07831104 -0.07279269 -0.07378468 -0.09926832 -0.09883434 -0.06777039\n",
      " -0.08085323 -0.05158735 -0.06479413 -0.11377732]\n",
      "[0.9963]\n",
      "[1.0015441]\n"
     ]
    }
   ],
   "source": [
    "print test_data[0]\n",
    "print test_labels[0]\n",
    "print outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primitive policy temporarily in place of a RL policy network\n",
    "\n",
    "# signal step for our policy network\n",
    "signals = map(lambda arr: sum(arr) / len(arr),outputs)\n",
    "# signals = map(lambda arr: arr[0],outputs)\n",
    "trades = map(lambda signal: 1 if round(signal,4) > 1 else 0,signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one day return\n",
    "actual = map(lambda arr: arr[0],test_labels)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'signal':signals,\n",
    "    'actual': actual,\n",
    "    'trade':trades,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['entry_success'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['entry_failure'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['avoid_success'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 0) else 0,axis=1)\n",
    "df['avoid_failure'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 0) else 0,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>signal</th>\n",
       "      <th>trade</th>\n",
       "      <th>entry_success</th>\n",
       "      <th>entry_failure</th>\n",
       "      <th>avoid_success</th>\n",
       "      <th>avoid_failure</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9963</td>\n",
       "      <td>1.001544</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0029</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0081</td>\n",
       "      <td>1.002632</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0090</td>\n",
       "      <td>1.002150</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0119</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0428</td>\n",
       "      <td>0.998876</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9673</td>\n",
       "      <td>1.001314</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0040</td>\n",
       "      <td>1.001779</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0084</td>\n",
       "      <td>1.003031</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.999902</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0039</td>\n",
       "      <td>1.003340</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0624</td>\n",
       "      <td>1.008765</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.9724</td>\n",
       "      <td>1.008437</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.9690</td>\n",
       "      <td>1.006463</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.001201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.999175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0063</td>\n",
       "      <td>1.000371</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9773</td>\n",
       "      <td>1.000047</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.997558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0003</td>\n",
       "      <td>0.998098</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0088</td>\n",
       "      <td>0.999723</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0033</td>\n",
       "      <td>1.000676</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0105</td>\n",
       "      <td>0.998929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.9670</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0071</td>\n",
       "      <td>0.998134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9616</td>\n",
       "      <td>0.999039</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0175</td>\n",
       "      <td>0.998341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0073</td>\n",
       "      <td>0.997710</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0322</td>\n",
       "      <td>1.003770</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11910</th>\n",
       "      <td>0.9898</td>\n",
       "      <td>1.001106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11911</th>\n",
       "      <td>1.0190</td>\n",
       "      <td>1.001271</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11912</th>\n",
       "      <td>0.9935</td>\n",
       "      <td>1.000874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11913</th>\n",
       "      <td>0.9982</td>\n",
       "      <td>1.002886</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11914</th>\n",
       "      <td>1.0018</td>\n",
       "      <td>1.000020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11915</th>\n",
       "      <td>0.9700</td>\n",
       "      <td>1.003114</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11916</th>\n",
       "      <td>1.0079</td>\n",
       "      <td>1.002077</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>0.9944</td>\n",
       "      <td>1.000962</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11918</th>\n",
       "      <td>1.0061</td>\n",
       "      <td>1.000233</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11919</th>\n",
       "      <td>0.9808</td>\n",
       "      <td>1.001595</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11920</th>\n",
       "      <td>0.9989</td>\n",
       "      <td>1.001174</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11921</th>\n",
       "      <td>0.9934</td>\n",
       "      <td>1.000225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11922</th>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.997856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11923</th>\n",
       "      <td>1.0074</td>\n",
       "      <td>0.998386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11924</th>\n",
       "      <td>1.0068</td>\n",
       "      <td>1.001107</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11925</th>\n",
       "      <td>1.0002</td>\n",
       "      <td>0.999838</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11926</th>\n",
       "      <td>0.9887</td>\n",
       "      <td>1.000291</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11927</th>\n",
       "      <td>1.0237</td>\n",
       "      <td>1.003596</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11928</th>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.003003</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11929</th>\n",
       "      <td>0.9847</td>\n",
       "      <td>1.001978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11930</th>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11931</th>\n",
       "      <td>0.9783</td>\n",
       "      <td>0.998693</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11932</th>\n",
       "      <td>1.0206</td>\n",
       "      <td>1.000637</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11933</th>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.999088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11934</th>\n",
       "      <td>0.9588</td>\n",
       "      <td>1.000176</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11935</th>\n",
       "      <td>1.0091</td>\n",
       "      <td>1.002002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11936</th>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11937</th>\n",
       "      <td>1.0037</td>\n",
       "      <td>0.999344</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11938</th>\n",
       "      <td>1.0043</td>\n",
       "      <td>0.999273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11939</th>\n",
       "      <td>0.9779</td>\n",
       "      <td>1.000145</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11940 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual    signal  trade  entry_success  entry_failure  avoid_success  \\\n",
       "0      0.9963  1.001544      1              0              1              0   \n",
       "1      1.0029  0.999394      0              0              0              0   \n",
       "2      1.0081  1.002632      1              1              0              0   \n",
       "3      1.0090  1.002150      1              1              0              0   \n",
       "4      1.0119  0.999772      0              0              0              0   \n",
       "5      1.0428  0.998876      0              0              0              0   \n",
       "6      0.9673  1.001314      1              0              1              0   \n",
       "7      1.0040  1.001779      1              1              0              0   \n",
       "8      1.0084  1.003031      1              1              0              0   \n",
       "9      0.9995  0.999902      0              0              0              1   \n",
       "10     1.0039  1.003340      1              1              0              0   \n",
       "11     1.0624  1.008765      1              1              0              0   \n",
       "12     0.9724  1.008437      1              0              1              0   \n",
       "13     0.9690  1.006463      1              0              1              0   \n",
       "14     0.9855  1.001201      1              0              1              0   \n",
       "15     0.9754  0.999175      0              0              0              1   \n",
       "16     1.0063  1.000371      1              1              0              0   \n",
       "17     0.9773  1.000047      0              0              0              1   \n",
       "18     0.9757  0.997558      0              0              0              1   \n",
       "19     1.0003  0.998098      0              0              0              0   \n",
       "20     1.0088  0.999723      0              0              0              0   \n",
       "21     1.0033  1.000676      1              1              0              0   \n",
       "22     1.0105  0.998929      0              0              0              0   \n",
       "23     0.9670  0.998857      0              0              0              1   \n",
       "24     1.0071  0.998134      0              0              0              0   \n",
       "25     0.9616  0.999039      0              0              0              1   \n",
       "26     1.0175  0.998341      0              0              0              0   \n",
       "27     1.0073  0.997710      0              0              0              0   \n",
       "28     0.9967  0.999798      0              0              0              1   \n",
       "29     1.0322  1.003770      1              1              0              0   \n",
       "...       ...       ...    ...            ...            ...            ...   \n",
       "11910  0.9898  1.001106      1              0              1              0   \n",
       "11911  1.0190  1.001271      1              1              0              0   \n",
       "11912  0.9935  1.000874      1              0              1              0   \n",
       "11913  0.9982  1.002886      1              0              1              0   \n",
       "11914  1.0018  1.000020      0              0              0              0   \n",
       "11915  0.9700  1.003114      1              0              1              0   \n",
       "11916  1.0079  1.002077      1              1              0              0   \n",
       "11917  0.9944  1.000962      1              0              1              0   \n",
       "11918  1.0061  1.000233      1              1              0              0   \n",
       "11919  0.9808  1.001595      1              0              1              0   \n",
       "11920  0.9989  1.001174      1              0              1              0   \n",
       "11921  0.9934  1.000225      1              0              1              0   \n",
       "11922  0.9836  0.997856      0              0              0              1   \n",
       "11923  1.0074  0.998386      0              0              0              0   \n",
       "11924  1.0068  1.001107      1              1              0              0   \n",
       "11925  1.0002  0.999838      0              0              0              0   \n",
       "11926  0.9887  1.000291      1              0              1              0   \n",
       "11927  1.0237  1.003596      1              1              0              0   \n",
       "11928  0.9991  1.003003      1              0              1              0   \n",
       "11929  0.9847  1.001978      1              0              1              0   \n",
       "11930  0.9954  0.999982      0              0              0              1   \n",
       "11931  0.9783  0.998693      0              0              0              1   \n",
       "11932  1.0206  1.000637      1              1              0              0   \n",
       "11933  0.9912  0.999088      0              0              0              1   \n",
       "11934  0.9588  1.000176      1              0              1              0   \n",
       "11935  1.0091  1.002002      1              1              0              0   \n",
       "11936  0.9879  0.999656      0              0              0              1   \n",
       "11937  1.0037  0.999344      0              0              0              0   \n",
       "11938  1.0043  0.999273      0              0              0              0   \n",
       "11939  0.9779  1.000145      1              0              1              0   \n",
       "\n",
       "       avoid_failure  success  \n",
       "0                  0        0  \n",
       "1                  1        1  \n",
       "2                  0        1  \n",
       "3                  0        1  \n",
       "4                  1        1  \n",
       "5                  1        1  \n",
       "6                  0        0  \n",
       "7                  0        1  \n",
       "8                  0        1  \n",
       "9                  0        1  \n",
       "10                 0        1  \n",
       "11                 0        1  \n",
       "12                 0        0  \n",
       "13                 0        0  \n",
       "14                 0        0  \n",
       "15                 0        1  \n",
       "16                 0        1  \n",
       "17                 0        1  \n",
       "18                 0        1  \n",
       "19                 1        1  \n",
       "20                 1        1  \n",
       "21                 0        1  \n",
       "22                 1        1  \n",
       "23                 0        1  \n",
       "24                 1        1  \n",
       "25                 0        1  \n",
       "26                 1        1  \n",
       "27                 1        1  \n",
       "28                 0        1  \n",
       "29                 0        1  \n",
       "...              ...      ...  \n",
       "11910              0        0  \n",
       "11911              0        1  \n",
       "11912              0        0  \n",
       "11913              0        0  \n",
       "11914              1        1  \n",
       "11915              0        0  \n",
       "11916              0        1  \n",
       "11917              0        0  \n",
       "11918              0        1  \n",
       "11919              0        0  \n",
       "11920              0        0  \n",
       "11921              0        0  \n",
       "11922              0        1  \n",
       "11923              1        1  \n",
       "11924              0        1  \n",
       "11925              1        1  \n",
       "11926              0        0  \n",
       "11927              0        1  \n",
       "11928              0        0  \n",
       "11929              0        0  \n",
       "11930              0        1  \n",
       "11931              0        1  \n",
       "11932              0        1  \n",
       "11933              0        1  \n",
       "11934              0        0  \n",
       "11935              0        1  \n",
       "11936              0        1  \n",
       "11937              1        1  \n",
       "11938              1        1  \n",
       "11939              0        0  \n",
       "\n",
       "[11940 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primitive policy - replace with a policy network which maximizes reward\n",
    "def label_success (row):\n",
    "    return 0 if (row['entry_failure'] == 1 or row['entry_failure'] == 1) else 1\n",
    "\n",
    "success = df.apply (lambda row: label_success (row),axis=1)\n",
    "df['success'] = success;\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022550472959340324\n",
      "0.019040744790196962\n",
      "0.4823471019910659\n",
      "0.48413551831602264\n"
     ]
    }
   ],
   "source": [
    "print df['actual'].corr(df['signal'])\n",
    "print df['actual'].corr(df['trade'])\n",
    "print df['actual'].corr(df['entry_success'])\n",
    "print df['actual'].corr(df['success'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11940.00000\n",
       "mean         1.00027\n",
       "std          0.01017\n",
       "min          0.94640\n",
       "25%          0.99580\n",
       "50%          1.00060\n",
       "75%          1.00550\n",
       "max          1.06240\n",
       "Name: actual, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['actual'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11940.000000\n",
       "mean         1.000431\n",
       "std          0.001408\n",
       "min          0.993326\n",
       "25%          0.999669\n",
       "50%          1.000387\n",
       "75%          1.001119\n",
       "max          1.015006\n",
       "Name: signal, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['signal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11940\n",
      "\n",
      "Precision\n",
      "0.541402989094\n",
      "\n",
      "Recall\n",
      "0.634527378886\n",
      "\n",
      "Accuracy\n",
      "0.336767169179\n",
      "\n",
      "Non-loss events\n",
      "8534\n",
      "0.714740368509\n",
      "\n",
      "Lose trades\n",
      "3406\n",
      "0.285259631491\n",
      "\n",
      "Win trades\n",
      "4021\n",
      "0.336767169179\n",
      "\n",
      "Missed opportunities\n",
      "2316\n",
      "0.193969849246\n",
      "\n",
      "Bullets dodged\n",
      "2101\n",
      "0.175963149079\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST_SAMPLES = len(test_data)\n",
    "print NUM_TEST_SAMPLES\n",
    "\n",
    "print '\\nPrecision' # optimize for this since we can increase discovery, so long as we find enough trades\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['entry_failure'])) \n",
    "\n",
    "print '\\nRecall'\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['avoid_failure']))\n",
    "\n",
    "print '\\nAccuracy'\n",
    "print sum(df['entry_success']) * 1.00 / (NUM_TEST_SAMPLES)\n",
    "\n",
    "print '\\nNon-loss events'\n",
    "print sum(df['success'])\n",
    "print sum(df['success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nLose trades'\n",
    "print sum(df['entry_failure'])\n",
    "print sum(df['entry_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nWin trades'\n",
    "print sum(df['entry_success'])\n",
    "print sum(df['entry_success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nMissed opportunities'\n",
    "print sum(df['avoid_failure'])\n",
    "print sum(df['avoid_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nBullets dodged'\n",
    "print sum(df['avoid_success'])\n",
    "print sum(df['avoid_success']) / (NUM_TEST_SAMPLES * 1.00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.02081959, -0.01544686, -0.01007412, -0.02014825,\n",
       "        -0.00470139, -0.00335821, -0.00268637,  0.00268637, -0.03022188,\n",
       "        -0.02552098, -0.03022188, -0.01074547,  0.0047009 ,  0.01947592,\n",
       "         0.03828049,  0.02350547,  0.02350547,  0.03156457,  0.02350547,\n",
       "         0.0288782 ,  0.03022139,  0.03962367,  0.05708506,  0.06245779,\n",
       "         0.06245779,  0.07186008,  0.0705169 ,  0.04902596,  0.04365322,\n",
       "         0.01678955,  0.00335771,  0.02216228,  0.01947592,  0.02753502,\n",
       "         0.04768277,  0.05439869,  0.06380098,  0.06111461,  0.06648735,\n",
       "         0.05708506,  0.06514416,  0.06917371,  0.06447282,  0.0705169 ,\n",
       "         0.07991918,  0.08932147,  0.06917371,  0.03559412,  0.0174609 ,\n",
       "         0.00335771,  0.01410318, -0.04231053, -0.05708555, -0.053056  ,\n",
       "        -0.0449969 , -0.03962416, -0.06648784, -0.07991967, -0.06514465,\n",
       "        -0.07186057, -0.053056  , -0.04768327, -0.05036963]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_data = np.array(convert_to_train(XLE.copy(), 0)[0][:1])\n",
    "today_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0034628]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_prediction = model.predict(today_data)\n",
    "future_prediction # [1 day prediction, 5 day prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
