{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "\n",
    "Creates a neural network which evaluates a time series and produce a set of predicted values for the time series\n",
    "Predicted values may be used in a policy to make a trade. This policy may be modeled by simple multiple regression or a neural network.\n",
    "\n",
    "## Data\n",
    "Test data is taken as most recent to avoid lookahead bias. Train data is split into a validation and training set during fitting.\n",
    "\n",
    "\n",
    "## TODO\n",
    "- Convert feature percentages to stdev\n",
    "- Adding VIX as a signal\n",
    "- Adding High/Low as signals\n",
    "- Multiple securities/ aggregate samples\n",
    "- Policy network\n",
    "- Regularization (l2)\n",
    "- Dilated convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "NUM_INPUT_NEURONS = 64\n",
    "NUM_OUTPUT_NEURONS = 1\n",
    "DAY_OFFSET = 5\n",
    "\n",
    "def from_network(symbol):\n",
    "    return pdr.get_data_yahoo(symbols=symbol, start=datetime(1900, 1, 1))\n",
    "\n",
    "def from_file(symbol):\n",
    "    dataset_path = keras.utils.get_file(\"{}.csv\".format(symbol), \"http://localhost:8000/data/daily/{}.csv\".format(symbol))\n",
    "    column_names = ['Date','Open','High','Low','Close','Adj Close','Volume'] \n",
    "    return pd.read_csv(dataset_path, \n",
    "                              names=column_names, \n",
    "                              dtype={'Close': np.float64,'Open': np.float64,'High': np.float64,'Adj Close': np.float64, 'Volume': np.float64},\n",
    "                              header=0,\n",
    "                              na_values = \"?\", \n",
    "                              comment='\\t',\n",
    "                              sep=\",\",\n",
    "                              skipinitialspace=True)\n",
    "\n",
    "\n",
    "# Create features (only close price for now)\n",
    "def convert_to_percentage(old, new):\n",
    "    return (old - new) / old\n",
    "\n",
    "def convert_labels_to_category(labels): \n",
    "    # Simplification - If positive return, 1, else 0\n",
    "    # return map(lambda arr: 1 if arr[0] > 1 else 0, labels)\n",
    "    # rounding simpliciation\n",
    "    return map(lambda arr: map(lambda val: round(val,4),arr), labels)\n",
    "\n",
    "def convert_to_train(raw_dataset, offset=5):\n",
    "    dataset = raw_dataset.copy()\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(offset, len(dataset) - NUM_INPUT_NEURONS):\n",
    "\n",
    "        feature_dataset = dataset[i:i+NUM_INPUT_NEURONS].copy()\n",
    "        latest_close = feature_dataset['Close'].iloc[0]\n",
    "        \n",
    "        features.append(\n",
    "            feature_dataset['Close']\n",
    "                .map(lambda current: convert_to_percentage(latest_close, current))\n",
    "                .tolist()\n",
    "        )\n",
    "        labels.append([\n",
    "            dataset['Close'].iloc[i-1] / latest_close, # 1 day trade\n",
    "#             dataset['Close'].iloc[i-2] / latest_close, # 2 day trade\n",
    "#             dataset['Close'].iloc[i-3] / latest_close, # 3 day trade\n",
    "#             dataset['Close'].iloc[i-4] / latest_close, # 4 day trade\n",
    "#             dataset['Close'].iloc[i-5] / latest_close, # 5 day trade\n",
    "        ])\n",
    "        \n",
    "    # Without converting labels the precision is hard to determine accuracy. \n",
    "    # Rather than crude 0/1, maybe this can be more sophisticated\n",
    "    labels = convert_labels_to_category(labels)\n",
    "    \n",
    "    return [features,labels]\n",
    "def split_data(symbol):\n",
    "    fetched = from_network(symbol).sort_values(by=['Date'],ascending=False)\n",
    "    converted = convert_to_train(fetched)\n",
    "    features = converted[0]\n",
    "    labels = converted[1]\n",
    "    training = [ # since we decide a model, use all data for training\n",
    "        features[1:],\n",
    "        labels[1:]\n",
    "    ]\n",
    "    validation = [\n",
    "        features[5:1000],\n",
    "        labels[5:1000]\n",
    "    ]\n",
    "    prediction = [\n",
    "        features[:5],\n",
    "        labels[:5],\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'prediction': prediction,\n",
    "        'validation': validation,\n",
    "        'training': training,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 48)                3120      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                888       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 14,405\n",
      "Trainable params: 14,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.nn.relu, input_shape=[NUM_INPUT_NEURONS]),\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dense(48, activation=tf.nn.relu),\n",
    "    layers.Dense(36, activation=tf.nn.relu),\n",
    "    layers.Dense(24, activation=tf.nn.relu),\n",
    "    layers.Dense(12, activation=tf.nn.relu),\n",
    "\n",
    "      \n",
    "#     layers.Dense(64, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(48, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(36, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(24, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(12, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "    layers.Dense(NUM_OUTPUT_NEURONS)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_squared_logarithmic_error',\n",
    "                optimizer='sgd',\n",
    "#                 metrics=[\n",
    "#                     'mae',\n",
    "#                 ]\n",
    "               )\n",
    "  return model\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset = raw_dataset.copy()\n",
    "#dataset = from_network('SPY').sort_values(by=['Date'],ascending=False)\n",
    "# add function to cache fetch\n",
    "QQQ = from_network('QQQ')\n",
    "SPY = from_network('SPY')\n",
    "XLK = from_network('XLK')\n",
    "XLF = from_network('XLF')\n",
    "XLE = from_network('XLE')\n",
    "XLP = from_network('XLP')\n",
    "XLV = from_network('XLV')\n",
    "XLY = from_network('XLY')\n",
    "XLI = from_network('XLI')\n",
    "XLU = from_network('XLU')\n",
    "\n",
    "#dataset = pd.concat([QQQ,SPY,XLK,XLF,XLE,XLP,XLV,XLY,XLI,XLU]).sort_values(by=['Date'],ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepped_data = map(split_data, [\n",
    "    'QQQ',\n",
    "    'SPY',\n",
    "    'IWM',\n",
    "    'VTI',\n",
    "    'XLK',\n",
    "    'XLF',\n",
    "    'XLE',\n",
    "    'XLP',\n",
    "    'XLV',\n",
    "    'XLY',\n",
    "    'XLI',\n",
    "    'XLU',\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQQ\n",
      "SPY\n",
      "IWM\n",
      "VTI\n",
      "XLK\n",
      "XLF\n",
      "XLE\n",
      "XLP\n",
      "XLV\n",
      "XLY\n",
      "XLI\n",
      "XLU\n",
      "0\n",
      "4834\n",
      "11210\n",
      "15736\n",
      "19997\n",
      "24883\n",
      "29769\n",
      "34655\n",
      "39541\n",
      "44427\n",
      "49313\n",
      "54199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(prepped_data)):\n",
    "    print prepped_data[i]['symbol']\n",
    "    \n",
    "def combine_all(accum, prep):\n",
    "    \n",
    "    print len(accum['training'][0])\n",
    "    return {   \n",
    "        'prediction':[\n",
    "            accum['prediction'][0] + prep['prediction'][0],\n",
    "            accum['prediction'][1] + prep['prediction'][1],\n",
    "        ],\n",
    "        'validation':[\n",
    "            accum['validation'][0] + prep['validation'][0],\n",
    "            accum['validation'][1] + prep['validation'][1],\n",
    "        ],\n",
    "        'training':[\n",
    "            accum['training'][0] + prep['training'][0],\n",
    "            accum['training'][1] + prep['training'][1],\n",
    "        ],\n",
    "    }\n",
    "combined = reduce(combine_all, prepped_data,{\n",
    "    'prediction':[[],[]],\n",
    "    'validation':[[],[]],\n",
    "    'training':[[],[]],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59085"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "len(combined['training'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59085\n",
      "11940\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "#len(converted_feature_set[0][0])\n",
    "print len(combined['training'][0])\n",
    "train_data = np.array(combined['training'][0])\n",
    "train_labels = np.array(combined['training'][1])\n",
    "\n",
    "print len(combined['validation'][0])\n",
    "test_data = np.array(combined['validation'][0])\n",
    "test_labels = np.array(combined['validation'][1])\n",
    "\n",
    "print len(combined['prediction'][0])\n",
    "prediction_data = np.array(combined['prediction'][0])\n",
    "prediction_labels = np.array(combined['prediction'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.00143802  0.00243364 ...  0.06543142  0.06128319\n",
      "   0.06222345]\n",
      " [ 0.          0.0038661   0.00751133 ...  0.06263115  0.06357006\n",
      "   0.06329389]\n",
      " [ 0.          0.00365937  0.00277223 ...  0.05993567  0.05965843\n",
      "   0.06753165]\n",
      " ...\n",
      " [ 0.         -0.01635514 -0.02161122 ... -0.13434394 -0.14368973\n",
      "  -0.12499815]\n",
      " [ 0.         -0.0051715  -0.0183908  ... -0.12528553 -0.10689473\n",
      "  -0.11264368]\n",
      " [ 0.         -0.01315129 -0.01658001 ... -0.10119987 -0.10691924\n",
      "  -0.08690688]]\n",
      "[[1.0093]\n",
      " [0.9986]\n",
      " [1.0039]\n",
      " ...\n",
      " [0.9977]\n",
      " [0.9839]\n",
      " [0.9949]]\n"
     ]
    }
   ],
   "source": [
    "print train_data\n",
    "print train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58789 samples, validate on 296 samples\n",
      "Epoch 1/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.3129e-05 - val_loss: 4.8547e-05\n",
      "Epoch 2/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.3104e-05 - val_loss: 4.8559e-05\n",
      "Epoch 3/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.3089e-05 - val_loss: 4.8554e-05\n",
      "Epoch 4/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.3051e-05 - val_loss: 4.8522e-05\n",
      "Epoch 5/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.3033e-05 - val_loss: 4.8502e-05\n",
      "Epoch 6/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.3014e-05 - val_loss: 4.8492e-05\n",
      "Epoch 7/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2986e-05 - val_loss: 4.8494e-05\n",
      "Epoch 8/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2974e-05 - val_loss: 4.8655e-05\n",
      "Epoch 9/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.2959e-05 - val_loss: 4.8552e-05\n",
      "Epoch 10/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2922e-05 - val_loss: 4.8451e-05\n",
      "Epoch 11/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2903e-05 - val_loss: 4.8421e-05\n",
      "Epoch 12/150\n",
      "58789/58789 [==============================] - 3s 43us/step - loss: 5.2890e-05 - val_loss: 4.8427e-05\n",
      "Epoch 13/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2860e-05 - val_loss: 4.8380e-05\n",
      "Epoch 14/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2841e-05 - val_loss: 4.8434e-05\n",
      "Epoch 15/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2798e-05 - val_loss: 4.8395e-05\n",
      "Epoch 16/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.2809e-05 - val_loss: 4.8446e-05\n",
      "Epoch 17/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2786e-05 - val_loss: 4.8424e-05\n",
      "Epoch 18/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.2769e-05 - val_loss: 4.8326e-05\n",
      "Epoch 19/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2743e-05 - val_loss: 4.8314e-05\n",
      "Epoch 20/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.2724e-05 - val_loss: 4.8306e-05\n",
      "Epoch 21/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.2699e-05 - val_loss: 4.8304e-05\n",
      "Epoch 22/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.2679e-05 - val_loss: 4.8294e-05\n",
      "Epoch 23/150\n",
      "58789/58789 [==============================] - 3s 49us/step - loss: 5.2657e-05 - val_loss: 4.8545e-05\n",
      "Epoch 24/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.2650e-05 - val_loss: 4.8308e-05\n",
      "Epoch 25/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2627e-05 - val_loss: 4.8249e-05\n",
      "Epoch 26/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.2609e-05 - val_loss: 4.8235e-05\n",
      "Epoch 27/150\n",
      "58789/58789 [==============================] - 3s 49us/step - loss: 5.2583e-05 - val_loss: 4.8223e-05\n",
      "Epoch 28/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.2573e-05 - val_loss: 4.8304e-05\n",
      "Epoch 29/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.2559e-05 - val_loss: 4.8281e-05\n",
      "Epoch 30/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2534e-05 - val_loss: 4.8232e-05\n",
      "Epoch 31/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2519e-05 - val_loss: 4.8251e-05\n",
      "Epoch 32/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2495e-05 - val_loss: 4.8195e-05\n",
      "Epoch 33/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2479e-05 - val_loss: 4.8185e-05\n",
      "Epoch 34/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2465e-05 - val_loss: 4.8400e-05\n",
      "Epoch 35/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.2439e-05 - val_loss: 4.8188e-05\n",
      "Epoch 36/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.2425e-05 - val_loss: 4.8168e-05\n",
      "Epoch 37/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2420e-05 - val_loss: 4.8167e-05\n",
      "Epoch 38/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2398e-05 - val_loss: 4.8190e-05\n",
      "Epoch 39/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2381e-05 - val_loss: 4.8307e-05\n",
      "Epoch 40/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2367e-05 - val_loss: 4.8120e-05\n",
      "Epoch 41/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2358e-05 - val_loss: 4.8298e-05\n",
      "Epoch 42/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2327e-05 - val_loss: 4.8115e-05\n",
      "Epoch 43/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.2320e-05 - val_loss: 4.8100e-05\n",
      "Epoch 44/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.2310e-05 - val_loss: 4.8095e-05\n",
      "Epoch 45/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2291e-05 - val_loss: 4.8142e-05\n",
      "Epoch 46/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2274e-05 - val_loss: 4.8152e-05\n",
      "Epoch 47/150\n",
      "58789/58789 [==============================] - 3s 43us/step - loss: 5.2262e-05 - val_loss: 4.8078e-05\n",
      "Epoch 48/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.2252e-05 - val_loss: 4.8086e-05\n",
      "Epoch 49/150\n",
      "58789/58789 [==============================] - 3s 49us/step - loss: 5.2221e-05 - val_loss: 4.8133e-05\n",
      "Epoch 50/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.2197e-05 - val_loss: 4.8119e-05\n",
      "Epoch 51/150\n",
      "58789/58789 [==============================] - 3s 50us/step - loss: 5.2192e-05 - val_loss: 4.8072e-05\n",
      "Epoch 52/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.2185e-05 - val_loss: 4.8074e-05\n",
      "Epoch 53/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2169e-05 - val_loss: 4.8083e-05\n",
      "Epoch 54/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2151e-05 - val_loss: 4.8055e-05\n",
      "Epoch 55/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.2136e-05 - val_loss: 4.8153e-05\n",
      "Epoch 56/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2118e-05 - val_loss: 4.8037e-05\n",
      "Epoch 57/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2106e-05 - val_loss: 4.8159e-05\n",
      "Epoch 58/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2088e-05 - val_loss: 4.8032e-05\n",
      "Epoch 59/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.2083e-05 - val_loss: 4.8026e-05\n",
      "Epoch 60/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2063e-05 - val_loss: 4.8010e-05\n",
      "Epoch 61/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2052e-05 - val_loss: 4.8021e-05\n",
      "Epoch 62/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.2034e-05 - val_loss: 4.8129e-05\n",
      "Epoch 63/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.2021e-05 - val_loss: 4.8088e-05\n",
      "Epoch 64/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.2003e-05 - val_loss: 4.8023e-05\n",
      "Epoch 65/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.2000e-05 - val_loss: 4.7987e-05\n",
      "Epoch 66/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.1984e-05 - val_loss: 4.8136e-05\n",
      "Epoch 67/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1981e-05 - val_loss: 4.8003e-05\n",
      "Epoch 68/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1951e-05 - val_loss: 4.8281e-05\n",
      "Epoch 69/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1936e-05 - val_loss: 4.7996e-05\n",
      "Epoch 70/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1928e-05 - val_loss: 4.7973e-05\n",
      "Epoch 71/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1915e-05 - val_loss: 4.8010e-05\n",
      "Epoch 72/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1902e-05 - val_loss: 4.8043e-05\n",
      "Epoch 73/150\n",
      "58789/58789 [==============================] - 3s 52us/step - loss: 5.1877e-05 - val_loss: 4.8000e-051s - loss: 5. - ETA: 0s - \n",
      "Epoch 74/150\n",
      "58789/58789 [==============================] - 3s 52us/step - loss: 5.1883e-05 - val_loss: 4.7958e-05\n",
      "Epoch 75/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.1871e-05 - val_loss: 4.7997e-05\n",
      "Epoch 76/150\n",
      "58789/58789 [==============================] - 3s 49us/step - loss: 5.1849e-05 - val_loss: 4.7959e-05\n",
      "Epoch 77/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1841e-05 - val_loss: 4.7947e-05\n",
      "Epoch 78/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.1830e-05 - val_loss: 4.7945e-05\n",
      "Epoch 79/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1804e-05 - val_loss: 4.8007e-05\n",
      "Epoch 80/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1810e-05 - val_loss: 4.7947e-05\n",
      "Epoch 81/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1791e-05 - val_loss: 4.8070e-05\n",
      "Epoch 82/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1772e-05 - val_loss: 4.8000e-05\n",
      "Epoch 83/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1770e-05 - val_loss: 4.7917e-05\n",
      "Epoch 84/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1753e-05 - val_loss: 4.7951e-05\n",
      "Epoch 85/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1743e-05 - val_loss: 4.8044e-05\n",
      "Epoch 86/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.1730e-05 - val_loss: 4.7926e-05\n",
      "Epoch 87/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1716e-05 - val_loss: 4.7913e-05: 5.1084 - ETA: 0s - loss: 5\n",
      "Epoch 88/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1711e-05 - val_loss: 4.7901e-05\n",
      "Epoch 89/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1686e-05 - val_loss: 4.7953e-05\n",
      "Epoch 90/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1675e-05 - val_loss: 4.8043e-05\n",
      "Epoch 91/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1665e-05 - val_loss: 4.7925e-05\n",
      "Epoch 92/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.1655e-05 - val_loss: 4.7939e-05\n",
      "Epoch 93/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1642e-05 - val_loss: 4.7917e-05\n",
      "Epoch 94/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1644e-05 - val_loss: 4.7884e-05\n",
      "Epoch 95/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1626e-05 - val_loss: 4.7889e-05\n",
      "Epoch 96/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1608e-05 - val_loss: 4.7969e-05\n",
      "Epoch 97/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1591e-05 - val_loss: 4.7971e-05\n",
      "Epoch 98/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1587e-05 - val_loss: 4.7901e-05\n",
      "Epoch 99/150\n",
      "58789/58789 [==============================] - 3s 49us/step - loss: 5.1582e-05 - val_loss: 4.7890e-05\n",
      "Epoch 100/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1566e-05 - val_loss: 4.8045e-05\n",
      "Epoch 101/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1548e-05 - val_loss: 4.8067e-05\n",
      "Epoch 102/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1550e-05 - val_loss: 4.7919e-05\n",
      "Epoch 103/150\n",
      "58789/58789 [==============================] - 3s 50us/step - loss: 5.1534e-05 - val_loss: 4.7900e-05\n",
      "Epoch 104/150\n",
      "58789/58789 [==============================] - 3s 49us/step - loss: 5.1531e-05 - val_loss: 4.7873e-05\n",
      "Epoch 105/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1516e-05 - val_loss: 4.7858e-05\n",
      "Epoch 106/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.1496e-05 - val_loss: 4.7887e-05\n",
      "Epoch 107/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1491e-05 - val_loss: 4.7880e-05\n",
      "Epoch 108/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1483e-05 - val_loss: 4.7853e-05\n",
      "Epoch 109/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1470e-05 - val_loss: 4.7850e-05\n",
      "Epoch 110/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1467e-05 - val_loss: 4.7890e-05\n",
      "Epoch 111/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1459e-05 - val_loss: 4.8028e-05\n",
      "Epoch 112/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1443e-05 - val_loss: 4.7919e-05\n",
      "Epoch 113/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1439e-05 - val_loss: 4.7826e-05\n",
      "Epoch 114/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1426e-05 - val_loss: 4.7895e-05\n",
      "Epoch 115/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1413e-05 - val_loss: 4.7824e-05\n",
      "Epoch 116/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1403e-05 - val_loss: 4.7838e-05\n",
      "Epoch 117/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1391e-05 - val_loss: 4.7947e-05\n",
      "Epoch 118/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1380e-05 - val_loss: 4.7824e-05\n",
      "Epoch 119/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1369e-05 - val_loss: 4.7820e-05\n",
      "Epoch 120/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.1358e-05 - val_loss: 4.7850e-05\n",
      "Epoch 121/150\n",
      "58789/58789 [==============================] - 3s 49us/step - loss: 5.1360e-05 - val_loss: 4.7799e-05\n",
      "Epoch 122/150\n",
      "58789/58789 [==============================] - 3s 49us/step - loss: 5.1344e-05 - val_loss: 4.7785e-05\n",
      "Epoch 123/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1332e-05 - val_loss: 4.7856e-05\n",
      "Epoch 124/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1329e-05 - val_loss: 4.7789e-05\n",
      "Epoch 125/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1308e-05 - val_loss: 4.7836e-05\n",
      "Epoch 126/150\n",
      "58789/58789 [==============================] - 3s 43us/step - loss: 5.1318e-05 - val_loss: 4.7767e-05\n",
      "Epoch 127/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1292e-05 - val_loss: 4.7785e-05\n",
      "Epoch 128/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1285e-05 - val_loss: 4.7756e-05\n",
      "Epoch 129/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1278e-05 - val_loss: 4.7830e-05\n",
      "Epoch 130/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1263e-05 - val_loss: 4.7746e-05\n",
      "Epoch 131/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1259e-05 - val_loss: 4.7734e-05\n",
      "Epoch 132/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1245e-05 - val_loss: 4.7726e-05\n",
      "Epoch 133/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1234e-05 - val_loss: 4.7762e-05\n",
      "Epoch 134/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1229e-05 - val_loss: 4.7712e-05\n",
      "Epoch 135/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1222e-05 - val_loss: 4.7764e-05\n",
      "Epoch 136/150\n",
      "58789/58789 [==============================] - 3s 50us/step - loss: 5.1213e-05 - val_loss: 4.7702e-05\n",
      "Epoch 137/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1200e-05 - val_loss: 4.7929e-05\n",
      "Epoch 138/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1199e-05 - val_loss: 4.7727e-05\n",
      "Epoch 139/150\n",
      "58789/58789 [==============================] - 3s 48us/step - loss: 5.1189e-05 - val_loss: 4.7710e-05\n",
      "Epoch 140/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1170e-05 - val_loss: 4.7665e-05\n",
      "Epoch 141/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1169e-05 - val_loss: 4.7688e-05\n",
      "Epoch 142/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1164e-05 - val_loss: 4.7656e-05\n",
      "Epoch 143/150\n",
      "58789/58789 [==============================] - 3s 47us/step - loss: 5.1152e-05 - val_loss: 4.7657e-05\n",
      "Epoch 144/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1138e-05 - val_loss: 4.7655e-05\n",
      "Epoch 145/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1133e-05 - val_loss: 4.7687e-05\n",
      "Epoch 146/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1127e-05 - val_loss: 4.7692e-05\n",
      "Epoch 147/150\n",
      "58789/58789 [==============================] - 3s 44us/step - loss: 5.1117e-05 - val_loss: 4.7641e-05\n",
      "Epoch 148/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1110e-05 - val_loss: 4.7640e-05\n",
      "Epoch 149/150\n",
      "58789/58789 [==============================] - 3s 45us/step - loss: 5.1091e-05 - val_loss: 4.7769e-05\n",
      "Epoch 150/150\n",
      "58789/58789 [==============================] - 3s 46us/step - loss: 5.1095e-05 - val_loss: 4.7617e-05\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=5, validation_split = 0.005, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=200, validation_split = 0.005, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00285213  0.01091266  0.01977928  0.03131202  0.0710565\n",
      "  0.0396825   0.04352681  0.05152528  0.05102924  0.05468745  0.11024305\n",
      "  0.08500739  0.0557416   0.04185268  0.01773314  0.02387157  0.00124006\n",
      " -0.02368556 -0.02337552 -0.01444694 -0.01109876 -0.00062008 -0.03478423\n",
      " -0.02752978 -0.0685144  -0.05016119 -0.0425967  -0.04600693 -0.01339288\n",
      " -0.00998264  0.01283477  0.00564238  0.01314481 -0.0048363  -0.03856648\n",
      " -0.04222469 -0.02430554 -0.03218007 -0.03131202 -0.0634921  -0.08172128\n",
      " -0.08866569 -0.0555556  -0.0476191  -0.05022325 -0.06684027 -0.05295144\n",
      " -0.02876984 -0.01209075 -0.03335817 -0.06063987 -0.0251736  -0.07434279\n",
      " -0.07831104 -0.07279269 -0.07378468 -0.09926832 -0.09883434 -0.06777039\n",
      " -0.08085323 -0.05158735 -0.06479413 -0.11377732]\n",
      "[0.9963]\n",
      "[1.0070935]\n"
     ]
    }
   ],
   "source": [
    "print test_data[0]\n",
    "print test_labels[0]\n",
    "print outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primitive policy temporarily in place of a RL policy network\n",
    "\n",
    "# signal step for our policy network\n",
    "signals = map(lambda arr: sum(arr) / len(arr),outputs)\n",
    "# signals = map(lambda arr: arr[0],outputs)\n",
    "trades = map(lambda signal: 1 if round(signal,4) > 1 else 0,signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one day return\n",
    "actual = map(lambda arr: arr[0],test_labels)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'signal':signals,\n",
    "    'actual': actual,\n",
    "    'trade':trades,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['entry_success'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['entry_failure'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['avoid_success'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 0) else 0,axis=1)\n",
    "df['avoid_failure'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 0) else 0,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>signal</th>\n",
       "      <th>trade</th>\n",
       "      <th>entry_success</th>\n",
       "      <th>entry_failure</th>\n",
       "      <th>avoid_success</th>\n",
       "      <th>avoid_failure</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9963</td>\n",
       "      <td>1.007094</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0029</td>\n",
       "      <td>0.997021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0081</td>\n",
       "      <td>0.994398</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0090</td>\n",
       "      <td>0.999603</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0119</td>\n",
       "      <td>0.995621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0428</td>\n",
       "      <td>0.994062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.994644</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0040</td>\n",
       "      <td>0.995939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0084</td>\n",
       "      <td>0.994749</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.993876</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0039</td>\n",
       "      <td>0.994875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0624</td>\n",
       "      <td>1.003123</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.9724</td>\n",
       "      <td>1.005780</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.9690</td>\n",
       "      <td>1.009245</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.006575</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.999860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0063</td>\n",
       "      <td>1.000572</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9773</td>\n",
       "      <td>1.001606</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.9757</td>\n",
       "      <td>1.001479</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0003</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0088</td>\n",
       "      <td>1.001782</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0033</td>\n",
       "      <td>1.002527</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0105</td>\n",
       "      <td>0.999432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.9670</td>\n",
       "      <td>1.002331</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0071</td>\n",
       "      <td>1.004507</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9616</td>\n",
       "      <td>0.995769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0175</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0073</td>\n",
       "      <td>0.997428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.9967</td>\n",
       "      <td>1.001682</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0322</td>\n",
       "      <td>1.004790</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11910</th>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.997275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11911</th>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.996240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11912</th>\n",
       "      <td>0.9935</td>\n",
       "      <td>0.995732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11913</th>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.996718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11914</th>\n",
       "      <td>1.0018</td>\n",
       "      <td>0.994530</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11915</th>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.994887</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11916</th>\n",
       "      <td>1.0079</td>\n",
       "      <td>0.997013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.998506</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11918</th>\n",
       "      <td>1.0061</td>\n",
       "      <td>0.998823</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11919</th>\n",
       "      <td>0.9808</td>\n",
       "      <td>1.000082</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11920</th>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11921</th>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.998783</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11922</th>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.999349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11923</th>\n",
       "      <td>1.0074</td>\n",
       "      <td>1.003042</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11924</th>\n",
       "      <td>1.0068</td>\n",
       "      <td>1.004705</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11925</th>\n",
       "      <td>1.0002</td>\n",
       "      <td>1.000892</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11926</th>\n",
       "      <td>0.9887</td>\n",
       "      <td>1.000515</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11927</th>\n",
       "      <td>1.0237</td>\n",
       "      <td>1.003272</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11928</th>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.005763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11929</th>\n",
       "      <td>0.9847</td>\n",
       "      <td>1.010705</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11930</th>\n",
       "      <td>0.9954</td>\n",
       "      <td>1.004184</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11931</th>\n",
       "      <td>0.9783</td>\n",
       "      <td>1.000057</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11932</th>\n",
       "      <td>1.0206</td>\n",
       "      <td>1.003612</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11933</th>\n",
       "      <td>0.9912</td>\n",
       "      <td>1.004351</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11934</th>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.998590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11935</th>\n",
       "      <td>1.0091</td>\n",
       "      <td>0.999290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11936</th>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.999469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11937</th>\n",
       "      <td>1.0037</td>\n",
       "      <td>0.999675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11938</th>\n",
       "      <td>1.0043</td>\n",
       "      <td>0.999554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11939</th>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.998742</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11940 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual    signal  trade  entry_success  entry_failure  avoid_success  \\\n",
       "0      0.9963  1.007094      1              0              1              0   \n",
       "1      1.0029  0.997021      0              0              0              0   \n",
       "2      1.0081  0.994398      0              0              0              0   \n",
       "3      1.0090  0.999603      0              0              0              0   \n",
       "4      1.0119  0.995621      0              0              0              0   \n",
       "5      1.0428  0.994062      0              0              0              0   \n",
       "6      0.9673  0.994644      0              0              0              1   \n",
       "7      1.0040  0.995939      0              0              0              0   \n",
       "8      1.0084  0.994749      0              0              0              0   \n",
       "9      0.9995  0.993876      0              0              0              1   \n",
       "10     1.0039  0.994875      0              0              0              0   \n",
       "11     1.0624  1.003123      1              1              0              0   \n",
       "12     0.9724  1.005780      1              0              1              0   \n",
       "13     0.9690  1.009245      1              0              1              0   \n",
       "14     0.9855  1.006575      1              0              1              0   \n",
       "15     0.9754  0.999860      0              0              0              1   \n",
       "16     1.0063  1.000572      1              1              0              0   \n",
       "17     0.9773  1.001606      1              0              1              0   \n",
       "18     0.9757  1.001479      1              0              1              0   \n",
       "19     1.0003  0.999715      0              0              0              0   \n",
       "20     1.0088  1.001782      1              1              0              0   \n",
       "21     1.0033  1.002527      1              1              0              0   \n",
       "22     1.0105  0.999432      0              0              0              0   \n",
       "23     0.9670  1.002331      1              0              1              0   \n",
       "24     1.0071  1.004507      1              1              0              0   \n",
       "25     0.9616  0.995769      0              0              0              1   \n",
       "26     1.0175  0.999439      0              0              0              0   \n",
       "27     1.0073  0.997428      0              0              0              0   \n",
       "28     0.9967  1.001682      1              0              1              0   \n",
       "29     1.0322  1.004790      1              1              0              0   \n",
       "...       ...       ...    ...            ...            ...            ...   \n",
       "11910  0.9898  0.997275      0              0              0              1   \n",
       "11911  1.0190  0.996240      0              0              0              0   \n",
       "11912  0.9935  0.995732      0              0              0              1   \n",
       "11913  0.9982  0.996718      0              0              0              1   \n",
       "11914  1.0018  0.994530      0              0              0              0   \n",
       "11915  0.9700  0.994887      0              0              0              1   \n",
       "11916  1.0079  0.997013      0              0              0              0   \n",
       "11917  0.9944  0.998506      0              0              0              1   \n",
       "11918  1.0061  0.998823      0              0              0              0   \n",
       "11919  0.9808  1.000082      1              0              1              0   \n",
       "11920  0.9989  0.999664      0              0              0              1   \n",
       "11921  0.9934  0.998783      0              0              0              1   \n",
       "11922  0.9836  0.999349      0              0              0              1   \n",
       "11923  1.0074  1.003042      1              1              0              0   \n",
       "11924  1.0068  1.004705      1              1              0              0   \n",
       "11925  1.0002  1.000892      1              1              0              0   \n",
       "11926  0.9887  1.000515      1              0              1              0   \n",
       "11927  1.0237  1.003272      1              1              0              0   \n",
       "11928  0.9991  1.005763      1              0              1              0   \n",
       "11929  0.9847  1.010705      1              0              1              0   \n",
       "11930  0.9954  1.004184      1              0              1              0   \n",
       "11931  0.9783  1.000057      1              0              1              0   \n",
       "11932  1.0206  1.003612      1              1              0              0   \n",
       "11933  0.9912  1.004351      1              0              1              0   \n",
       "11934  0.9588  0.998590      0              0              0              1   \n",
       "11935  1.0091  0.999290      0              0              0              0   \n",
       "11936  0.9879  0.999469      0              0              0              1   \n",
       "11937  1.0037  0.999675      0              0              0              0   \n",
       "11938  1.0043  0.999554      0              0              0              0   \n",
       "11939  0.9779  0.998742      0              0              0              1   \n",
       "\n",
       "       avoid_failure  success  \n",
       "0                  0        0  \n",
       "1                  1        1  \n",
       "2                  1        1  \n",
       "3                  1        1  \n",
       "4                  1        1  \n",
       "5                  1        1  \n",
       "6                  0        1  \n",
       "7                  1        1  \n",
       "8                  1        1  \n",
       "9                  0        1  \n",
       "10                 1        1  \n",
       "11                 0        1  \n",
       "12                 0        0  \n",
       "13                 0        0  \n",
       "14                 0        0  \n",
       "15                 0        1  \n",
       "16                 0        1  \n",
       "17                 0        0  \n",
       "18                 0        0  \n",
       "19                 1        1  \n",
       "20                 0        1  \n",
       "21                 0        1  \n",
       "22                 1        1  \n",
       "23                 0        0  \n",
       "24                 0        1  \n",
       "25                 0        1  \n",
       "26                 1        1  \n",
       "27                 1        1  \n",
       "28                 0        0  \n",
       "29                 0        1  \n",
       "...              ...      ...  \n",
       "11910              0        1  \n",
       "11911              1        1  \n",
       "11912              0        1  \n",
       "11913              0        1  \n",
       "11914              1        1  \n",
       "11915              0        1  \n",
       "11916              1        1  \n",
       "11917              0        1  \n",
       "11918              1        1  \n",
       "11919              0        0  \n",
       "11920              0        1  \n",
       "11921              0        1  \n",
       "11922              0        1  \n",
       "11923              0        1  \n",
       "11924              0        1  \n",
       "11925              0        1  \n",
       "11926              0        0  \n",
       "11927              0        1  \n",
       "11928              0        0  \n",
       "11929              0        0  \n",
       "11930              0        0  \n",
       "11931              0        0  \n",
       "11932              0        1  \n",
       "11933              0        0  \n",
       "11934              0        1  \n",
       "11935              1        1  \n",
       "11936              0        1  \n",
       "11937              1        1  \n",
       "11938              1        1  \n",
       "11939              0        1  \n",
       "\n",
       "[11940 rows x 8 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primitive policy - replace with a policy network which maximizes reward\n",
    "def label_success (row):\n",
    "    return 0 if (row['entry_failure'] == 1 or row['entry_failure'] == 1) else 1\n",
    "\n",
    "success = df.apply (lambda row: label_success (row),axis=1)\n",
    "df['success'] = success;\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013263587143018878\n",
      "0.011024579248785298\n",
      "0.4308439770003506\n",
      "0.43953834081849685\n"
     ]
    }
   ],
   "source": [
    "print df['actual'].corr(df['signal'])\n",
    "print df['actual'].corr(df['trade'])\n",
    "print df['actual'].corr(df['entry_success'])\n",
    "print df['actual'].corr(df['success'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11940.00000\n",
       "mean         1.00027\n",
       "std          0.01017\n",
       "min          0.94640\n",
       "25%          0.99580\n",
       "50%          1.00060\n",
       "75%          1.00550\n",
       "max          1.06240\n",
       "Name: actual, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['actual'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11940.000000\n",
       "mean         1.000289\n",
       "std          0.002431\n",
       "min          0.987680\n",
       "25%          0.998944\n",
       "50%          1.000198\n",
       "75%          1.001571\n",
       "max          1.016850\n",
       "Name: signal, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['signal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11940\n",
      "\n",
      "Precision\n",
      "0.538376325368\n",
      "\n",
      "Recall\n",
      "0.536847088528\n",
      "\n",
      "Accuracy\n",
      "0.284924623116\n",
      "\n",
      "Non-loss events\n",
      "9023\n",
      "0.755695142379\n",
      "\n",
      "Lose trades\n",
      "2917\n",
      "0.244304857621\n",
      "\n",
      "Win trades\n",
      "3402\n",
      "0.284924623116\n",
      "\n",
      "Missed opportunities\n",
      "2935\n",
      "0.24581239531\n",
      "\n",
      "Bullets dodged\n",
      "2590\n",
      "0.216917922948\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST_SAMPLES = len(test_data)\n",
    "print NUM_TEST_SAMPLES\n",
    "\n",
    "print '\\nPrecision' # optimize for this since we can increase discovery, so long as we find enough trades\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['entry_failure'])) \n",
    "\n",
    "print '\\nRecall'\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['avoid_failure']))\n",
    "\n",
    "print '\\nAccuracy'\n",
    "print sum(df['entry_success']) * 1.00 / (NUM_TEST_SAMPLES)\n",
    "\n",
    "print '\\nNon-loss events'\n",
    "print sum(df['success'])\n",
    "print sum(df['success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nLose trades'\n",
    "print sum(df['entry_failure'])\n",
    "print sum(df['entry_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nWin trades'\n",
    "print sum(df['entry_success'])\n",
    "print sum(df['entry_success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nMissed opportunities'\n",
    "print sum(df['avoid_failure'])\n",
    "print sum(df['avoid_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nBullets dodged'\n",
    "print sum(df['avoid_success'])\n",
    "print sum(df['avoid_success']) / (NUM_TEST_SAMPLES * 1.00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEWCAYAAAApTuNLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3Xl4VdX18PHvyjwnZCCBJBDmGREijhUFnC3YahWtdS611drRqvXtz9baVjtbtVrrULWtqDih1eKIdURA5jkyJgQSEjKS+a73j31CLjEJAXKTC1mf57kP9+6zz777HEgWe5919hFVxRhjjOlpIT3dAWOMMQYsIBljjAkSFpCMMcYEBQtIxhhjgoIFJGOMMUHBApIxxpigYAHJmCAnIjkioiIS1om6V4nIB4fbjjE9wQKSMV1IRLaISL2IpLYqX+oFg5ye6Zkxwc8CkjFdbzNwafMHERkHxPRcd4w5MlhAMqbrPQVc4ff5SuBJ/woikigiT4pIsYhsFZH/JyIh3rZQEfm9iOwWkU3AeW3s+6iIFIpIgYjcJSKhB9tJEekvIvNEpFRE8kTkm37bJovIYhGpEJFdIvJHrzxKRP4pIiUiUiYii0Qk/WC/25i2WEAyput9AiSIyCgvUMwC/tmqzn1AIjAYmIILYFd7274JnA8cC+QCF7Xa9x9AIzDUq3MmcN0h9HMOkA/0977j1yIy1dt2L3CvqiYAQ4BnvfIrvX5nAynA9UDNIXy3MV9gAcmYwGgeJZ0BrAUKmjf4BanbVLVSVbcAfwC+4VW5GPizqm5X1VLgN377pgPnAt9X1WpVLQL+5LXXaSKSDZwM3KKqtaq6DHiElpFdAzBURFJVtUpVP/ErTwGGqmqTqi5R1YqD+W5j2mMByZjAeAq4DLiKVtN1QCoQDmz1K9sKZHrv+wPbW21rNtDbt9CbMisD/gb0Pcj+9QdKVbWynT5cCwwH1nnTcuf7Hdd8YI6I7BCR34pI+EF+tzFtsoBkTACo6lZccsO5wAutNu/GjTQG+pUNoGUUVYibEvPf1mw7UAekqmqS90pQ1TEH2cUdQLKIxLfVB1XdqKqX4gLdPcBcEYlV1QZV/YWqjgZOwk0tXoExXcACkjGBcy0wVVWr/QtVtQl3TeZXIhIvIgOBH9JynelZ4CYRyRKRPsCtfvsWAm8AfxCRBBEJEZEhIjLlYDqmqtuBj4DfeIkK473+/hNARC4XkTRV9QFl3m4+ETldRMZ5044VuMDqO5jvNqY9FpCMCRBV/VxVF7ez+btANbAJ+AD4N/CYt+3vuGmx5cBnfHGEdQUQAawB9gBzgX6H0MVLgRzcaOlF4A5VfcvbdjawWkSqcAkOs1S1Bsjwvq8Cd23sPdw0njGHTewBfcYYY4KBjZCMMcYEBQtIxhhjgoIFJGOMMUHBApIxxpigYMvQH4TU1FTNycnp6W4YY8wRZcmSJbtVNe1A9SwgHYScnBwWL24vi9cYY0xbRGTrgWvZlJ0xxpggYQHJGGNMULCAZIwxJijYNaTD1NDQQH5+PrW1tT3dlW4TFRVFVlYW4eG2yLMxputYQDpM+fn5xMfHk5OTg4j0dHcCTlUpKSkhPz+fQYMG9XR3jDFHEZuyO0y1tbWkpKT0imAEICKkpKT0qhGhMaZ7WEDqAr0lGDXrbcdrjOkeFpC6gapSWF7D3vrGnu6KMcYELQtI3aC+0UdpVT15RVXkFVWxu6qO2oYmuuLRHyUlJUyYMIEJEyaQkZFBZmbmvs/19fWdauPqq69m/fr1h90XY4w5HJbU0A0iw0MZ2S+BPXvrKamqZ0dZDQBxkWFk9YkhIuzQ/1+QkpLCsmXLAPj5z39OXFwcP/7xj/ero6qoKiEhbX/P448/fsjfb4wxXSWgIyQROVtE1otInojc2sb2SBF5xtu+UERy/Lbd5pWvF5GzDtSmiAzy2sjz2ozwyq8SkWIRWea9rvPKB4rIZ17ZahG5PpDnIjRESI2LZERGPCMz4umXGM3e+iY27qpkZ3ktNfWNXTJiapaXl8fo0aP5+te/zpgxYygsLGT27Nnk5uYyZswY7rzzzn11TznlFJYtW0ZjYyNJSUnceuutHHPMMZx44okUFRV1WZ+MMaYjARshiUgo8ABwBpAPLBKReaq6xq/atcAeVR0qIrOAe4BLRGQ0MAsYA/QH3hKR4d4+7bV5D/AnVZ0jIg95bT/o7fOMqt7YqouFwImqWiciccAqr60dh3rMv3hlNWt2VHS6vqpS1+ijyecCkYgQFiqEhQghXuLA6P4J3PHlMYfUn3Xr1vHkk0+Sm5sLwN13301ycjKNjY2cfvrpXHTRRYwePXq/fcrLy5kyZQp33303P/zhD3nssce49dYv/F/CGGO6XCBHSJOBPFXdpKr1wBxgZqs6M4EnvPdzgWniUrhmAnNUtU5VNwN5XntttuntM9VrA6/NCzrqnKrWq2qd9zGSHrieJiJEhYcSExlGZHgIIQINTT5q6puoa2yiyaeHNWoaMmTIvmAE8PTTTzNx4kQmTpzI2rVrWbNmzRf2iY6O5pxzzgFg0qRJbNmy5ZC/3xhjDkYgryFlAtv9PucDx7dXR1UbRaQcSPHKP2m1b6b3vq02U4AyVW1soz7AhSJyKrAB+IGqbgcQkWzgP8BQ4Oa2RkciMhuYDTBgwIAOD/hQRzL+Gpt87K6qp6SqjiYvGK3fWUl8VBhJ0eHERHb+ryw2Nnbf+40bN3Lvvffy6aefkpSUxOWXX97mvUQRERH73oeGhtLYaJmBxpju0Ruy7F4BclR1PPAmLSMyVHW7Vz4UuFJE0lvvrKoPq2ququampR3wcR6HLSw0hIzEKEb2S2BwaiwZiVFEhIVQWl1PXnEVBWU1+6b4DkZFRQXx8fEkJCRQWFjI/PnzA9B7Y4w5dIEcIRUA2X6fs7yyturki0gYkAiUHGDftspLgCQRCfNGSfvqq2qJX/1HgN+27qiq7hCRVcCXaJn261GhIUJcVDhxUeEQD00+ZVdFLbur6iirric+KpyE6DBiIsIID5UD3qw6ceJERo8ezciRIxk4cCAnn3xyNx2JMcZ0jnRlZtd+DbsAswGYhgsOi4DLVHW1X50bgHGqer2X1PBVVb1YRMYA/8ZdM+oPvA0MA6S9NkXkOeB5v6SGFar6VxHpp6qF3vd9BbhFVU8QkSygRFVrRKQPsBC4UFVXtndMubm52voBfWvXrmXUqFGHfb46a299I6XV9VTUNNDojZQiw0LJSIgkITq821ZR6O7jNsYcuURkiarmHqhewEZI3jWhG4H5QCjwmBc47gQWq+o84FHgKRHJA0pxmXV49Z4F1gCNwA2q2gTQVpveV94CzBGRu4ClXtsAN4nIDK+dUuAqr3wU8AcRUVyg+31HwShYxES4UZEmKbUNTVTXN1FaVc/W0r1EhYeSFBNOUnTEYd3bZIwxPSFgI6SjUTCMkNqiquzZ20BpdT176xsREfonRpEcG4GIoKpdPnIKhuM2xhwZenyEZLqPiJAcG0FybAR1jU3sKKuloKyG0up6GnyKz6ckRoeTEhtBdESoLY5qjAlKFpCOMpFhoeSkxLC7qo7ymgbiI8MQoKymgT176wkPDSE+KoyU2EiiI0J7urvGGLOPBaSjkIiQFh9FWnzUvrJ+Ph8VNY1U1jZQ5k3vxUeF0ycmnPiocEJDbNRkjOlZFpB6idCQEPrERtAnNoLGJh8l1fWUVNdTWdpAiAhJMeGkxkUSFW6jJmNMz7CAdIQrKSlh2rRpAOzcuZPQ0FCab+D99NNP91t5oVlYaAjpCVH0jY+kur6Jsup6HnvsMU45/QwGZGXSJyacxOhwwkItU88Y030sIB3hOvP4ifaICHGRYcRFhjH/hTmccsJx+Hz9KSirYUd5LfGRYURHhBLhXXeyAGWMCSQLSEexJ554ggceeID6+npOOukk7r//fnw+H1dffTXLli1DVZk9ezbp6eksX76Mb19zBdHR0bz3wUdUNwrlNQ1U1DYALnglRYeTFBNO7EGsp2eMMZ1lv1m60uu3ws4uvrc2Yxycc/dB77Zq1SpefPFFPvroI8LCwpg9ezZz5sxhyJAh7N69m5UrXT/LyspISkrivvvu4/7772fChAmAW8Opf1I0Pp9S19hE6d4Gyqrr2bO3nhARKvbWs2V3NTmpsR30whhjOs8C0lHqrbfeYtGiRfseP1FTU0N2djZnnXUW69ev56abbuK8887jzDPP7LCdkBAhOiKMzIgw+iVEUVXXSHlNAzvrm5j6hwWMzEhgeHoc47OSOH5wMqMyEgixjD1jzCGwgNSVDmEkEyiqyjXXXMMvf/nLL2xbsWIFr7/+Og888ADPP/88Dz/8cKfaDAkREqLDSYgOpzwhihunDmP59jIWbi7lpWXuyR05KTFcc8ogvnJsJvFR4V16TMaYo5sFpKPU9OnTueiii/je975HamoqJSUlVFdXEx0dTVRUFF/72tcYNmwY1113HQDx8fFUVlZ2uv3QEOGHZwzf93lHWQ0f5u3mXwu38X8vr+YXr6xhXGYiZ45J55LcbFLiIrv8GI0xRxcLSEepcePGcccddzB9+nR8Ph/h4eE89NBDhIaGcu211+5b3+6ee+4B4Oqrr+a6664jOjq63XTxjvRPiuZrudl8LTebpdv28M66Ij7I281v/7ueP7+5kemj+3L6iL6cOjyN9ISoAzdojOl1bHHVgxCsi6v2hM4ed15RJf/8ZBuvrypkV4V7Ynx2cjRThqdxzcmDGJwWF+iuGmN6mC2uaoLC0L7x/HzGGO748mhW76jgk00lLNpSyrOL8/nXwm18aVgaJw9J4ZjsJNITouiXGGWrRRjTS1lAMt1CRBibmcjYzESu+9JgiivreOKjLby+qpDfvF68r150eCjnjuvHhZMyOS4nmXC7GdeYXsMCUhcIxPOGgllXTPOmxUfy47NG8OOzRlBUUcv6XZUUVdSxeOseXlm+g+c/yyc2IpQTBqdwyrBUThqSyuC0WAtQxhzFLCAdpqioKEpKSkhJSekVQUlVKSkpISqq6xIT+iZE0ddLdLhwUhY/O38U/9uwmw/yinl/427eXlcEQFiIMDw9nutPG8L54/rZ/U7GHGUsqeEgtJXU0NDQQH5+PrW1tT3Uq+4XFRVFVlYW4eHdc5/R9tK9LNpSSl5RFe+sK2LdzkpGpMczNjORzD7RjEiPZ1S/eAalxvaK/xQYc6TpbFKDBaSD0FZAMt2ryae8vKyApz/dRv6eGnZV1OLz/gkPSI7hrDHpnDUmg4kD+tgIypggYQEpACwgBZ/ahibyiqpYnl/Gm2t28WHebhqalNS4CI4d0Icx/RMY0z+RsZkJ9EuM7unuGtMrWUAKAAtIwa+itoF31xXx7roiVhaUs2l3Nc3/xL80LJVbzh7J2MzEnu2kMb2MBaQAsIB05Nlb38jawko+2VTC39/fRNneBqaP6suVJ+Vw/KAUIsIsa8+YQLOAFAAWkI5s5TUNPPrBZv69cCu7q+oBSImNoG9CFBkJkYzNTOS0EWkck5VkDyM0pgtZQAoAC0hHh7rGJt5aU0ReURW7KmvZVV7LjvJa1u+swKeQGB3OKcNSmTI8jdOGp+1LSTfGHBpbOsiYdkSGhXLe+H5fKC/f28AHebtZsL6I9zYU858VhQCM7pfAlBEuOE0c2MduzjUmQGyEdBBshNR7qCprCytZsKGI99YXs2TrHhp9SnxkGCcPTeW0EWlMGtiHtPhIEqPD7f4nYzpgU3YBYAGp96qobeCjvBLe21DEgvXFFJa33AidGhfB+eP7c/bYDMb0T7AHExrTSlAEJBE5G7gXCAUeUdW7W22PBJ4EJgElwCWqusXbdhtwLdAE3KSq8ztqU0QGAXOAFGAJ8A1VrReRq4DfAQXe196vqo+IyATgQSDB+45fqeozHR2PBSQDbvS0saiKtYUV7K6q57Ote3hz7S7qG30ADEmLZfrodM4cnc74rCSb4jO9Xo8HJBEJBTYAZwD5wCLgUlVd41fnO8B4Vb1eRGYBX1HVS0RkNPA0MBnoD7wFND+etM02ReRZ4AVVnSMiDwHLVfVBLyDlquqNrfo3HFBV3Sgi/XFBbJSqlrV3TBaQTHvKaxpYvKWUtYUVLNxcysefl9DoU6LDQ8nN6cP0UelMGZ5GclwEsRFhhNoqEqYXCYakhslAnqpu8jo0B5gJrPGrMxP4ufd+LnC/uMn4mcAcVa0DNotIntcebbUpImuBqcBlXp0nvHYfbK9zqrrB7/0OESkC0oB2A5Ix7UmMDmfaqHSmjUrnRloSJBZtKeX9jcXcMW/1vroRoSFMHpTMiUNSSIuLpE9sBKcOTyUyzJ4DZXq3QAakTGC73+d84Pj26qhqo4iU46bcMoFPWu2b6b1vq80UoExVG9uoD3ChiJyKG139QFX920BEJgMRwOetD0JEZgOzAQYMGNDB4RrTIjEmnPPG99uXzZdXVMWSraVU1jayo6yWD/KK+d389fvqZyZF892pQzl3fD8S7BqU6aV6Q9r3K8DTqlonIt/CjZ6mNm8UkX7AU8CVquprvbOqPgw8DG7Krnu6bI42Q/vGMbTv/o9rL69poLK2gQ27Krn3rY3c+sJKfvriSsZlJTEyPZ5BabGMy0zkmOwk4iJ7w4+q6e0C+a+8AMj2+5xFS2JB6zr5IhIGJOKSGzrat63yEiBJRMK8UdK++qpa4lf/EeC3zR9EJAH4D3C7qvqPyIwJuMTocBKjw8nqE8PpI/qyaMsePthYzCebS3l7XRG7F9cBECIwIiOBSQOTmDkhk9yBfSzN3ByVAhmQFgHDvOy3AmAWLdd4ms0DrgQ+Bi4C3lFVFZF5wL9F5I+4pIZhwKeAtNWmt8+7XhtzvDZfBjcCUtVC7/tmAGu98gjgReBJVZ0biBNgTGeJCJMHJTN5UPK+svK9DSzLL2PJ1j0s3baHl5bu4J+fbOOY7CTS4yMpqqwjPSGSY7KTmDqyLyMzEnrwCIw5fIFO+z4X+DMuRfsxVf2ViNwJLFbVeSIShZsuOxYoBWb5JSzcDlwDNALfV9XX22vTKx+MC0bJwFLgcm+a7je4QNTofce3VXWdiFwOPA60XG2Gq1R1WXvHY1l2pifV1Dcxd8l2/rVwG00+JS0+kh1lNWwp2QvAMVmJnDai777pwUGpsUSFW6KE6Xk9nvZ9NLKAZILR7qo65i3bwXNL8lm3s2Lf4zZE4LiByXxrymBOGJxCUWUdcZFhpMVH9myHTa9jASkALCCZYFfb0MSm4mryiqvYsLOSF5cWUFBWs297c5A6d1wG54zrR7otHGu6gQWkALCAZI40DU0+XltZSP6eGjISoti+Zy+vrSxkw64qRGBQaiwxEaGkx0cxZUQaXxqWRk5KjCVNmC5lASkALCCZo0VeUSWvrdzJup0V1Db42FRcte9aVGJ0OOOzEhmflcgxWUlMHpRMUkxED/fYHMmCYaUGY0yQGto3npumxe9Xtnl3NR9/XsLKgjKWby/nofc20eRTRGBoWhwZiVFkJkVz0aQsJlnquQkAC0jGGMBN3w1KjQXciiS1DU2syC9n4aYSVhSUs7uqjv+sLGTOou2MzIjn+EHJjOmfyJjMBIb1jbfHwZvDZlN2B8Gm7Exvt7e+kReXFvDS0gLW7Kigur4JcOvzDUuPY6wXoEZmJBAeKvhUGZmRQKytNNGr2TWkALCAZEwLn0/ZUlLN6h0VrNpRzpodFazeUUFpdf1+9SLCQjh5SArTR6czfVS6Zfb1QhaQAsACkjEdU1UKy2vZsKsSBZqalI8+L+HNtTvZXurSz4/JSmT6qHRGZMSTEhdBSmwkyXERxEeG2XWpo5QFpACwgGTMoVFVNuyq4q21u3hjzS6Wb//iU16yk6OZOqIvp4/sywmDU2yViaOIBaQAsIBkTNcoqapjR1ktJdV1lFTVU1xVx6LNpXz4+W5qG3xEh4fSPymK+iYfGQlRnDw0lTH9E4mLDCOrTzTZyTE9fQjmIFjatzEmaKXERZISt/8SRtdPGUJtQxMfbyphwboidlfXEx4ibNpdzb1vb8T//84DkmM4fUQaMyb0Z+IAS0E/WtgI6SDYCMmYnrGnup7te/ZSVdfIhp2VfJC3m/c37qau0UdSTDiZSdH0S4ymf1IUA5JjOHZAH8ZmJthTeIOETdkFgAUkY4JHVV0j81ftZMm2PRSW1VBYXktheS3lNQ376sREhNInJoKsPtEM6RvHyUNSOWlICkkx4Taq6kYWkALAApIxwa+ospbPtu5h/c4qKmsbKK2uZ1vpXtbvqqSythFwqehpcZGM6pfAMVmJjM9OYnxmIn1ibYmkQLBrSMaYXqlvfBRnj+3H2WP3L29s8rHce+BhSVU9OytqWVlQzltrd+2rMyA5Zt8afuOyEhnTP4H4qPBuPoLeywKSMaZXCAsNYdLAZCYNTN6vvKK2gVX55SzPL2dFfhlLt5Xx6gr3kOnmFdHHZyYyLiuJ0f0SGJERT3R4KPVNPhKi7N6prmQByRjTqyVEhXPS0FROGpq6r6ykqo6VBeWszC9nRUE5n2wq5aVlO76wb9/4SKYMT+P4wSmMz0okq0800eGhFqQOkV1DOgh2DcmY3quospZ1hZVs2FVJQ5MSGgLL88t5f0MxFd61KXCjqtH9ErhoUhaZSdGs31lJSlwkXzk2k+iI3pn1Z0kNAWAByRjTWpNP2by7ihX55RRV1lFZ28B7G4pZVVCxX73k2AjOHZfBgOQYsvrEkJkUzaC0WBJ6wTUqC0gBYAHJGNNZG3ZVUl3XyPD0eFbvqOBv733Op5tLqazbfzQ1pn8Cx2QlkRwbQd+EKI7NTmJkRjxhoUfP4zwsy84YY3rQ8PSWByBOHpTM5EEumaK8poGCPTXk79nLmsIKPv68hNdWFlJe04DPGx/ERIRyTFYS47MS6Z8UTXpCJH0Touif6N4frdeobIR0EGyEZIwJFJ9PKSir4bNte1i6zaWnr9tZQUPT/r+j4yLDGNo3jgnZLmDFRoYRGRbCoNRYsvvEEBISfMHKpuwCwAKSMaY7+XxK6d56dlXUsquiloI9NXxeXM2awgpW5pdT09C0X/2YiFCmDE/j7LEZpCdEER4qJMVEkJ4QRVwPPiTRpuyMMeYIFxIipMZFkhoXyZj+iftta2jysbWkmrpGHzX1TXxeXMXy/HLeWL2L11ft/EJb/ROjGJuZyHE5yZw4JIWhfeOC7hEfNkI6CDZCMsYEuyafsnpHOVV1jTQ0KaXVdRSWu5T1FfllbCnZu69uQlQYafGRpMVHkpkUw6DUGAalxpGTGsPQvnFdtjitjZCMMaYXCg0Rxmcltbt9Z3ktCzeXkL+nhuLKOoor6yiqrOXDvN08/1ntvnrxkWGcMSadYX3jKaupJzU2km+eOjigfQ9oQBKRs4F7gVDgEVW9u9X2SOBJYBJQAlyiqlu8bbcB1wJNwE2qOr+jNkVkEDAHSAGWAN9Q1XoRuQr4HVDgfe39qvqIt89/gROAD1T1/ECcA2OMCSYZiVHMnJDZ5ra99Y1s2b2Xz4ur+N+GYuav3skLnxUQERrC8YOTAx6QAjZlJyKhwAbgDCAfWARcqqpr/Op8BxivqteLyCzgK6p6iYiMBp4GJgP9gbeA4d5ubbYpIs8CL6jqHBF5CFiuqg96ASlXVW9so4/TgBjgW50JSDZlZ4zpTRqafNQ3+oiJOLzlkDo7ZRfIO68mA3mquklV63Gjl5mt6swEnvDezwWmiTvqmcAcVa1T1c1Antdem216+0z12sBr84IDdVBV3wYqD+cgjTHmaBUeGkJsZPctIBvIgJQJbPf7nO+VtVlHVRuBctyUW3v7tleeApR5bbT1XReKyAoRmSsi2YdzUMYYYwLj6Fmbon2vADmqOh54k5YRWaeIyGwRWSwii4uLiwPSQWOMMYENSAWA/2gki5bEgi/UEZEwIBGX3NDevu2VlwBJXhv7fZeqlqhqnVf+CC6BotNU9WFVzVXV3LS0tIPZ1RhjzEEIZEBaBAwTkUEiEgHMAua1qjMPuNJ7fxHwjrosi3nALBGJ9LLnhgGfttemt8+7Xht4bb4MICL9/L5vBrC2i4/TGGNMFwhY2reqNorIjcB8XIr2Y6q6WkTuBBar6jzgUeApEckDSnEBBq/es8AaoBG4QVWbANpq0/vKW4A5InIXsNRrG+AmEZnhtVMKXNXcRxF5HxgJxIlIPnBtc3q5McaY7mUrNRwES/s2xpiDFwxp38YYY0ynWUAyxhgTFCwgGWOMCQoWkIwxxgQFC0jGGGOCggUkY4wxQcECkjHGmKBgAckYY0xQsIBkjDEmKHQqIInIEO/projIaSJyk4i0/4xcY4wx5iB1doT0PNAkIkOBh3Erbv87YL0yxhjT63Q2IPm8h999BbhPVW8G+h1gH2OMMabTOhuQGkTkUtxjHV71ysID0yVjjDG9UWcD0tXAicCvVHWz94yipwLXLWOMMb1Np56HpKprgJsARKQPEK+q9wSyY8YYY3qXzmbZLRCRBBFJBj4D/i4ifwxs14wxxvQmnZ2yS1TVCuCrwJOqejwwPXDdMsYY09t0NiCFiUg/4GJakhqMMcaYLtPZgHQnMB/4XFUXichgYGPgumWMMaa36WxSw3PAc36fNwEXBqpTxhhjep/OJjVkiciLIlLkvZ4XkaxAd84YY0zv0dkpu8eBeUB/7/WKV2aMMcZ0ic4GpDRVfVxVG73XP4C0APbLGGNML9PZgFQiIpeLSKj3uhwoCWTHjDHG9C6dDUjX4FK+dwKFwEXAVQHqkzHGmF6oUwFJVbeq6gxVTVPVvqp6AZZlZ4wxpgsdzhNjf9hlvTDGGNPrHU5AkgNWEDlbRNaLSJ6I3NrG9kgRecbbvlBEcvy23eaVrxeRsw7UpogM8trI89qM8MqvEpFiEVnmva7z2+dKEdnova489FNhjDHmcB1OQNKONopIKPAAcA4wGrhUREa3qnYtsEdVhwJ/Au7x9h0NzALGAGcDf21OqOigzXuAP3lt7fHabvaMqk7wXo9435EM3AEcD0wG7vBWMjfGGNMDOgxIIlIpIhVtvCpx9yN1ZDKQp6qbVLUemAPMbFVnJvCE934uME1ExCufo6p1qroZyPPaa7NNb5+pXhsgHQWkAAAfF0lEQVR4bV5wgP6dBbypqqWqugd4Exf8jDHG9IAOA5KqxqtqQhuveFU90LJDmcB2v8/5XlmbdbxHpJcDKR3s2155ClDmtdHWd10oIitEZK6IZB9E/xCR2SKyWEQWFxcXd3zExhhjDtnhTNkdKV4BclR1PG4U9MQB6u9HVR9W1VxVzU1Ls3uBjTEmUAIZkAqAbL/PWV5Zm3VEJAxIxN1w296+7ZWXAEleG/t9l6qWqGqdV/4IMOkg+meMMaabBDIgLQKGedlvEbgkhXmt6swDmrPbLgLeUVX1ymd5WXiDgGHAp+216e3zrtcGXpsvA3jPcWo2A1jrvZ8PnCkifbxkhjO9MmOMMT2gU4+fOBSq2igiN+J+yYcCj6nqahG5E1isqvOAR4GnRCQPKMUFGLx6zwJrgEbgBlVtAmirTe8rbwHmiMhdwFKvbYCbRGSG104p3goTqloqIr/EBTmAO1W1NECnwxhjzAGIG1yYzsjNzdXFixf3dDeMMeaIIiJLVDX3QPV6Q1KDMcaYI4AFJGOMMUHBApIxxpigYAHJGGNMULCAZIwxJihYQDLGGBMULCAZY4wJChaQjDHGBAULSMYYY4KCBSRjjDFBwQKSMcaYoGAByRhjTFCwgGSMMSYoWEAyxhgTFCwgGWOMCQoWkIwxxgQFC0jGGGOCggUkY4wxQcECkjHGmKBgAckYY0xQsIBkjDEmKFhAMsYYExQsIBljjAkKFpCMMcYEBQtIxhhjgoIFJGOMMUHBApIxxpigENCAJCJni8h6EckTkVvb2B4pIs942xeKSI7fttu88vUictaB2hSRQV4beV6bEa2+60IRURHJ9T5HiMjjIrJSRJaLyGkBOAXGGGM6KWABSURCgQeAc4DRwKUiMrpVtWuBPao6FPgTcI+372hgFjAGOBv4q4iEHqDNe4A/eW3t8dpu7ks88D1god93fxNAVccBZwB/EBEbMRpjTA8J5C/gyUCeqm5S1XpgDjCzVZ2ZwBPe+7nANBERr3yOqtap6mYgz2uvzTa9faZ6beC1eYHf9/wSF7Bq/cpGA+8AqGoRUAbkHv5hG2OMORSBDEiZwHa/z/leWZt1VLURKAdSOti3vfIUoMxrY7/vEpGJQLaq/qfVdy8HZohImIgMAiYB2a0PQkRmi8hiEVlcXFzcmeM2xhhzCMJ6ugOB5E3B/RG4qo3NjwGjgMXAVuAjoKl1JVV9GHgYIDc3VwPVV2OM6e0CGZAK2H/EkeWVtVUnX0TCgESg5AD7tlVeAiSJSJg3SmoujwfGAgvcrB4ZwDwRmaGqi4EfNDckIh8BGw75aI0xxhyWQE7ZLQKGedlvEbgkhXmt6swDrvTeXwS8o6rqlc/ysvAGAcOAT9tr09vnXa8NvDZfVtVyVU1V1RxVzQE+AWao6mIRiRGRWAAROQNoVNU1ATkTxhhjDihgIyRVbRSRG4H5QCjwmKquFpE7gcWqOg94FHhKRPKAUlyAwav3LLAGaARuUNUmgLba9L7yFmCOiNwFLPXa7khfYL6I+HCjqW901bEbY4w5eOIGF6YzcnNzdfHixT3dDWOMOaKIyBJVPWAWs913Y4wxJihYQDLGGBMULCAZY4wJChaQjDHGBAULSMYYY4KCBaQjgWVCGmN6AQtIwe7fl8CzdouUMebod1SvZXfEK9sGG/7r3ue9DUOn9Wx/jDEmgGyEFMxWPuf+jMuAN34Gvi+s/WqMMUcNGyF1B18TFK+DgiVQuAJCIyCuL4z6MqQMaXsfVVjxHGQfDyd8B567Epb+EyZd2Xb9QKjfCxEx3fd9xphezQJSd6gshAdPcu8jE0B9UF8F7/wSjrsOTv4+JPTbf59dq6F4LZz7exg9E7JPgHd/BWMvhMi4wPe5PB/umwQXPwnDzzpwfWOMOUwWkLpDQiZc+Cj0m+BGRCJQuRMW3A2fPgwL/wY5p8DgKZA2CiLjYfnTEBIGY77q6p/1K3hkGnx4L0y9PfB93vIBNNbChvkWkIwx3cICUncQgXEX7V8WnwFf/jOc9F1Y8QysegHeuWv/OiPOg9gU9z4r142OProPJl0Fia0fvtvFti90f279KLDfY4wxHgtIPS1lCJz+U/eqrYDdG6GxBsKioe/I/etOuwPWvgov3wAz74fErC+25/NBQ7UbZQEUb4A1L7vrVa3b68j2T73918LeUohJPrTjM8aYTrKAFEyiEiBrUvvb+wyEM++CN26Hv0yEgSdCdYkLTDP+AhFx8OwVbrrtjDshfQzMuRRqy+Hdu2DgKXDZnJZg1Z7aCncNa/BpsGkBbPsYRp7XhQdqjDFfZGnfR5rjZ8N3l8Axl0BNGST0d0Hj4dPgqQsg7y1IHw2v3wz/OBdi+8I334Hpv4CtH8D7fzjwd+QvAhSO/zaERnb9tN37f4B5N3Vtm8aYI56NkI5ESQNgxn0tnwuXw9OXQf5iuPARd63psydgy4dwzj1uui1zEhSvh48fgIlXQPLg9tvf/ilICAw8ye237eOu67sqLHrMZR6eeZcbFRpjDDZCOjr0Owaufx++/ZFLnhBxiQ8X/n3/az/T73D3QP33Njct157tC6HvGBcsBp4EO5ZBXVXX9LV0E1TkgzbB1g+7pk1jzFHBRkhHi5jkAycexGfAqT+Gt34Od2dD8hBIHQZ9ciAiFsKjIWWoG2mNv9jtM/BEeP/3bpQ07Iy22137itt38FQIOcD/cTa96/6UUPj8XRhxTvt192yBDW/A5G+6IOvzQdmWjkd3xpgjlgWk3ubk77sRVf4S2LkcSja5JIiGGjdqaTbgxJY/4zLg5Rvh6te+uLLE5+/AM5e7930GwfSfw5gL2v/+TQsgcQCkDW8JTu157Sewcb67Ryt9NCx9Cl79PtzwqQukR5OGGvjf79zfj01jml7KAlJvIwJDprpXaw01sHsDlBfAsDNdWUQsXPES/OM8eGIGnHWXWzUioZ9LB3/pO5A6Ar70I/j4frfEUcnPYMxXXLp52kgYea5ry9cEm/8Ho2a48jdud9/V1j1VO5a5YASw/jUXkFa/4Fa5WDvPfd/RJO8tl+yRMhQmXNbTvTGmR9g1JNMiPNqNnkaeC6F+/1fpOwq+8ZJbueG5q+CPI+HP4+Dxc6G6GL76sMv6u+4tGHexWxLpvonw9i9gzmWwfI5rp3CZS0EffJp7QfujpPd/D5GJbuWK9a+74Lf5fbdt3WsBOfwetWOp9+eynu2HMT3IRkimc/qNhx+uhZ0rXdLD9k+gYCmc8UvoP8HVCYt0wSnnZJc0MeIc+M8P4cXroWgNVO5y9QZNgdhUl5K+YT4cc9n+157yF7vrUqf+xCVhvHsXLPmHm1IceT6sexUqCr+4/t+RrOAz92ehBSTTe4na00g7LTc3VxcvXtzT3Tiy1O+FuVe3PNep3wT41nvu/SvfhyWPuzT2IVPd6hTFa911pug+8N3PWhamDYt2QeyyZ+HBE+G8P8Jx17b/vapuevJw+Xzuz+aAubcUGuu6Nhiqwj05UFvmjvO2/P1HqMYc4URkiarmHqie/as3gRURA5c949LGSzZCvN8v8rPvdkkTK+bAmnnga3SZgqff7u6Vikl2gSlpgHtY4agvu+nD5MGw7j9fDEhl2+CtX7jpr7KtMPOvbirR37aFEB4F6WMhJLTjvvt87mbj6D5w8RMucDw9yy2M+93POg4aviaoKmo/cDXWuVHgmK+6zMHaMhhwEmz7yF3HSx/dcd+MOQpZQDLdIzIO+h+7f1l4lAsYrYOGPxEYcS4sfMgFpH2f/wYf/9WlpSdmu5t+n73C/aIfcrpbHumV70HGWLeEUlMjzL/Nra4OEJUIp/wQTv5e+yOpFXNgszeay1/skj6aF51dOw/GfrX9fr91h7sJ+dSbvanHVj9qix51/fGXe40LSIXLLCCZXskCkgl+J97gpuuyT3Cfc69x155a/0JPHuJGY6nD3OjkoS/BM99wNwtvWuCCyQk3uGteK+e6oLFrtWu/sc7dsFu6yWUADjrV3a/V/1jYsxXe+60bwcX2dcH1o/tcJmFbwaxsuwuY8f3gvXtcMsbXn21ZQ1DVPWwRXIDMOs4t0TTqyxAe6xIbAplp5/MBeuARojHdLKDXkETkbOBeIBR4RFXvbrU9EngSmASUAJeo6hZv223AtUATcJOqzu+oTREZBMwBUoAlwDdUtd7vuy4E5gLHqepiEQkHHgEm4gLzk6r6m46Ox64hBZnyfDdyqdrlMgAnXuGm15pt/Qie+io01bkFaE/9CUz8htum6jL5Wj/yo1lkItSVw3XvuEzAd37pyqfd4e4T+s+P4OrX3UoWrb18o3ukyHc/c6tRvPQdl1V42TMQGu4SGP5+ustoLFzuAldCJnzzbXjsbJfafu0bX2y3fq/LhDzca2Mv3QAFi13/27qZWhXqqzv3IMjGegiLOLz++KvcBUufhOXPuFsPzv5117VtekxnryGhqgF54QLG58BgIAJYDoxuVec7wEPe+1nAM9770V79SGCQ105oR20CzwKzvPcPAd/2+5544H/AJ0CuV3YZMMd7HwNsAXI6OqZJkyapOcLUVak21LW/veAz1bWvqm58U3XnKtXaStWVc1X/NkX19dtcnZpy1d9kq/46S7WmTLWuWvXuHNX7clVf/ZHqJw+pFq1Xra9RzXtb9edJqq/d0vIdS55UvSNB9cVvqzY2qL7yA9Vf9lXds031V/3dtv/82NV97RbVuzJUmxpb9q8pU53//1TvTFX99yzV6pJDPx+781TvSHTf+cRM15/W/vtT1d8MUK0s6rit1S+r/ipTddvCQ++Pv6Ym1XuPdX377VB3jvbu6Zq2K3aqfnBv28drAg5YrJ2IG4G8D2kykKeqm9SNVOYAM1vVmQk84b2fC0wTEfHK56hqnapuBvK89tps09tnqtcGXpv+ywX8ErgHqPUrUyBWRMKAaKAe6GCBN3NEiojt+H/w/Y91j9YYOt1da4qMc4vTzl7Q8r/zqAS48DGX0h6V6BI1zv2dW/5o5bPw+k/ggePgVxnw1FfcY+r9b9yd+A2Yciss+xf8/TQ3XTh6JiRlwzGXtvQD3HRiw14oWOJGKqtecI+S/+g+N8ra+KabitzqLXir6pIjdq1p+/h8Prd24X9vc+8/+otLpZ/2f27k99Yd+9ffsQw++atLsvjkgfbPW80eN0qsr3RPPu4KWz+E0s/hggfdaLKxFlbNPfB+nfG/38KbP3PXBU3QCuQ1pExgu9/nfOD49uqoaqOIlOOm3DJxoxn/fZtv52+rzRSgTFUbW9cXkYlAtqr+R0Ru9tt3Li7wFeJGSD9Q1dLWByEis4HZAAMGDDjwUZuj07Dp+38ed1HLU4D3bIG8t132XcZYly0Xl7Z//dNudQHvtZvdVOCx3nJLJ33XXbca6rU/8CQIj4FHz3TXsorXQv+J8PXnXNAq+Myl0f/jPLcuYf5i+Pxt97j7U2+GU37g7gcDF6zeuN0FGIC9JbD6JXd96ks/goodbnWNodNc2r3P5+4bi0l13/Xp3+Gkm9qe1nvzDtfe+Evc9GTBErcyfEdU3WobxRvc9bhjLoXUoS3bl8+BiHgYfYGbmkwf6661HXfdAf96OlRX6aYAwQXPcV9rOUcmqBzVSQ0iEgL8Ebiqjc2Tcden+gN9gPdF5C1V3eRfSVUfBh4Gdw0poB02R6Y+OR3fEwXuus/oGTB4ihuFDDrV23cgfOOFlnpJA9xafUufcokb038BJ97YkqWXORG+9T93D9d797jgddavXar7gt/AB3+CjPFu9NVQ45ZdOv569wv+gz8B4oIguMd/bP6fu6b0zbfhw7+4wPLVv0Pf0fDQyS7p4rRb9z+WVS+4x5ucdBNM+Ynr5/9+766vFa12AbmtdPcVz8KLs5tPiAtAs9+FuL7u+tial9w6iBExrsqx34D/3gI7V7lAf6hWPOtGcqf/P3eT9eLH4YTrO96nodatNHL8t9zfr+kWgQxIBUC23+csr6ytOvne1FkiLrmho33bKi8BkkQkzBslNZfHA2OBBW5WjwxgnojMwF1D+q+qNgBFIvIhkAvsF5CM6VJRiS4odSQpu+Wx9u21cdFjboSRNrzlF+aEr7s18QqWuKDXWOdGF2f9xgXEsChXr3mB3PBo+Mrf4JHp8Ofx4Gtwjy0Z97WW9PoP/gyFK9yTjLOOg6K18PotkH08nHabCx4nfNsFw/Xekk4S6laGn/KTllHTni1uii/7BLjqVbdyx6NnuYV5r3zF3VdWX9UyhQluxfk3f+amFU+7zbV1sAkdqrD4McgY50aUm99zi9ge+/WOn5y86BE3sqwuds8YC0Y7V7r1Ja9946hZbDhgWXZegNkATMMFh0XAZaq62q/ODcA4Vb1eRGYBX1XVi0VkDPBv3CimP/A2MAyQ9toUkeeA51V1jog8BKxQ1b+26tMC4MfqsuxuAUaq6tUiEuu1NUtVV7R3TJZlZ45KnzwIa1+FaT+DASe0lJdth3d/DfmfQkleS/nwc1xAbB7J1FW2LAybNtIFl8+ehL27YfwsFzDXvepuXL7+AzcqBDfSmns1RCe7FPSwaPje8v2XkVpwtxt9+Rogdbi7b2zcxe66oKpL5y9c5jIV+452S1z52zAf/n0xfPleF2zzl8AjU92o86xfQdE6l0E5/ectv9RrK+DeY9xxqQ9uWtrS52Cy4B5Y8GuY+jMXbINYZ7PsAp32fS7wZ1x23GOq+isRuROXcTFPRKKAp4BjgVJcQNjk7Xs7cA3QCHxfVV9vr02vfDAuySEZWApcrqp1rfqzgJaAFAc8jsvoE+BxVf1dR8djAcn0WntL3fWrmj3u/qsDLW1UW+ESCT55yF0viusL5/3B3Wvlb+2rsOF197/93Gtc0GitpswFuYUPunpRiW7Ks6qo5UblZoOmuFFhbJobLX7wRxcQr//AJbgAzLvJXZu6ch68fIMbvaUMc9OWUYktv+gv+Sc8dzXkXu2SWJpV7nTX2Vqfg4Il7j61pGy6xT/Ohy3vuynSa17vnu88REERkI42FpCMOUj1e929V6Hhh9+WqkvgWP0ifL7AjaRO/p7LiqwucYHtw79AdVHLPsde7qYs/Z8xtbfUZS7WlrlkkGn/526CzvmSuzds0SMuo3HWv9w9ZKtecNf5wqLcDc8rnoGBJ7vt0Umuza0fwxPnu0D4zXcgLh0+vNd9br73rbWSz929dAeawm1LQy3cPcBNYTY1wC2bXTANUhaQAsACkjFBrqHGjaLqq9wv6PYy/5b92wWbCx6ECZe6jMLXfuwCVNZxcMFf3ZqJRevgoVPclCG4acXRM2HV826K8vw/uScxP3qGG4FVl0ByjptC3Ojd3HzJP784MqzcBX871V2juu7NA2cotrblA5dpedJNLpX/4qdc0kyQsoAUABaQjDmK1JS1jHDArYeYmN1ybaxZ6SbYvdFdU8o5xQWgTe+5hIw679bFiHg35Ve2zV2zkhCX/bjiGRfUvvRDl9EYFulGde/+xk3xRSW617fec0kmrfl8+19Ta6h1a0AuuNu9bs6DvxzrplFn/KXrz1F5gTuWw1zd3gJSAFhAMsbss7fUjVS2L4ThZ7Wk8m9a4DL4Mie5600Pnw6VO1xSRs0eNyoC+MrDbo3Gf37V3c818QqXmBGT7J739frNbh3Erz0Og06D+T+FxY/CxU+6hXvrKtwtAM9c7q7v/WB11zxypZnP5274Do+B698/rKbs8RPGGBNIMclumqz1VNng01rex2e4e63qKl0WX321uw9KQlpWuT/xRneD8grv5t24DFfP1+Cm/v71Ncia7FaCj01zT21WH0z27ukaOt2t1jH/p+6m4uzJXROYNr7Rkl25c6VLnQ8wGyEdBBshGWMComKHuzeraK17+Rphyi0uAD17hVvm6Yw73b1mj53lAsWlz8CIs6G2HJ7/pkv48DW6+8i+/BcXtLa8D5m5X1w5BFySyNYP3bJTNaUw9Ax3v1mzJ77sVtXYW+KC32EsdGtTdgFgAckY0+2aGqF8m0uyAHd/2Mrn3MjKf53G2nKXBfjur919U9UlboWKiHh3DSv7eDcyCwl1I7AP/+ymF5uFRsKFf3dJGztXuZU6pv/cTQdu/Qh+tO6QsyUtIAWABSRjTNDLewve/LlbbmnUDHeT8oY27lOKSnIrYIz9qgtUcy6D7Z+6VTbKtrunLv9wDWz7xD0p+dI5MOKcQ+qSXUMyxpjeaOj0lsV6AUae60Y8e3e7abzmBzRm5e7//LArXnaL/+5Y5rZPucVtHzrd3Qi87F+HHJA6ywKSMcYc7TqzOG14NMy8/4vloeFuMdqG2i9u62IWkIwxxnTs1JsPXKcLBPIBfcYYY0ynWUAyxhgTFCwgGWOMCQoWkIwxxgQFC0jGGGOCggUkY4wxQcECkjHGmKBgAckYY0xQsLXsDoKIFANbD6OJVGB3F3UnUIK9j8HeP7A+dhXrY9cIhj4OVNU2lhzfnwWkbiQiizuzwGBPCvY+Bnv/wPrYVayPXeNI6GMzm7IzxhgTFCwgGWOMCQoWkLrXwz3dgU4I9j4Ge//A+thVrI9d40joI2DXkIwxxgQJGyEZY4wJChaQjDHGBAULSN1ARM4WkfUikicit/Z0fwBEJFtE3hWRNSKyWkS+55Uni8ibIrLR+7PPgdrqhr6GishSEXnV+zxIRBZ65/MZEYno4f4lichcEVknImtF5MRgOo8i8gPv73iViDwtIlHBcA5F5DERKRKRVX5lbZ43cf7i9XeFiEzsof79zvt7XiEiL4pIkt+227z+rReRswLdv/b66LftRyKiIpLqfe72c3iwLCAFmIiEAg8A5wCjgUtFZHTP9gqARuBHqjoaOAG4wevXrcDbqjoMeNv73NO+B6z1+3wP8CdVHQrsAa7tkV61uBf4r6qOBI7B9TUozqOIZAI3AbmqOhYIBWYRHOfwH8DZrcraO2/nAMO812zgwR7q35vAWFUdD2wAbgPwfnZmAWO8ff7q/ez3RB8RkWzgTGCbX3FPnMODYgEp8CYDeaq6SVXrgTnAzB7uE6paqKqfee8rcb9EM3F9e8Kr9gRwQc/00BGRLOA84BHvswBTgblelR7to4gkAqcCjwKoar2qlhFc5zEMiBaRMCAGKCQIzqGq/g8obVXc3nmbCTypzidAkoj06+7+qeobqtroffwEyPLr3xxVrVPVzUAe7mc/oNo5hwB/An4C+Getdfs5PFgWkAIvE9ju9znfKwsaIpIDHAssBNJVtdDbtBNI76FuNfsz7gfL531OAcr8fin09PkcBBQDj3vTio+ISCxBch5VtQD4Pe5/yoVAObCE4DqH/to7b8H4c3QN8Lr3Pmj6JyIzgQJVXd5qU9D0sT0WkHo5EYkDnge+r6oV/tvU3RPQY/cFiMj5QJGqLumpPnRCGDAReFBVjwWqaTU915Pn0bsGMxMXOPsDsbQxxROMevrfX0dE5HbctPe/erov/kQkBvgp8H893ZdDYQEp8AqAbL/PWV5ZjxORcFww+peqvuAV72oexnt/FvVU/4CTgRkisgU31TkVd70myZt+gp4/n/lAvqou9D7PxQWoYDmP04HNqlqsqg3AC7jzGkzn0F975y1ofo5E5CrgfODr2nIjZ7D0bwjuPx/LvZ+bLOAzEckgePrYLgtIgbcIGOZlNUXgLnzO6+E+NV+LeRRYq6p/9Ns0D7jSe38l8HJ3962Zqt6mqlmqmoM7b++o6teBd4GLvGo93cedwHYRGeEVTQPWEDzncRtwgojEeH/nzf0LmnPYSnvnbR5whZcpdgJQ7je1121E5GzcFPIMVd3rt2keMEtEIkVkEC5x4NPu7p+qrlTVvqqa4/3c5AMTvX+nQXEOO6Sq9grwCzgXl5HzOXB7T/fH69MpuOmQFcAy73Uu7hrN28BG4C0guaf76vX3NOBV7/1g3A97HvAcENnDfZsALPbO5UtAn2A6j8AvgHXAKuApIDIYziHwNO66VgPuF+e17Z03QHDZqp8DK3FZgz3RvzzcdZjmn5mH/Orf7vVvPXBOT53DVtu3AKk9dQ4P9mVLBxljjAkKNmVnjDEmKFhAMsYYExQsIBljjAkKFpCMMcYEBQtIxhhjgoIFJGOCjIg0icgyv1eXLcwqIjltrQxtTDAIO3AVY0w3q1HVCT3dCWO6m42QjDlCiMgWEfmtiKwUkU9FZKhXniMi73jPuHlbRAZ45eneM3uWe6+TvKZCReTv4p6R9IaIRPfYQRnjxwKSMcEnutWU3SV+28pVdRxwP24ldID7gCfUPaPnX8BfvPK/AO+p6jG49fVWe+XDgAdUdQxQBlwY4OMxplNspQZjgoyIVKlqXBvlW4CpqrrJWxh3p6qmiMhuoJ+qNnjlhaqaKiLFQJaq1vm1kQO8qe4BeIjILUC4qt4V+CMzpmM2QjLmyKLtvD8YdX7vm7BrySZIWEAy5shyid+fH3vvP8Kthg7wdeB97/3bwLcBRCTUe7qtMUHL/mdkTPCJFpFlfp//q6rNqd99RGQFbpRzqVf2XdwTa2/GPb32aq/8e8DD/7+dO7YBGAaBAGh2ykapvHt2wIWzgpWPdDcB3etBoqrusZvQHPszNERyQ4KfeG9IV3c/X88CJ1jZARBBQwIggoYEQASBBEAEgQRABIEEQASBBECEBSgCJS51xybMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY [[0.99781907]]\n",
      "QQQ [[0.99573797]]\n",
      "XLE [[0.99497455]]\n",
      "XLF [[1.0024977]]\n",
      "XLK [[0.99753517]]\n",
      "XLP [[1.0021014]]\n",
      "XLV [[1.0002216]]\n",
      "XLU [[0.999964]]\n",
      "XLY [[1.0035602]]\n",
      "XLI [[0.9998115]]\n"
     ]
    }
   ],
   "source": [
    "print \"{} {}\".format('SPY', model.predict(np.array(convert_to_train(SPY.copy(), 0)[0][:1])))\n",
    "print \"{} {}\".format('QQQ', model.predict(np.array(convert_to_train(QQQ.copy(), 0)[0][:1])))\n",
    "print \"{} {}\".format('XLE', model.predict(np.array(convert_to_train(XLE.copy(), 0)[0][:1])))\n",
    "print \"{} {}\".format('XLF', model.predict(np.array(convert_to_train(XLF.copy(), 0)[0][:1])))\n",
    "print \"{} {}\".format('XLK', model.predict(np.array(convert_to_train(XLK.copy(), 0)[0][:1])))\n",
    "print \"{} {}\".format('XLP', model.predict(np.array(convert_to_train(XLP.copy(), 0)[0][:1])))\n",
    "print \"{} {}\".format('XLV', model.predict(np.array(convert_to_train(XLV.copy(), 0)[0][:1])))\n",
    "print \"{} {}\".format('XLU', model.predict(np.array(convert_to_train(XLU.copy(), 0)[0][:1])))\n",
    "print \"{} {}\".format('XLY', model.predict(np.array(convert_to_train(XLY.copy(), 0)[0][:1])))\n",
    "print \"{} {}\".format('XLI', model.predict(np.array(convert_to_train(XLI.copy(), 0)[0][:1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
