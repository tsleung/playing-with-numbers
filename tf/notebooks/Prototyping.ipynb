{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "\n",
    "Creates a neural network which evaluates a time series and produce a set of predicted values for the time series\n",
    "Predicted values may be used in a policy to make a trade. This policy may be modeled by simple multiple regression or a neural network.\n",
    "\n",
    "## Data\n",
    "Test data is taken as most recent to avoid lookahead bias. Train data is split into a validation and training set during fitting.\n",
    "\n",
    "\n",
    "## TODO\n",
    "- Adding VIX as a signal\n",
    "- Adding High/Low as signals\n",
    "- Multiple securities/ aggregate samples\n",
    "- Policy network\n",
    "- Regularization (l2)\n",
    "- Dilated convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "NUM_INPUT_NEURONS = 32\n",
    "NUM_OUTPUT_NEURONS = 1\n",
    "DAY_OFFSET = 5\n",
    "\n",
    "def from_network(symbol):\n",
    "    return pdr.get_data_yahoo(symbols=symbol, start=datetime(1900, 1, 1))\n",
    "\n",
    "def from_file(symbol):\n",
    "    dataset_path = keras.utils.get_file(\"{}.csv\".format(symbol), \"http://localhost:8000/data/daily/{}.csv\".format(symbol))\n",
    "    column_names = ['Date','Open','High','Low','Close','Adj Close','Volume'] \n",
    "    return pd.read_csv(dataset_path, \n",
    "                              names=column_names, \n",
    "                              dtype={'Close': np.float64,'Open': np.float64,'High': np.float64,'Adj Close': np.float64, 'Volume': np.float64},\n",
    "                              header=0,\n",
    "                              na_values = \"?\", \n",
    "                              comment='\\t',\n",
    "                              sep=\",\",\n",
    "                              skipinitialspace=True)\n",
    "\n",
    "\n",
    "# Create features (only close price for now)\n",
    "def convert_to_percentage(old, new):\n",
    "    return (old - new) / old\n",
    "\n",
    "def convert_labels_to_category(labels): \n",
    "    # Simplification - If positive return, 1, else 0\n",
    "    # return map(lambda arr: 1 if arr[0] > 1 else 0, labels)\n",
    "    # rounding simpliciation\n",
    "    return map(lambda arr: map(lambda val: round(val,4),arr), labels)\n",
    "\n",
    "def convert_to_train(raw_dataset, offset=5):\n",
    "    dataset = raw_dataset.copy()\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(offset, len(dataset) - NUM_INPUT_NEURONS):\n",
    "\n",
    "        feature_dataset = dataset[i:i+NUM_INPUT_NEURONS].copy()\n",
    "        latest_close = feature_dataset['Close'].iloc[0]\n",
    "        \n",
    "        close_features = feature_dataset['Close'].map(lambda current: convert_to_percentage(latest_close, current)).tolist()\n",
    "        high_features = feature_dataset['High'].map(lambda current: convert_to_percentage(latest_close, current)).tolist()\n",
    "        low_features = feature_dataset['Low'].map(lambda current: convert_to_percentage(latest_close, current)).tolist()\n",
    "        open_features = feature_dataset['Open'].map(lambda current: convert_to_percentage(latest_close, current)).tolist()\n",
    "        features.append(\n",
    "            zip(\n",
    "                close_features,\n",
    "                high_features,\n",
    "                low_features,\n",
    "                open_features,\n",
    "            )\n",
    "        )\n",
    "        labels.append([\n",
    "            dataset['Close'].iloc[i-1] / latest_close, # 1 day trade\n",
    "#             dataset['Close'].iloc[i-2] / latest_close, # 2 day trade\n",
    "#             dataset['Close'].iloc[i-3] / latest_close, # 3 day trade\n",
    "#             dataset['Close'].iloc[i-4] / latest_close, # 4 day trade\n",
    "#             dataset['Close'].iloc[i-5] / latest_close, # 5 day trade\n",
    "        ])\n",
    "        \n",
    "    # Without converting labels the precision is hard to determine accuracy. \n",
    "    # Rather than crude 0/1, maybe this can be more sophisticated\n",
    "    labels = convert_labels_to_category(labels)\n",
    "    \n",
    "    return [features,labels]\n",
    "def split_data(symbol):\n",
    "    fetched = from_network(symbol).sort_values(by=['Date'],ascending=False)\n",
    "    converted = convert_to_train(fetched)\n",
    "    features = converted[0]\n",
    "    labels = converted[1]\n",
    "    prediction = [\n",
    "        features[:5],\n",
    "        labels[:5],\n",
    "    ]\n",
    "    validation = [\n",
    "        features[5:1000],\n",
    "        labels[5:1000]\n",
    "    ]\n",
    "    training = [\n",
    "        features[1000:],\n",
    "        labels[1000:]\n",
    "    ]\n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'prediction': prediction,\n",
    "        'validation': validation,\n",
    "        'training': training,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset = raw_dataset.copy()\n",
    "#dataset = from_network('SPY').sort_values(by=['Date'],ascending=False)\n",
    "# add function to cache fetch\n",
    "QQQ = from_network('QQQ')\n",
    "SPY = from_network('SPY')\n",
    "XLK = from_network('XLK')\n",
    "XLF = from_network('XLF')\n",
    "XLE = from_network('XLE')\n",
    "XLP = from_network('XLP')\n",
    "XLV = from_network('XLV')\n",
    "XLY = from_network('XLY')\n",
    "XLI = from_network('XLI')\n",
    "XLU = from_network('XLU')\n",
    "\n",
    "#dataset = pd.concat([QQQ,SPY,XLK,XLF,XLE,XLP,XLV,XLY,XLI,XLU]).sort_values(by=['Date'],ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepped_data = map(split_data, [\n",
    "    'QQQ',\n",
    "    'SPY',\n",
    "#     'XLK',\n",
    "#     'XLF',\n",
    "#     'XLE',\n",
    "#     'XLP',\n",
    "#     'XLV',\n",
    "#     'XLY',\n",
    "#     'XLI',\n",
    "#     'XLU',\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQQ\n",
      "SPY\n",
      "0\n",
      "3966\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(prepped_data)):\n",
    "    print prepped_data[i]['symbol']\n",
    "    \n",
    "def combine_all(accum, prep):\n",
    "    \n",
    "    print len(accum['training'][0])\n",
    "    return {   \n",
    "        'prediction':[\n",
    "            accum['prediction'][0] + prep['prediction'][0],\n",
    "            accum['prediction'][1] + prep['prediction'][1],\n",
    "        ],\n",
    "        'validation':[\n",
    "            accum['validation'][0] + prep['validation'][0],\n",
    "            accum['validation'][1] + prep['validation'][1],\n",
    "        ],\n",
    "        'training':[\n",
    "            accum['training'][0] + prep['training'][0],\n",
    "            accum['training'][1] + prep['training'][1],\n",
    "        ],\n",
    "    }\n",
    "combined = reduce(combine_all, prepped_data,{\n",
    "    'prediction':[[],[]],\n",
    "    'validation':[[],[]],\n",
    "    'training':[[],[]],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9474"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "len(combined['training'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9474\n",
      "1990\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#len(converted_feature_set[0][0])\n",
    "print len(combined['training'][0])\n",
    "train_data = np.array(combined['training'][0])\n",
    "train_labels = np.array(combined['training'][1])\n",
    "\n",
    "print len(combined['validation'][0])\n",
    "test_data = np.array(combined['validation'][0])\n",
    "test_labels = np.array(combined['validation'][1])\n",
    "\n",
    "print len(combined['prediction'][0])\n",
    "prediction_data = np.array(combined['prediction'][0])\n",
    "prediction_labels = np.array(combined['prediction'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print train_data\n",
    "#print train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 28, 32)            672       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                28704     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24)                792       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 32,593\n",
      "Trainable params: 32,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    #layers.Dense(64, activation=tf.nn.relu, input_shape=[NUM_INPUT_NEURONS]),\n",
    "    layers.Conv1D(32, kernel_size=(5), strides=(1),\n",
    "        activation='relu',\n",
    "        input_shape=(32,4)),\n",
    "#    layers.MaxPooling1D(3),\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(32, activation=tf.nn.relu),\n",
    "    layers.Dense(32, activation=tf.nn.relu),\n",
    "    layers.Dense(32, activation=tf.nn.relu),\n",
    "    layers.Dense(24, activation=tf.nn.relu),\n",
    "    layers.Dense(12, activation=tf.nn.relu),\n",
    "\n",
    "      \n",
    "#     layers.Dense(64, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(48, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(36, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(24, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(12, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "    layers.Dense(NUM_OUTPUT_NEURONS)\n",
    "  ])\n",
    "\n",
    "  model.compile(\n",
    "#       loss='mean_squared_logarithmic_error',\n",
    "#       loss='mean_squared_error',\n",
    "     loss='logcosh',\n",
    "#       loss='mean_absolute_error',\n",
    "                optimizer='sgd',\n",
    "#                 metrics=[\n",
    "#                     'mae',\n",
    "#                 ]\n",
    "               )\n",
    "  return model\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7579 samples, validate on 1895 samples\n",
      "Epoch 1/2\n",
      "7579/7579 [==============================] - 1s 66us/step - loss: 1.3486e-04 - val_loss: 5.5197e-05\n",
      "Epoch 2/2\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3480e-04 - val_loss: 5.5681e-05\n",
      "Train on 7579 samples, validate on 1895 samples\n",
      "Epoch 1/60\n",
      "7579/7579 [==============================] - 1s 69us/step - loss: 1.3480e-04 - val_loss: 5.5143e-05\n",
      "Epoch 2/60\n",
      "7579/7579 [==============================] - 1s 70us/step - loss: 1.3475e-04 - val_loss: 5.5137e-05\n",
      "Epoch 3/60\n",
      "7579/7579 [==============================] - 1s 68us/step - loss: 1.3479e-04 - val_loss: 5.5116e-05\n",
      "Epoch 4/60\n",
      "7579/7579 [==============================] - 1s 68us/step - loss: 1.3473e-04 - val_loss: 5.5323e-05\n",
      "Epoch 5/60\n",
      "7579/7579 [==============================] - 1s 68us/step - loss: 1.3472e-04 - val_loss: 5.5362e-05\n",
      "Epoch 6/60\n",
      "7579/7579 [==============================] - 1s 66us/step - loss: 1.3464e-04 - val_loss: 5.5524e-05\n",
      "Epoch 7/60\n",
      "7579/7579 [==============================] - 1s 67us/step - loss: 1.3462e-04 - val_loss: 5.5301e-05\n",
      "Epoch 8/60\n",
      "7579/7579 [==============================] - 1s 68us/step - loss: 1.3463e-04 - val_loss: 5.5476e-05\n",
      "Epoch 9/60\n",
      "7579/7579 [==============================] - 1s 70us/step - loss: 1.3458e-04 - val_loss: 5.5286e-05\n",
      "Epoch 10/60\n",
      "7579/7579 [==============================] - 1s 68us/step - loss: 1.3456e-04 - val_loss: 5.5250e-05\n",
      "Epoch 11/60\n",
      "7579/7579 [==============================] - 0s 62us/step - loss: 1.3454e-04 - val_loss: 5.5133e-05\n",
      "Epoch 12/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3442e-04 - val_loss: 5.5285e-05\n",
      "Epoch 13/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3446e-04 - val_loss: 5.5097e-05\n",
      "Epoch 14/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3444e-04 - val_loss: 5.5870e-05\n",
      "Epoch 15/60\n",
      "7579/7579 [==============================] - 0s 62us/step - loss: 1.3439e-04 - val_loss: 5.5003e-05\n",
      "Epoch 16/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3438e-04 - val_loss: 5.5448e-05\n",
      "Epoch 17/60\n",
      "7579/7579 [==============================] - 0s 65us/step - loss: 1.3434e-04 - val_loss: 5.5132e-05\n",
      "Epoch 18/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3435e-04 - val_loss: 5.5180e-05\n",
      "Epoch 19/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3437e-04 - val_loss: 5.5092e-05\n",
      "Epoch 20/60\n",
      "7579/7579 [==============================] - 0s 62us/step - loss: 1.3429e-04 - val_loss: 5.5207e-05\n",
      "Epoch 21/60\n",
      "7579/7579 [==============================] - 0s 65us/step - loss: 1.3426e-04 - val_loss: 5.5697e-05\n",
      "Epoch 22/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3423e-04 - val_loss: 5.4919e-05\n",
      "Epoch 23/60\n",
      "7579/7579 [==============================] - 0s 65us/step - loss: 1.3427e-04 - val_loss: 5.5048e-05\n",
      "Epoch 24/60\n",
      "7579/7579 [==============================] - 0s 62us/step - loss: 1.3425e-04 - val_loss: 5.5210e-05\n",
      "Epoch 25/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3412e-04 - val_loss: 5.5400e-05\n",
      "Epoch 26/60\n",
      "7579/7579 [==============================] - 0s 65us/step - loss: 1.3418e-04 - val_loss: 5.5125e-05\n",
      "Epoch 27/60\n",
      "7579/7579 [==============================] - 0s 66us/step - loss: 1.3416e-04 - val_loss: 5.5299e-05\n",
      "Epoch 28/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3409e-04 - val_loss: 5.5723e-05\n",
      "Epoch 29/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3409e-04 - val_loss: 5.5139e-05\n",
      "Epoch 30/60\n",
      "7579/7579 [==============================] - 1s 66us/step - loss: 1.3410e-04 - val_loss: 5.4990e-05\n",
      "Epoch 31/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3400e-04 - val_loss: 5.5123e-05\n",
      "Epoch 32/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3404e-04 - val_loss: 5.5153e-05\n",
      "Epoch 33/60\n",
      "7579/7579 [==============================] - 0s 66us/step - loss: 1.3403e-04 - val_loss: 5.5393e-05\n",
      "Epoch 34/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3400e-04 - val_loss: 5.5066e-05\n",
      "Epoch 35/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3398e-04 - val_loss: 5.5413e-05\n",
      "Epoch 36/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3394e-04 - val_loss: 5.5044e-05\n",
      "Epoch 37/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3393e-04 - val_loss: 5.5218e-05\n",
      "Epoch 38/60\n",
      "7579/7579 [==============================] - 0s 65us/step - loss: 1.3390e-04 - val_loss: 5.5455e-05\n",
      "Epoch 39/60\n",
      "7579/7579 [==============================] - 0s 62us/step - loss: 1.3381e-04 - val_loss: 5.5267e-05\n",
      "Epoch 40/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3388e-04 - val_loss: 5.5113e-05\n",
      "Epoch 41/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3387e-04 - val_loss: 5.5003e-05\n",
      "Epoch 42/60\n",
      "7579/7579 [==============================] - 1s 70us/step - loss: 1.3386e-04 - val_loss: 5.5018e-05\n",
      "Epoch 43/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3382e-04 - val_loss: 5.5056e-05\n",
      "Epoch 44/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3381e-04 - val_loss: 5.5004e-05\n",
      "Epoch 45/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3376e-04 - val_loss: 5.5309e-05\n",
      "Epoch 46/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3376e-04 - val_loss: 5.4894e-05\n",
      "Epoch 47/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3374e-04 - val_loss: 5.5187e-05\n",
      "Epoch 48/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3372e-04 - val_loss: 5.5121e-05\n",
      "Epoch 49/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3369e-04 - val_loss: 5.4856e-05\n",
      "Epoch 50/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3368e-04 - val_loss: 5.4947e-05\n",
      "Epoch 51/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3360e-04 - val_loss: 5.5820e-05\n",
      "Epoch 52/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3367e-04 - val_loss: 5.4933e-05\n",
      "Epoch 53/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3362e-04 - val_loss: 5.4873e-05\n",
      "Epoch 54/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3358e-04 - val_loss: 5.5035e-05\n",
      "Epoch 55/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3356e-04 - val_loss: 5.4988e-05\n",
      "Epoch 56/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3355e-04 - val_loss: 5.5398e-05\n",
      "Epoch 57/60\n",
      "7579/7579 [==============================] - 0s 62us/step - loss: 1.3349e-04 - val_loss: 5.4819e-05\n",
      "Epoch 58/60\n",
      "7579/7579 [==============================] - 0s 64us/step - loss: 1.3350e-04 - val_loss: 5.5511e-05\n",
      "Epoch 59/60\n",
      "7579/7579 [==============================] - 0s 63us/step - loss: 1.3335e-04 - val_loss: 5.4775e-05\n",
      "Epoch 60/60\n",
      "7579/7579 [==============================] - 0s 62us/step - loss: 1.3350e-04 - val_loss: 5.4889e-05\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=2, validation_split = 0.2, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=60, validation_split = 0.2, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=1990, minmax=(array([0.9542]), array([1.0624])), mean=array([1.00040372]), variance=array([9.89254361e-05]), skewness=array([-0.34777366]), kurtosis=array([4.06081482]))\n",
      "DescribeResult(nobs=1990, minmax=(array([0.9917507], dtype=float32), array([1.0081553], dtype=float32)), mean=array([1.0001328], dtype=float32), variance=array([2.3961315e-06], dtype=float32), skewness=array([0.56681484], dtype=float32), kurtosis=array([3.823104], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print stats.describe(test_labels)\n",
    "print stats.describe(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999989]\n",
      "[0.9963]\n",
      "[[ 0.         -0.00055801  0.01599704  0.01041662]\n",
      " [ 0.00285213 -0.00148813  0.0112227   0.00706845]\n",
      " [ 0.01091266  0.00725445  0.02529763  0.01078872]\n",
      " [ 0.01977928  0.01500495  0.03205604  0.02889387]\n",
      " [ 0.03131202  0.02653769  0.05915174  0.05543156]\n",
      " [ 0.0710565   0.04972721  0.07310264  0.0538194 ]\n",
      " [ 0.0396825   0.03428819  0.06448409  0.06380204]\n",
      " [ 0.04352681  0.03906252  0.05313735  0.04265876]\n",
      " [ 0.05152528  0.03528027  0.05927578  0.04420886]\n",
      " [ 0.05102924  0.05022325  0.08804562  0.06764635]\n",
      " [ 0.05468745  0.0541915   0.10658484  0.10044641]\n",
      " [ 0.11024305  0.0824653   0.11049102  0.09424601]\n",
      " [ 0.08500739  0.04458087  0.09027776  0.05102924]\n",
      " [ 0.0557416   0.03354417  0.06752232  0.04420886]\n",
      " [ 0.04185268  0.00347221  0.04923117  0.01922127]\n",
      " [ 0.01773314  0.00824654  0.02628972  0.01630707]\n",
      " [ 0.02387157 -0.00241815  0.03168403  0.00551835]\n",
      " [ 0.00124006 -0.015687    0.00359624 -0.01196682]\n",
      " [-0.02368556 -0.03435025 -0.01705109 -0.03007196]\n",
      " [-0.02337552 -0.03918655 -0.02281751 -0.02926588]\n",
      " [-0.01444694 -0.02783982 -0.00589036 -0.02759175]\n",
      " [-0.01109876 -0.01550099  0.01159471  0.00124006]\n",
      " [-0.00062008 -0.0362103   0.00260416 -0.03025797]\n",
      " [-0.03478423 -0.03490826 -0.00303823 -0.00682047]\n",
      " [-0.02752978 -0.06591025 -0.02628972 -0.06293399]\n",
      " [-0.0685144  -0.07459077 -0.05102924 -0.07335071]\n",
      " [-0.05016119 -0.05078127 -0.03881445 -0.04402286]\n",
      " [-0.0425967  -0.04947914 -0.03435025 -0.0416047 ]\n",
      " [-0.04600693 -0.04600693 -0.01357888 -0.0207093 ]\n",
      " [-0.01339288 -0.01593507  0.00062008 -0.00446429]\n",
      " [-0.00998264 -0.01047869  0.00322423 -0.00099209]\n",
      " [ 0.01283477  0.00272819  0.01357888  0.01357888]]\n"
     ]
    }
   ],
   "source": [
    "print outputs[0]\n",
    "print test_labels[0]\n",
    "print test_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primitive policy temporarily in place of a RL policy network\n",
    "\n",
    "# signal step for our policy network\n",
    "signals = map(lambda arr: sum(arr) / len(arr),outputs)\n",
    "# signals = map(lambda arr: arr[0],outputs)\n",
    "trades = map(lambda signal: 1 if round(signal,4) > 1 else 0,signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one day return\n",
    "actual = map(lambda arr: arr[0],test_labels)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'signal':signals,\n",
    "    'actual': actual,\n",
    "    'trade':trades,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['entry_success'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['entry_failure'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['avoid_success'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 0) else 0,axis=1)\n",
    "df['avoid_failure'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 0) else 0,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>signal</th>\n",
       "      <th>trade</th>\n",
       "      <th>entry_success</th>\n",
       "      <th>entry_failure</th>\n",
       "      <th>avoid_success</th>\n",
       "      <th>avoid_failure</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9963</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0029</td>\n",
       "      <td>0.998994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0081</td>\n",
       "      <td>1.001618</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0090</td>\n",
       "      <td>1.000459</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0119</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0428</td>\n",
       "      <td>1.000054</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.998201</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0040</td>\n",
       "      <td>1.001339</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0084</td>\n",
       "      <td>1.004279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9995</td>\n",
       "      <td>1.005271</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0039</td>\n",
       "      <td>1.006141</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0624</td>\n",
       "      <td>1.007130</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.9724</td>\n",
       "      <td>1.006657</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.9690</td>\n",
       "      <td>1.005112</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.000949</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0063</td>\n",
       "      <td>1.000172</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9773</td>\n",
       "      <td>1.000703</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.997979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0003</td>\n",
       "      <td>0.997982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0088</td>\n",
       "      <td>0.997544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0033</td>\n",
       "      <td>0.991751</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0105</td>\n",
       "      <td>0.997002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.9670</td>\n",
       "      <td>0.996616</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0071</td>\n",
       "      <td>0.998658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9616</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0175</td>\n",
       "      <td>0.999455</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0073</td>\n",
       "      <td>1.000939</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.9967</td>\n",
       "      <td>1.001776</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0322</td>\n",
       "      <td>1.002272</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>0.9939</td>\n",
       "      <td>1.000959</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>1.0127</td>\n",
       "      <td>1.001955</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>0.9977</td>\n",
       "      <td>1.001866</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>0.9838</td>\n",
       "      <td>1.003046</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>1.0041</td>\n",
       "      <td>1.002284</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>0.9859</td>\n",
       "      <td>1.001481</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>1.0011</td>\n",
       "      <td>1.000926</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>0.9958</td>\n",
       "      <td>0.999742</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>0.9959</td>\n",
       "      <td>0.999697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>1.0063</td>\n",
       "      <td>1.000305</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>0.9966</td>\n",
       "      <td>0.999781</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>0.9988</td>\n",
       "      <td>1.000768</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.999029</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>1.0028</td>\n",
       "      <td>1.000399</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.999650</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>1.0060</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.999701</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>1.0001</td>\n",
       "      <td>0.998774</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>1.0016</td>\n",
       "      <td>1.000037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>1.0041</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1.0096</td>\n",
       "      <td>1.000404</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>1.0006</td>\n",
       "      <td>1.000072</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>1.0107</td>\n",
       "      <td>0.999211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.998577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>0.9972</td>\n",
       "      <td>1.000087</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>1.0101</td>\n",
       "      <td>0.998608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>1.0145</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>1.0124</td>\n",
       "      <td>1.000373</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0.9874</td>\n",
       "      <td>0.998466</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1990 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual    signal  trade  entry_success  entry_failure  avoid_success  \\\n",
       "0     0.9963  0.999999      0              0              0              1   \n",
       "1     1.0029  0.998994      0              0              0              0   \n",
       "2     1.0081  1.001618      1              1              0              0   \n",
       "3     1.0090  1.000459      1              1              0              0   \n",
       "4     1.0119  0.999805      0              0              0              0   \n",
       "5     1.0428  1.000054      1              1              0              0   \n",
       "6     0.9673  0.998201      0              0              0              1   \n",
       "7     1.0040  1.001339      1              1              0              0   \n",
       "8     1.0084  1.004279      1              1              0              0   \n",
       "9     0.9995  1.005271      1              0              1              0   \n",
       "10    1.0039  1.006141      1              1              0              0   \n",
       "11    1.0624  1.007130      1              1              0              0   \n",
       "12    0.9724  1.006657      1              0              1              0   \n",
       "13    0.9690  1.005112      1              0              1              0   \n",
       "14    0.9855  1.000949      1              0              1              0   \n",
       "15    0.9754  0.999580      0              0              0              1   \n",
       "16    1.0063  1.000172      1              1              0              0   \n",
       "17    0.9773  1.000703      1              0              1              0   \n",
       "18    0.9757  0.997979      0              0              0              1   \n",
       "19    1.0003  0.997982      0              0              0              0   \n",
       "20    1.0088  0.997544      0              0              0              0   \n",
       "21    1.0033  0.991751      0              0              0              0   \n",
       "22    1.0105  0.997002      0              0              0              0   \n",
       "23    0.9670  0.996616      0              0              0              1   \n",
       "24    1.0071  0.998658      0              0              0              0   \n",
       "25    0.9616  0.999784      0              0              0              1   \n",
       "26    1.0175  0.999455      0              0              0              0   \n",
       "27    1.0073  1.000939      1              1              0              0   \n",
       "28    0.9967  1.001776      1              0              1              0   \n",
       "29    1.0322  1.002272      1              1              0              0   \n",
       "...      ...       ...    ...            ...            ...            ...   \n",
       "1960  0.9939  1.000959      1              0              1              0   \n",
       "1961  1.0127  1.001955      1              1              0              0   \n",
       "1962  0.9977  1.001866      1              0              1              0   \n",
       "1963  0.9838  1.003046      1              0              1              0   \n",
       "1964  1.0041  1.002284      1              1              0              0   \n",
       "1965  0.9859  1.001481      1              0              1              0   \n",
       "1966  1.0011  1.000926      1              1              0              0   \n",
       "1967  0.9958  0.999742      0              0              0              1   \n",
       "1968  0.9959  0.999697      0              0              0              1   \n",
       "1969  1.0063  1.000305      1              1              0              0   \n",
       "1970  0.9966  0.999781      0              0              0              1   \n",
       "1971  0.9988  1.000768      1              0              1              0   \n",
       "1972  0.9992  0.999029      0              0              0              1   \n",
       "1973  1.0028  1.000399      1              1              0              0   \n",
       "1974  0.9999  0.999650      0              0              0              1   \n",
       "1975  1.0060  0.999159      0              0              0              0   \n",
       "1976  0.9993  0.999701      0              0              0              1   \n",
       "1977  1.0001  0.998774      0              0              0              0   \n",
       "1978  1.0016  1.000037      0              0              0              0   \n",
       "1979  1.0041  0.999638      0              0              0              0   \n",
       "1980  1.0096  1.000404      1              1              0              0   \n",
       "1981  1.0006  1.000072      1              1              0              0   \n",
       "1982  1.0107  0.999211      0              0              0              0   \n",
       "1983  0.9955  0.998577      0              0              0              1   \n",
       "1984  0.9972  1.000087      1              0              1              0   \n",
       "1985  1.0101  0.998608      0              0              0              0   \n",
       "1986  0.9962  0.999062      0              0              0              1   \n",
       "1987  1.0145  0.999541      0              0              0              0   \n",
       "1988  1.0124  1.000373      1              1              0              0   \n",
       "1989  0.9874  0.998466      0              0              0              1   \n",
       "\n",
       "      avoid_failure  success  \n",
       "0                 0        1  \n",
       "1                 1        1  \n",
       "2                 0        1  \n",
       "3                 0        1  \n",
       "4                 1        1  \n",
       "5                 0        1  \n",
       "6                 0        1  \n",
       "7                 0        1  \n",
       "8                 0        1  \n",
       "9                 0        0  \n",
       "10                0        1  \n",
       "11                0        1  \n",
       "12                0        0  \n",
       "13                0        0  \n",
       "14                0        0  \n",
       "15                0        1  \n",
       "16                0        1  \n",
       "17                0        0  \n",
       "18                0        1  \n",
       "19                1        1  \n",
       "20                1        1  \n",
       "21                1        1  \n",
       "22                1        1  \n",
       "23                0        1  \n",
       "24                1        1  \n",
       "25                0        1  \n",
       "26                1        1  \n",
       "27                0        1  \n",
       "28                0        0  \n",
       "29                0        1  \n",
       "...             ...      ...  \n",
       "1960              0        0  \n",
       "1961              0        1  \n",
       "1962              0        0  \n",
       "1963              0        0  \n",
       "1964              0        1  \n",
       "1965              0        0  \n",
       "1966              0        1  \n",
       "1967              0        1  \n",
       "1968              0        1  \n",
       "1969              0        1  \n",
       "1970              0        1  \n",
       "1971              0        0  \n",
       "1972              0        1  \n",
       "1973              0        1  \n",
       "1974              0        1  \n",
       "1975              1        1  \n",
       "1976              0        1  \n",
       "1977              1        1  \n",
       "1978              1        1  \n",
       "1979              1        1  \n",
       "1980              0        1  \n",
       "1981              0        1  \n",
       "1982              1        1  \n",
       "1983              0        1  \n",
       "1984              0        0  \n",
       "1985              1        1  \n",
       "1986              0        1  \n",
       "1987              1        1  \n",
       "1988              0        1  \n",
       "1989              0        1  \n",
       "\n",
       "[1990 rows x 8 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primitive policy - replace with a policy network which maximizes reward\n",
    "def label_success (row):\n",
    "    return 0 if (row['entry_failure'] == 1 or row['entry_failure'] == 1) else 1\n",
    "\n",
    "success = df.apply (lambda row: label_success (row),axis=1)\n",
    "df['success'] = success;\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04235195089212175\n",
      "0.020018278246728534\n",
      "0.41465551622319835\n",
      "0.4196964576256884\n"
     ]
    }
   ],
   "source": [
    "print df['actual'].corr(df['signal'])\n",
    "print df['actual'].corr(df['trade'])\n",
    "print df['actual'].corr(df['entry_success'])\n",
    "print df['actual'].corr(df['success'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1990.000000\n",
       "mean        1.000404\n",
       "std         0.009946\n",
       "min         0.954200\n",
       "25%         0.996600\n",
       "50%         1.000600\n",
       "75%         1.005175\n",
       "max         1.062400\n",
       "Name: actual, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['actual'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1990.000000\n",
       "mean        1.000133\n",
       "std         0.001548\n",
       "min         0.991751\n",
       "25%         0.999318\n",
       "50%         1.000036\n",
       "75%         1.000823\n",
       "max         1.008155\n",
       "Name: signal, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['signal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n",
      "\n",
      "Precision\n",
      "0.550769230769\n",
      "\n",
      "Recall\n",
      "0.499534883721\n",
      "\n",
      "Accuracy\n",
      "0.269849246231\n",
      "\n",
      "Non-loss events\n",
      "1552\n",
      "0.779899497487\n",
      "\n",
      "Lose trades\n",
      "438\n",
      "0.220100502513\n",
      "\n",
      "Win trades\n",
      "537\n",
      "0.269849246231\n",
      "\n",
      "Missed opportunities\n",
      "538\n",
      "0.270351758794\n",
      "\n",
      "Bullets dodged\n",
      "460\n",
      "0.231155778894\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST_SAMPLES = len(test_data)\n",
    "print NUM_TEST_SAMPLES\n",
    "\n",
    "print '\\nPrecision' # optimize for this since we can increase discovery, so long as we find enough trades\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['entry_failure'])) \n",
    "\n",
    "print '\\nRecall'\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['avoid_failure']))\n",
    "\n",
    "print '\\nAccuracy'\n",
    "print sum(df['entry_success']) * 1.00 / (NUM_TEST_SAMPLES)\n",
    "\n",
    "print '\\nNon-loss events'\n",
    "print sum(df['success'])\n",
    "print sum(df['success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nLose trades'\n",
    "print sum(df['entry_failure'])\n",
    "print sum(df['entry_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nWin trades'\n",
    "print sum(df['entry_success'])\n",
    "print sum(df['entry_success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nMissed opportunities'\n",
    "print sum(df['avoid_failure'])\n",
    "print sum(df['avoid_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nBullets dodged'\n",
    "print sum(df['avoid_success'])\n",
    "print sum(df['avoid_success']) / (NUM_TEST_SAMPLES * 1.00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00189589, -0.08340681,  0.06239806, -0.03416989, -0.1204368 ,\n",
       "         -0.1119125 , -0.12959342, -0.11332761,  0.17328863,  0.0022723 ,\n",
       "         -0.02248038,  0.1372904 ,  0.02308114,  0.03597886, -0.00024042,\n",
       "          0.06198   ,  0.1413104 ,  0.0652935 ,  0.17251897, -0.06182472,\n",
       "         -0.15720631, -0.14738251,  0.17121056,  0.08472145,  0.15413718,\n",
       "         -0.13105005,  0.16324824, -0.17448615, -0.0084502 , -0.08476239,\n",
       "          0.05675653, -0.00895276],\n",
       "        [ 0.1495099 ,  0.16066135, -0.00362481, -0.16850182, -0.06312653,\n",
       "          0.02063698,  0.15394612,  0.04161632,  0.15972283, -0.16459852,\n",
       "          0.12565975, -0.06647352, -0.1591163 , -0.13607916,  0.14369775,\n",
       "         -0.02244516,  0.14077286, -0.14586696, -0.05428725,  0.16857435,\n",
       "          0.14634855,  0.11117543, -0.04610117,  0.10854615,  0.14961255,\n",
       "         -0.08942042, -0.0064688 , -0.09485752,  0.04934951, -0.114145  ,\n",
       "         -0.18017548,  0.15747216],\n",
       "        [ 0.0292776 ,  0.06034708, -0.03765607,  0.04646242,  0.05606264,\n",
       "          0.03394149, -0.06790888, -0.05581342, -0.09240131,  0.08248287,\n",
       "         -0.12581281,  0.03345698,  0.0916556 , -0.00990601,  0.15786292,\n",
       "          0.03386496,  0.15122898, -0.16651244, -0.15074967,  0.08022326,\n",
       "         -0.04868013,  0.1040872 , -0.12819453, -0.14269416, -0.09258194,\n",
       "          0.16115694, -0.0222987 ,  0.03384465,  0.00923016, -0.13788989,\n",
       "          0.11089444, -0.18039435],\n",
       "        [-0.18162937, -0.05083786,  0.09090744,  0.0063429 ,  0.12661971,\n",
       "         -0.00111936,  0.01861478,  0.14903256,  0.08545003,  0.13758191,\n",
       "          0.13139234, -0.07297468,  0.02435912,  0.00409482,  0.07319961,\n",
       "         -0.04316717,  0.07929289, -0.12236114, -0.1645559 , -0.09984096,\n",
       "          0.13922611, -0.07992395,  0.05646708,  0.03020501, -0.08906443,\n",
       "          0.01113837,  0.11385411,  0.00303531, -0.09978641, -0.08892261,\n",
       "          0.11705655, -0.17857863]], dtype=float32),\n",
       " array([[ 0.07503778,  0.08049341, -0.17751302,  0.06982853,  0.0221429 ,\n",
       "          0.11818979, -0.06785386,  0.16021013, -0.00989408, -0.09893678,\n",
       "         -0.16112576, -0.00859761,  0.02682585,  0.04694258,  0.07834376,\n",
       "         -0.11752884,  0.03289077, -0.10048169,  0.09839474, -0.07587661,\n",
       "          0.1141223 , -0.15504754, -0.06837419,  0.17742746, -0.00685247,\n",
       "         -0.10540668, -0.03894588,  0.0150768 ,  0.0005291 , -0.1322633 ,\n",
       "          0.09631019, -0.11830097],\n",
       "        [-0.03250172, -0.08028582,  0.13479382, -0.0062764 , -0.02115894,\n",
       "         -0.05522068,  0.04301779, -0.15182747, -0.08171992,  0.01586265,\n",
       "         -0.01216819,  0.15692271, -0.07839044,  0.04423171,  0.03574008,\n",
       "          0.15887195,  0.07076363, -0.13069071,  0.034651  ,  0.1384897 ,\n",
       "          0.17788136, -0.00188508,  0.01007239, -0.13307893,  0.04561859,\n",
       "          0.10735246,  0.13156496,  0.11938927,  0.09617619, -0.14004752,\n",
       "          0.1131384 , -0.03162103],\n",
       "        [ 0.08514614, -0.13403042, -0.16189681,  0.09696599, -0.07166604,\n",
       "          0.14398563, -0.1661969 ,  0.12271439,  0.12807469,  0.11701548,\n",
       "         -0.09170909,  0.11692353,  0.05122006,  0.10491083, -0.01250384,\n",
       "         -0.00063874, -0.13185307, -0.02376194,  0.01334775, -0.16765247,\n",
       "          0.03823069,  0.1801342 ,  0.07617509,  0.04053481,  0.16989322,\n",
       "         -0.16748154, -0.10692547, -0.02128854,  0.15927793,  0.08556258,\n",
       "          0.02085794, -0.0268513 ],\n",
       "        [-0.1205384 , -0.08819967,  0.12658152, -0.1118356 , -0.01737551,\n",
       "         -0.05297035,  0.05540907,  0.11770593, -0.09035444, -0.02652168,\n",
       "         -0.18231055, -0.07581108,  0.1463372 ,  0.12671544, -0.17482857,\n",
       "         -0.12392369, -0.03616185, -0.04681419,  0.02088088, -0.11745244,\n",
       "          0.05620446,  0.05640962, -0.18148598, -0.10859447,  0.03193973,\n",
       "         -0.09502828, -0.10396897,  0.04174092,  0.06204284,  0.05738991,\n",
       "         -0.12645315, -0.16644013]], dtype=float32),\n",
       " array([[ 0.01073456,  0.03358323, -0.01194784,  0.00665498, -0.1472146 ,\n",
       "         -0.11345777,  0.05975749, -0.1593642 , -0.09181195,  0.01416867,\n",
       "          0.10277801,  0.14765981, -0.16630079,  0.06061545, -0.04349549,\n",
       "         -0.15999612,  0.05621298, -0.00508171, -0.07293581,  0.10195013,\n",
       "         -0.14372888,  0.11405469, -0.08367787,  0.11891828,  0.1656583 ,\n",
       "          0.04796311, -0.07424508, -0.15827583,  0.06733716, -0.12976865,\n",
       "         -0.10487656,  0.02567215],\n",
       "        [ 0.16668119,  0.13706404, -0.04891735, -0.04106114, -0.11190265,\n",
       "         -0.01749056,  0.09996526, -0.13023317,  0.14608134, -0.06366756,\n",
       "          0.14696294,  0.0210937 ,  0.13695082,  0.12653653, -0.1713531 ,\n",
       "          0.0925374 ,  0.13449924,  0.17366566, -0.03612506, -0.14398214,\n",
       "         -0.01785082, -0.01010903,  0.09580795, -0.01308989, -0.11844505,\n",
       "          0.04913275,  0.09792763,  0.03558118, -0.11653984, -0.13303941,\n",
       "          0.13936573,  0.10775686],\n",
       "        [-0.11793381,  0.15319745, -0.02870331,  0.07734495, -0.17864439,\n",
       "         -0.13345663,  0.09576324,  0.10783518,  0.03774876, -0.13555738,\n",
       "         -0.02087778, -0.0359034 , -0.16710973, -0.15073434,  0.14495744,\n",
       "         -0.16447265,  0.10906558,  0.17192939,  0.07360363, -0.0463886 ,\n",
       "         -0.10768524, -0.08762643,  0.08458981,  0.05761821,  0.1263601 ,\n",
       "         -0.01943206,  0.05969361, -0.01357266,  0.02813026,  0.09737927,\n",
       "         -0.0026684 ,  0.11599617],\n",
       "        [ 0.17645466,  0.14381258, -0.07132828,  0.13770151, -0.10477795,\n",
       "         -0.07519029,  0.1754296 , -0.14800201,  0.02862596, -0.01464878,\n",
       "         -0.12848055, -0.0562769 ,  0.07994021,  0.14113478,  0.16340244,\n",
       "          0.10345237,  0.09588849, -0.10862077, -0.15690432, -0.15491565,\n",
       "          0.03057157,  0.17980757, -0.06158036, -0.12064686,  0.01418414,\n",
       "         -0.07591768, -0.15911224,  0.01178161, -0.15254147, -0.04547844,\n",
       "         -0.15872455,  0.15678307]], dtype=float32),\n",
       " array([[-0.1774145 ,  0.14017628, -0.04593905, -0.12877022, -0.14421216,\n",
       "          0.0516376 ,  0.05901638, -0.05241745, -0.04271934, -0.14512624,\n",
       "          0.11771923,  0.09841266,  0.00332831, -0.08260654,  0.04919203,\n",
       "         -0.10452163, -0.03450768, -0.1430019 ,  0.10038538,  0.04107338,\n",
       "         -0.11552829, -0.06585675, -0.15890042,  0.1308654 , -0.07786246,\n",
       "         -0.09382994,  0.10215393,  0.03841563,  0.15509395, -0.09997614,\n",
       "          0.08383033,  0.17894046],\n",
       "        [-0.07691994, -0.09941239, -0.01866627, -0.02958579, -0.17617619,\n",
       "         -0.02287553, -0.05789407, -0.09835225, -0.06917576, -0.05888433,\n",
       "          0.01874381, -0.18061858,  0.13674355,  0.17605928,  0.00134626,\n",
       "          0.02285714,  0.02386219,  0.07195121, -0.06179516, -0.13985306,\n",
       "          0.18218744, -0.03784163, -0.07186313, -0.0054857 , -0.10915064,\n",
       "         -0.04955304, -0.10612696,  0.14265439, -0.07119019, -0.13650694,\n",
       "          0.04115998,  0.11266006],\n",
       "        [-0.1135537 ,  0.04423415,  0.08627595,  0.07749704, -0.03597579,\n",
       "          0.13265973, -0.14809948,  0.02577005,  0.08406722, -0.03287951,\n",
       "         -0.1623999 ,  0.10222465,  0.13147642, -0.03932213, -0.04034453,\n",
       "          0.17966522, -0.05129765,  0.02450631, -0.12492346,  0.01801217,\n",
       "          0.07784408, -0.05067475, -0.04548006, -0.17388816, -0.00726942,\n",
       "          0.14049253, -0.16488065, -0.06409947, -0.17190503, -0.13269621,\n",
       "          0.15577148, -0.10048743],\n",
       "        [ 0.17759562,  0.17781827,  0.1604861 , -0.02087711, -0.05534101,\n",
       "         -0.07298547, -0.09284936, -0.0876292 ,  0.0263322 ,  0.09681927,\n",
       "         -0.10249758, -0.0896413 , -0.1568015 ,  0.03602984, -0.03232718,\n",
       "         -0.17478208,  0.12676121, -0.13159816, -0.0682236 , -0.03411767,\n",
       "          0.07592312, -0.06356037,  0.11051837, -0.00115298,  0.06755002,\n",
       "         -0.08552814,  0.16050631,  0.11045554,  0.05215457,  0.14082623,\n",
       "          0.12641948, -0.13591588]], dtype=float32),\n",
       " array([[-0.16759276, -0.03534632,  0.04853496, -0.13809937, -0.13538624,\n",
       "         -0.09135849,  0.1746675 , -0.02842867,  0.15177101,  0.08230847,\n",
       "         -0.14164989, -0.07247344,  0.07543133,  0.17718248,  0.13149343,\n",
       "         -0.0797974 ,  0.01543362,  0.02528243, -0.01600717,  0.07738548,\n",
       "         -0.08023837,  0.15178967, -0.03473834, -0.04655253,  0.07590664,\n",
       "          0.06947721, -0.13061406,  0.02109865, -0.04985784, -0.07000966,\n",
       "          0.07785256, -0.01325994],\n",
       "        [ 0.12590908, -0.01097506,  0.15612862, -0.14942688, -0.12069774,\n",
       "         -0.04209737, -0.105648  , -0.13719735, -0.1532806 , -0.08017717,\n",
       "         -0.180391  , -0.00209097, -0.01733827, -0.17848633, -0.11715028,\n",
       "         -0.12377121, -0.09486634, -0.12014764, -0.11923458, -0.13681704,\n",
       "          0.09627338,  0.06589223,  0.14570381, -0.02305679, -0.06457032,\n",
       "          0.17335418, -0.09680474, -0.0874551 , -0.11387397,  0.14703347,\n",
       "          0.10227951,  0.10411789],\n",
       "        [ 0.12323888,  0.04824106, -0.07197839,  0.12592526, -0.07997934,\n",
       "          0.07647702,  0.01741863,  0.15865918, -0.16778305,  0.01892083,\n",
       "         -0.12786555, -0.10715043, -0.01279515, -0.10253285, -0.12361973,\n",
       "         -0.05205983,  0.08463269, -0.04150986, -0.00640574, -0.171821  ,\n",
       "          0.1496371 ,  0.14257802,  0.13199033, -0.08158349,  0.00194955,\n",
       "          0.0566526 , -0.16814661, -0.07320648, -0.08616792,  0.08493528,\n",
       "         -0.03463445, -0.02937781],\n",
       "        [ 0.07533351,  0.17947777,  0.0828576 ,  0.1635643 , -0.0130397 ,\n",
       "         -0.10398553, -0.08725989, -0.06991762,  0.14758797, -0.10171527,\n",
       "         -0.05185895,  0.1393364 ,  0.00216075, -0.16976736,  0.0604612 ,\n",
       "         -0.13301028,  0.05583659,  0.0257231 ,  0.02973461, -0.06337564,\n",
       "          0.09301926,  0.0009213 , -0.06244598,  0.03583182, -0.01578205,\n",
       "         -0.15829729, -0.17448975, -0.05988828, -0.10491405, -0.17987771,\n",
       "          0.07628918,  0.12401326]], dtype=float32)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        , -0.00071011,  0.00426743, -0.00071011],\n",
       "        [-0.00711238, -0.00711238, -0.00071011, -0.00071011],\n",
       "        [-0.00924496, -0.00995733, -0.00426743, -0.00640001],\n",
       "        [-0.01991465, -0.02062476, -0.00995733, -0.01066743],\n",
       "        [-0.02418208, -0.02631466, -0.01208991, -0.02346971],\n",
       "        [-0.02346971, -0.02560455, -0.01777981, -0.02346971],\n",
       "        [-0.02346971, -0.02702703, -0.02204724, -0.02346971],\n",
       "        [-0.01635734, -0.01991465, -0.01422475, -0.01991465],\n",
       "        [-0.01777981, -0.01849218, -0.01351238, -0.01635734],\n",
       "        [-0.0227596 , -0.02702703, -0.01920229, -0.01920229],\n",
       "        [-0.01493486, -0.02133713, -0.01493486, -0.02133713],\n",
       "        [ 0.01066969, -0.01208991,  0.01209217, -0.01208991],\n",
       "        [ 0.0113798 ,  0.00924722,  0.01493712,  0.01066969],\n",
       "        [ 0.01209217,  0.        ,  0.02560455,  0.        ],\n",
       "        [ 0.00853485,  0.00853485,  0.01351464,  0.01209217],\n",
       "        [ 0.00497979,  0.00355732,  0.00853485,  0.0056899 ],\n",
       "        [ 0.0056899 ,  0.00142248,  0.01066969,  0.00213484],\n",
       "        [-0.00711238, -0.00711238,  0.00497979,  0.00497979],\n",
       "        [-0.00924496, -0.00995733, -0.00426743, -0.00640001],\n",
       "        [-0.01066743, -0.0113798 , -0.0056899 , -0.0113798 ],\n",
       "        [-0.00782248, -0.01422475, -0.00640001, -0.01422475],\n",
       "        [-0.0227596 , -0.0227596 , -0.00711238, -0.00853485],\n",
       "        [-0.02702703, -0.02773714, -0.0227596 , -0.02418208],\n",
       "        [-0.02133713, -0.0284495 , -0.02133713, -0.0284495 ],\n",
       "        [-0.01849218, -0.02702703, -0.01777981, -0.0227596 ],\n",
       "        [-0.04125178, -0.04125178, -0.02062476, -0.02062476],\n",
       "        [-0.03769446, -0.0398293 , -0.03556188, -0.03911694],\n",
       "        [-0.0398293 , -0.0398293 , -0.03342704, -0.03769446],\n",
       "        [-0.03698435, -0.04338436, -0.03556188, -0.04053941],\n",
       "        [-0.02631466, -0.02915961, -0.01991465, -0.0284495 ],\n",
       "        [-0.03129445, -0.03129445, -0.02560455, -0.02560455],\n",
       "        [-0.03129445, -0.0341394 , -0.03129445, -0.03129445]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_data = np.array(convert_to_train(SPY.copy(), 0)[0][:1])\n",
    "today_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9985657]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_prediction = model.predict(today_data)\n",
    "future_prediction # [1 day prediction, 5 day prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
