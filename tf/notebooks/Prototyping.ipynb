{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "\n",
    "Creates a neural network which evaluates a time series and produce a set of predicted values for the time series\n",
    "Predicted values may be used in a policy to make a trade. This policy may be modeled by simple multiple regression or a neural network.\n",
    "\n",
    "## Data\n",
    "Test data is taken as most recent to avoid lookahead bias. Train data is split into a validation and training set during fitting.\n",
    "\n",
    "\n",
    "## TODO\n",
    "- Convert feature percentages to stdev\n",
    "- Adding VIX as a signal\n",
    "- Adding High/Low as signals\n",
    "- Multiple securities/ aggregate samples\n",
    "- Policy network\n",
    "- Regularization (l2)\n",
    "- Dilated convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "\n",
    "def from_network(symbol):\n",
    "    return pdr.get_data_yahoo(symbols=symbol, start=datetime(1900, 1, 1))\n",
    "\n",
    "def from_file(symbol):\n",
    "    dataset_path = keras.utils.get_file(\"{}.csv\".format(symbol), \"http://localhost:8000/data/daily/{}.csv\".format(symbol))\n",
    "    column_names = ['Date','Open','High','Low','Close','Adj Close','Volume'] \n",
    "    return pd.read_csv(dataset_path, \n",
    "                              names=column_names, \n",
    "                              dtype={'Close': np.float64,'Open': np.float64,'High': np.float64,'Adj Close': np.float64, 'Volume': np.float64},\n",
    "                              header=0,\n",
    "                              na_values = \"?\", \n",
    "                              comment='\\t',\n",
    "                              sep=\",\",\n",
    "                              skipinitialspace=True)\n",
    "\n",
    "#dataset = raw_dataset.copy()\n",
    "dataset = from_network('SPY')\n",
    "dataset = dataset.sort_values(by=['Date'],ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  High         Low        Open       Close       Volume  \\\n",
      "Date                                                                      \n",
      "2019-01-22  265.059998  261.059998  264.820007  262.859985  115355500.0   \n",
      "2019-01-18  266.980011  263.000000  264.980011  266.459991  127900300.0   \n",
      "2019-01-17  263.920013  259.959991  260.010010  262.959991   96118400.0   \n",
      "2019-01-16  261.970001  260.600006  260.829987  260.980011   77636700.0   \n",
      "2019-01-15  260.700012  257.809998  257.820007  260.350006   85208300.0   \n",
      "2019-01-14  258.299988  256.410004  256.859985  257.399994   70908200.0   \n",
      "2019-01-11  259.010010  257.029999  257.679993  258.980011   73858100.0   \n",
      "2019-01-10  259.160004  255.500000  256.260010  258.880005   96823900.0   \n",
      "2019-01-09  258.910004  256.190002  257.559998  257.970001   95006600.0   \n",
      "2019-01-08  257.309998  254.000000  256.820007  256.769989  102512600.0   \n",
      "2019-01-07  255.949997  251.690002  252.690002  254.380005  103139100.0   \n",
      "2019-01-04  253.110001  247.169998  247.589996  252.389999  142628800.0   \n",
      "2019-01-03  248.570007  243.669998  248.229996  244.210007  144140700.0   \n",
      "2019-01-02  251.210007  245.949997  245.979996  250.179993  126925200.0   \n",
      "2018-12-31  250.190002  247.470001  249.559998  249.919998  144299400.0   \n",
      "2018-12-28  251.399994  246.449997  249.580002  247.750000  153100200.0   \n",
      "2018-12-27  248.289993  238.960007  242.570007  248.070007  186267300.0   \n",
      "2018-12-26  246.179993  233.759995  235.970001  246.179993  218485400.0   \n",
      "2018-12-24  240.839996  234.270004  239.039993  234.339996  147311600.0   \n",
      "2018-12-21  249.710007  239.979996  246.740005  240.699997  255345600.0   \n",
      "2018-12-20  251.619995  244.649994  249.860001  247.169998  252053400.0   \n",
      "2018-12-19  259.399994  249.350006  255.169998  251.259995  214992800.0   \n",
      "2018-12-18  257.950012  253.279999  257.200012  255.080002  134515100.0   \n",
      "2018-12-17  260.649994  253.529999  259.399994  255.360001  165492300.0   \n",
      "2018-12-14  264.029999  259.850006  262.959991  260.470001  116961100.0   \n",
      "2018-12-13  267.489990  264.119995  266.519989  265.369995   96662700.0   \n",
      "2018-12-12  269.000000  265.369995  267.470001  265.459991   97976700.0   \n",
      "2018-12-11  267.869995  262.480011  267.660004  264.130005  121504400.0   \n",
      "2018-12-10  265.160004  258.619995  263.369995  264.070007  151445900.0   \n",
      "2018-12-07  271.220001  262.630005  269.459991  263.570007  161018900.0   \n",
      "...                ...         ...         ...         ...          ...   \n",
      "1993-03-12   45.218700   44.812500   45.187500   45.093700     643600.0   \n",
      "1993-03-11   45.843700   45.500000   45.718700   45.562500      70900.0   \n",
      "1993-03-10   45.687500   45.406200   45.593700   45.687500     194400.0   \n",
      "1993-03-09   45.687500   45.500000   45.656200   45.593700     169300.0   \n",
      "1993-03-08   45.750000   44.843700   44.843700   45.750000      50800.0   \n",
      "1993-03-05   45.125000   44.718700   44.937500   44.750000      40000.0   \n",
      "1993-03-04   45.187500   44.875000   45.187500   44.875000      89500.0   \n",
      "1993-03-03   45.156200   44.937500   45.000000   45.125000     280100.0   \n",
      "1993-03-02   44.937500   44.250000   44.312500   44.937500     182400.0   \n",
      "1993-03-01   44.562500   44.218700   44.562500   44.281200      66500.0   \n",
      "1993-02-26   44.437500   44.187500   44.437500   44.406200      66200.0   \n",
      "1993-02-25   44.375000   44.125000   44.218700   44.343700      44500.0   \n",
      "1993-02-24   44.250000   43.718700   43.718700   44.250000      26300.0   \n",
      "1993-02-23   43.875000   43.468700   43.843700   43.687500     373700.0   \n",
      "1993-02-22   43.781200   43.562500   43.687500   43.718700     513600.0   \n",
      "1993-02-19   43.562500   43.343700   43.406200   43.562500      34900.0   \n",
      "1993-02-18   43.937500   42.812500   43.937500   43.406200     378100.0   \n",
      "1993-02-17   43.531200   43.281200   43.468700   43.437500     210900.0   \n",
      "1993-02-16   44.468700   43.406200   44.468700   43.468700     374800.0   \n",
      "1993-02-12   44.875000   44.593700   44.875000   44.593700      42500.0   \n",
      "1993-02-11   45.125000   44.781200   44.781200   44.937500      19500.0   \n",
      "1993-02-10   44.750000   44.531200   44.656200   44.718700     379600.0   \n",
      "1993-02-09   44.812500   44.562500   44.812500   44.656200     122100.0   \n",
      "1993-02-08   45.125000   44.906200   44.968700   44.968700     596100.0   \n",
      "1993-02-05   45.062500   44.718700   44.968700   44.968700     492100.0   \n",
      "1993-02-04   45.093700   44.468700   44.968700   45.000000     531500.0   \n",
      "1993-02-03   44.843700   44.375000   44.406200   44.812500     529400.0   \n",
      "1993-02-02   44.375000   44.125000   44.218700   44.343700     201300.0   \n",
      "1993-02-01   44.250000   43.968700   43.968700   44.250000     480500.0   \n",
      "1993-01-29   43.968700   43.750000   43.968700   43.937500    1003200.0   \n",
      "\n",
      "             Adj Close  \n",
      "Date                    \n",
      "2019-01-22  262.859985  \n",
      "2019-01-18  266.459991  \n",
      "2019-01-17  262.959991  \n",
      "2019-01-16  260.980011  \n",
      "2019-01-15  260.350006  \n",
      "2019-01-14  257.399994  \n",
      "2019-01-11  258.980011  \n",
      "2019-01-10  258.880005  \n",
      "2019-01-09  257.970001  \n",
      "2019-01-08  256.769989  \n",
      "2019-01-07  254.380005  \n",
      "2019-01-04  252.389999  \n",
      "2019-01-03  244.210007  \n",
      "2019-01-02  250.179993  \n",
      "2018-12-31  249.919998  \n",
      "2018-12-28  247.750000  \n",
      "2018-12-27  248.070007  \n",
      "2018-12-26  246.179993  \n",
      "2018-12-24  234.339996  \n",
      "2018-12-21  240.699997  \n",
      "2018-12-20  245.735001  \n",
      "2018-12-19  249.801239  \n",
      "2018-12-18  253.599075  \n",
      "2018-12-17  253.877457  \n",
      "2018-12-14  258.957794  \n",
      "2018-12-13  263.829315  \n",
      "2018-12-12  263.918793  \n",
      "2018-12-11  262.596527  \n",
      "2018-12-10  262.536896  \n",
      "2018-12-07  262.039795  \n",
      "...                ...  \n",
      "1993-03-12   27.664164  \n",
      "1993-03-11   27.951756  \n",
      "1993-03-10   28.028440  \n",
      "1993-03-09   27.970888  \n",
      "1993-03-08   28.066788  \n",
      "1993-03-05   27.453302  \n",
      "1993-03-04   27.529985  \n",
      "1993-03-03   27.683357  \n",
      "1993-03-02   27.568338  \n",
      "1993-03-01   27.165707  \n",
      "1993-02-26   27.242388  \n",
      "1993-02-25   27.204052  \n",
      "1993-02-24   27.146563  \n",
      "1993-02-23   26.801489  \n",
      "1993-02-22   26.820633  \n",
      "1993-02-19   26.724798  \n",
      "1993-02-18   26.628904  \n",
      "1993-02-17   26.648104  \n",
      "1993-02-16   26.667248  \n",
      "1993-02-12   27.357416  \n",
      "1993-02-11   27.568338  \n",
      "1993-02-10   27.434109  \n",
      "1993-02-09   27.395754  \n",
      "1993-02-08   27.587467  \n",
      "1993-02-05   27.587467  \n",
      "1993-02-04   27.606684  \n",
      "1993-02-03   27.491642  \n",
      "1993-02-02   27.204052  \n",
      "1993-02-01   27.146563  \n",
      "1993-01-29   26.954851  \n",
      "\n",
      "[6542 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6542\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 6,785\n",
      "Trainable params: 6,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_stats = dataset.describe()\n",
    "dataset_stats = dataset_stats.transpose()\n",
    "NUM_INPUT_NEURONS = 64\n",
    "NUM_OUTPUT_NEURONS = 1\n",
    "NUM_SAMPLES = len(dataset)\n",
    "NUM_TEST_SAMPLES = int(.25 * NUM_SAMPLES)\n",
    "NUM_TEST_SAMPLES = 1000\n",
    "DAY_OFFSET = 5\n",
    "print NUM_SAMPLES\n",
    "# Create features (only close price for now)\n",
    "def convert_to_percentage(old, new):\n",
    "    return (old - new) / old\n",
    "\n",
    "def convert_labels_to_category(labels): \n",
    "    # Simplification - If positive return, 1, else 0\n",
    "    # return map(lambda arr: 1 if arr[0] > 1 else 0, labels)\n",
    "    # rounding simpliciation\n",
    "    return map(lambda arr: map(lambda val: round(val,4),arr), labels)\n",
    "\n",
    "def convert_to_train(raw_dataset):\n",
    "    dataset = raw_dataset.copy()\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(DAY_OFFSET, len(dataset) - NUM_INPUT_NEURONS):\n",
    "\n",
    "        feature_dataset = dataset[i:i+NUM_INPUT_NEURONS].copy()\n",
    "        latest_close = feature_dataset['Close'].iloc[0]\n",
    "        \n",
    "        features.append(\n",
    "            feature_dataset['Close']\n",
    "                .map(lambda current: convert_to_percentage(latest_close, current))\n",
    "                .tolist()\n",
    "        )\n",
    "        labels.append([\n",
    "            dataset['Close'].iloc[i-1] / latest_close, # 1 day trade\n",
    "#             dataset['Close'].iloc[i-2] / latest_close, # 2 day trade\n",
    "#             dataset['Close'].iloc[i-3] / latest_close, # 3 day trade\n",
    "#             dataset['Close'].iloc[i-4] / latest_close, # 4 day trade\n",
    "#             dataset['Close'].iloc[i-5] / latest_close, # 5 day trade\n",
    "        ])\n",
    "        \n",
    "    # Without converting labels the precision is hard to determine accuracy. \n",
    "    # Rather than crude 0/1, maybe this can be more sophisticated\n",
    "    labels = convert_labels_to_category(labels)\n",
    "    \n",
    "    return [features,labels]\n",
    "converted_feature_set = convert_to_train(dataset)\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.nn.relu, input_shape=[len(converted_feature_set[0][0])]),\n",
    "    layers.Dense(32, kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001), activation=tf.nn.relu),\n",
    "    layers.Dense(16, kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001), activation=tf.nn.relu),\n",
    "    #layers.Dense(16, activation=tf.nn.relu),\n",
    "    layers.Dense(NUM_OUTPUT_NEURONS)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer='sgd',\n",
    "                metrics=[\n",
    "                    'mae',\n",
    "                ])\n",
    "  return model\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "#len(converted_feature_set[0][0])\n",
    "print NUM_TEST_SAMPLES\n",
    "train_data = np.array(converted_feature_set[0][NUM_TEST_SAMPLES:])\n",
    "train_labels = np.array(converted_feature_set[1][NUM_TEST_SAMPLES:])\n",
    "\n",
    "test_data = np.array(converted_feature_set[0][:NUM_TEST_SAMPLES])\n",
    "test_labels = np.array(converted_feature_set[1][:NUM_TEST_SAMPLES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.00551303  0.00922086 ...  0.04166468  0.04898282\n",
      "   0.0599112 ]\n",
      " [ 0.          0.0146531   0.01960218 ...  0.05419706  0.06506552\n",
      "   0.05836972]\n",
      " [ 0.          0.00502267  0.00714003 ...  0.0511621   0.04436672\n",
      "   0.06293086]\n",
      " ...\n",
      " [ 0.          0.00280309  0.00630583 ... -0.00840926 -0.00911114\n",
      "  -0.00490651]\n",
      " [ 0.          0.00351259  0.00983838 ... -0.01194772 -0.00773127\n",
      "   0.00281097]\n",
      " [ 0.          0.00634809  0.00775852 ... -0.0112835  -0.0007041\n",
      "   0.00141044]]\n",
      "[[1.0023]\n",
      " [0.9945]\n",
      " [1.0149]\n",
      " ...\n",
      " [0.9965]\n",
      " [1.0028]\n",
      " [1.0035]]\n"
     ]
    }
   ],
   "source": [
    "print train_data\n",
    "print train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4378 samples, validate on 1095 samples\n",
      "Epoch 1/90\n",
      "4378/4378 [==============================] - 0s 90us/step - loss: 0.1249 - mean_absolute_error: 0.1588 - val_loss: 0.0410 - val_mean_absolute_error: 0.0151\n",
      "Epoch 2/90\n",
      "4378/4378 [==============================] - 0s 41us/step - loss: 0.0413 - mean_absolute_error: 0.0180 - val_loss: 0.0408 - val_mean_absolute_error: 0.0115\n",
      "Epoch 3/90\n",
      "4378/4378 [==============================] - 0s 44us/step - loss: 0.0411 - mean_absolute_error: 0.0155 - val_loss: 0.0407 - val_mean_absolute_error: 0.0101\n",
      "Epoch 4/90\n",
      "4378/4378 [==============================] - 0s 43us/step - loss: 0.0410 - mean_absolute_error: 0.0145 - val_loss: 0.0406 - val_mean_absolute_error: 0.0095\n",
      "Epoch 5/90\n",
      "4378/4378 [==============================] - 0s 41us/step - loss: 0.0409 - mean_absolute_error: 0.0141 - val_loss: 0.0406 - val_mean_absolute_error: 0.0092\n",
      "Epoch 6/90\n",
      "4378/4378 [==============================] - 0s 41us/step - loss: 0.0408 - mean_absolute_error: 0.0139 - val_loss: 0.0405 - val_mean_absolute_error: 0.0092\n",
      "Epoch 7/90\n",
      "4378/4378 [==============================] - 0s 42us/step - loss: 0.0407 - mean_absolute_error: 0.0138 - val_loss: 0.0405 - val_mean_absolute_error: 0.0089\n",
      "Epoch 8/90\n",
      "4378/4378 [==============================] - 0s 40us/step - loss: 0.0407 - mean_absolute_error: 0.0136 - val_loss: 0.0404 - val_mean_absolute_error: 0.0090\n",
      "Epoch 9/90\n",
      "4378/4378 [==============================] - 0s 39us/step - loss: 0.0406 - mean_absolute_error: 0.0135 - val_loss: 0.0403 - val_mean_absolute_error: 0.0086\n",
      "Epoch 10/90\n",
      "4378/4378 [==============================] - 0s 43us/step - loss: 0.0405 - mean_absolute_error: 0.0134 - val_loss: 0.0403 - val_mean_absolute_error: 0.0085\n",
      "Epoch 11/90\n",
      "4378/4378 [==============================] - 0s 41us/step - loss: 0.0405 - mean_absolute_error: 0.0133 - val_loss: 0.0402 - val_mean_absolute_error: 0.0086\n",
      "Epoch 12/90\n",
      "4378/4378 [==============================] - 0s 42us/step - loss: 0.0404 - mean_absolute_error: 0.0132 - val_loss: 0.0401 - val_mean_absolute_error: 0.0084\n",
      "Epoch 13/90\n",
      "4378/4378 [==============================] - 0s 42us/step - loss: 0.0404 - mean_absolute_error: 0.0131 - val_loss: 0.0401 - val_mean_absolute_error: 0.0084\n",
      "Epoch 14/90\n",
      "4378/4378 [==============================] - 0s 45us/step - loss: 0.0403 - mean_absolute_error: 0.0130 - val_loss: 0.0400 - val_mean_absolute_error: 0.0085\n",
      "Epoch 15/90\n",
      "4378/4378 [==============================] - 0s 46us/step - loss: 0.0402 - mean_absolute_error: 0.0130 - val_loss: 0.0400 - val_mean_absolute_error: 0.0081\n",
      "Epoch 16/90\n",
      "4378/4378 [==============================] - 0s 42us/step - loss: 0.0402 - mean_absolute_error: 0.0129 - val_loss: 0.0399 - val_mean_absolute_error: 0.0081\n",
      "Epoch 17/90\n",
      "4378/4378 [==============================] - 0s 44us/step - loss: 0.0401 - mean_absolute_error: 0.0128 - val_loss: 0.0399 - val_mean_absolute_error: 0.0080\n",
      "Epoch 18/90\n",
      "4378/4378 [==============================] - 0s 45us/step - loss: 0.0401 - mean_absolute_error: 0.0127 - val_loss: 0.0398 - val_mean_absolute_error: 0.0079\n",
      "Epoch 19/90\n",
      "4378/4378 [==============================] - 0s 46us/step - loss: 0.0400 - mean_absolute_error: 0.0127 - val_loss: 0.0397 - val_mean_absolute_error: 0.0079\n",
      "Epoch 20/90\n",
      "4378/4378 [==============================] - 0s 43us/step - loss: 0.0399 - mean_absolute_error: 0.0126 - val_loss: 0.0397 - val_mean_absolute_error: 0.0080\n",
      "Epoch 21/90\n",
      "4378/4378 [==============================] - 0s 42us/step - loss: 0.0399 - mean_absolute_error: 0.0125 - val_loss: 0.0396 - val_mean_absolute_error: 0.0080\n",
      "Epoch 22/90\n",
      "4378/4378 [==============================] - 0s 55us/step - loss: 0.0398 - mean_absolute_error: 0.0125 - val_loss: 0.0396 - val_mean_absolute_error: 0.0079\n",
      "Epoch 23/90\n",
      "4378/4378 [==============================] - 0s 42us/step - loss: 0.0397 - mean_absolute_error: 0.0124 - val_loss: 0.0395 - val_mean_absolute_error: 0.0078\n",
      "Epoch 24/90\n",
      "4378/4378 [==============================] - 0s 44us/step - loss: 0.0397 - mean_absolute_error: 0.0124 - val_loss: 0.0395 - val_mean_absolute_error: 0.0079\n",
      "Epoch 25/90\n",
      "4378/4378 [==============================] - 0s 40us/step - loss: 0.0396 - mean_absolute_error: 0.0123 - val_loss: 0.0394 - val_mean_absolute_error: 0.0077\n",
      "Epoch 26/90\n",
      "4378/4378 [==============================] - 0s 42us/step - loss: 0.0396 - mean_absolute_error: 0.0122 - val_loss: 0.0393 - val_mean_absolute_error: 0.0078\n",
      "Epoch 27/90\n",
      "4378/4378 [==============================] - 0s 45us/step - loss: 0.0395 - mean_absolute_error: 0.0122 - val_loss: 0.0393 - val_mean_absolute_error: 0.0076\n",
      "Epoch 28/90\n",
      "4378/4378 [==============================] - 0s 44us/step - loss: 0.0395 - mean_absolute_error: 0.0122 - val_loss: 0.0392 - val_mean_absolute_error: 0.0077\n",
      "Epoch 29/90\n",
      "4378/4378 [==============================] - 0s 48us/step - loss: 0.0394 - mean_absolute_error: 0.0121 - val_loss: 0.0392 - val_mean_absolute_error: 0.0077\n",
      "Epoch 30/90\n",
      "4378/4378 [==============================] - 0s 43us/step - loss: 0.0393 - mean_absolute_error: 0.0121 - val_loss: 0.0391 - val_mean_absolute_error: 0.0078\n",
      "Epoch 31/90\n",
      "4378/4378 [==============================] - 0s 46us/step - loss: 0.0393 - mean_absolute_error: 0.0121 - val_loss: 0.0390 - val_mean_absolute_error: 0.0075\n",
      "Epoch 32/90\n",
      "4378/4378 [==============================] - 0s 42us/step - loss: 0.0392 - mean_absolute_error: 0.0120 - val_loss: 0.0390 - val_mean_absolute_error: 0.0075\n",
      "Epoch 33/90\n",
      "4378/4378 [==============================] - 0s 44us/step - loss: 0.0392 - mean_absolute_error: 0.0120 - val_loss: 0.0389 - val_mean_absolute_error: 0.0075\n",
      "Epoch 34/90\n",
      "2592/4378 [================>.............] - ETA: 0s - loss: 0.0391 - mean_absolute_error: 0.0121"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=90, validation_split = 0.2, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print test_data[0]\n",
    "print test_labels[0]\n",
    "print outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primitive policy temporarily in place of a RL policy network\n",
    "\n",
    "# signal step for our policy network\n",
    "signals = map(lambda arr: sum(arr) / len(arr),outputs)\n",
    "# signals = map(lambda arr: arr[0],outputs)\n",
    "trades = map(lambda signal: 1 if round(signal,4) > 1 else 0,signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one day return\n",
    "actual = map(lambda arr: arr[0],test_labels)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'signal':signals,\n",
    "    'actual': actual,\n",
    "    'trade':trades,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['entry_success'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['entry_failure'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['avoid_success'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 0) else 0,axis=1)\n",
    "df['avoid_failure'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 0) else 0,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# primitive policy - replace with a policy network which maximizes reward\n",
    "def label_success (row):\n",
    "    return 0 if (row['entry_failure'] == 1 or row['entry_failure'] == 1) else 1\n",
    "\n",
    "success = df.apply (lambda row: label_success (row),axis=1)\n",
    "df['success'] = success;\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print '\\nPrecision'\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['entry_failure'])) \n",
    "\n",
    "print '\\nRecall'\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['avoid_failure']))\n",
    "\n",
    "print '\\nAccuracy'\n",
    "print sum(df['entry_success']) * 1.00 / (NUM_TEST_SAMPLES)\n",
    "\n",
    "print '\\nNon-loss events'\n",
    "print sum(df['success'])\n",
    "print sum(df['success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nLose trades'\n",
    "print sum(df['entry_failure'])\n",
    "print sum(df['entry_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nWin trades'\n",
    "print sum(df['entry_success'])\n",
    "print sum(df['entry_success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nMissed opportunities'\n",
    "print sum(df['avoid_failure'])\n",
    "print sum(df['avoid_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nBullets dodged'\n",
    "print sum(df['avoid_success'])\n",
    "print sum(df['avoid_success']) / (NUM_TEST_SAMPLES * 1.00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['entry_success'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dataset.copy().iloc[DAY_OFFSET-1:].head(NUM_TEST_SAMPLES)\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([df1.reset_index(),df2], axis=1).drop(['High','Low','Open','Volume','Adj Close'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
