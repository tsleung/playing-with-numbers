{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "\n",
    "Creates a neural network which evaluates a time series and produce a set of predicted values for the time series\n",
    "Predicted values may be used in a policy to make a trade. This policy may be modeled by simple multiple regression or a neural network.\n",
    "\n",
    "## Data\n",
    "Test data is taken as most recent to avoid lookahead bias. Train data is split into a validation and training set during fitting.\n",
    "\n",
    "\n",
    "## TODO\n",
    "- Adding VIX as a signal\n",
    "- Adding High/Low as signals\n",
    "- Multiple securities/ aggregate samples\n",
    "- Policy network\n",
    "- Regularization (l2)\n",
    "- Dilated convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "NUM_INPUT_NEURONS = 64\n",
    "NUM_OUTPUT_NEURONS = 1\n",
    "DAY_OFFSET = 5\n",
    "\n",
    "def from_network(symbol):\n",
    "    return pdr.get_data_yahoo(symbols=symbol, start=datetime(1900, 1, 1))\n",
    "\n",
    "def from_file(symbol):\n",
    "    dataset_path = keras.utils.get_file(\"{}.csv\".format(symbol), \"http://localhost:8000/data/daily/{}.csv\".format(symbol))\n",
    "    column_names = ['Date','Open','High','Low','Close','Adj Close','Volume'] \n",
    "    return pd.read_csv(dataset_path, \n",
    "                              names=column_names, \n",
    "                              dtype={'Close': np.float64,'Open': np.float64,'High': np.float64,'Adj Close': np.float64, 'Volume': np.float64},\n",
    "                              header=0,\n",
    "                              na_values = \"?\", \n",
    "                              comment='\\t',\n",
    "                              sep=\",\",\n",
    "                              skipinitialspace=True)\n",
    "\n",
    "\n",
    "# Create features (only close price for now)\n",
    "def convert_to_percentage(old, new):\n",
    "    return (old - new) / old\n",
    "\n",
    "def convert_labels_to_category(labels): \n",
    "    # Simplification - If positive return, 1, else 0\n",
    "    # return map(lambda arr: 1 if arr[0] > 1 else 0, labels)\n",
    "    # rounding simpliciation\n",
    "    return map(lambda arr: map(lambda val: round(val,4),arr), labels)\n",
    "\n",
    "def convert_to_train(raw_dataset, offset=5):\n",
    "    dataset = raw_dataset.copy()\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(offset, len(dataset) - NUM_INPUT_NEURONS):\n",
    "\n",
    "        feature_dataset = dataset[i:i+NUM_INPUT_NEURONS].copy()\n",
    "        latest_close = feature_dataset['Close'].iloc[0]\n",
    "        \n",
    "        close_features = feature_dataset['Close'].map(lambda current: convert_to_percentage(latest_close, current)).tolist()\n",
    "        high_features = feature_dataset['High'].map(lambda current: convert_to_percentage(latest_close, current)).tolist()\n",
    "        low_features = feature_dataset['Low'].map(lambda current: convert_to_percentage(latest_close, current)).tolist()\n",
    "        open_features = feature_dataset['Open'].map(lambda current: convert_to_percentage(latest_close, current)).tolist()\n",
    "        features.append(\n",
    "            zip(\n",
    "                close_features,\n",
    "                high_features,\n",
    "                low_features,\n",
    "                open_features,\n",
    "            )\n",
    "        )\n",
    "        labels.append([\n",
    "            dataset['Close'].iloc[i-1] / latest_close, # 1 day trade\n",
    "#             dataset['Close'].iloc[i-2] / latest_close, # 2 day trade\n",
    "#             dataset['Close'].iloc[i-3] / latest_close, # 3 day trade\n",
    "#             dataset['Close'].iloc[i-4] / latest_close, # 4 day trade\n",
    "#             dataset['Close'].iloc[i-5] / latest_close, # 5 day trade\n",
    "        ])\n",
    "        \n",
    "    # Without converting labels the precision is hard to determine accuracy. \n",
    "    # Rather than crude 0/1, maybe this can be more sophisticated\n",
    "    labels = convert_labels_to_category(labels)\n",
    "    \n",
    "    return [features,labels]\n",
    "def split_data(symbol):\n",
    "    fetched = from_network(symbol).sort_values(by=['Date'],ascending=False)\n",
    "    converted = convert_to_train(fetched)\n",
    "    features = converted[0]\n",
    "    labels = converted[1]\n",
    "    prediction = [\n",
    "        features[:5],\n",
    "        labels[:5],\n",
    "    ]\n",
    "    validation = [\n",
    "        features[5:1000],\n",
    "        labels[5:1000]\n",
    "    ]\n",
    "    training = [\n",
    "        features[1000:],\n",
    "        labels[1000:]\n",
    "    ]\n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'prediction': prediction,\n",
    "        'validation': validation,\n",
    "        'training': training,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 60, 64)            1344      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                245824    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 48)                3120      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 24)                888       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 253,253\n",
      "Trainable params: 253,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset = raw_dataset.copy()\n",
    "#dataset = from_network('SPY').sort_values(by=['Date'],ascending=False)\n",
    "# add function to cache fetch\n",
    "QQQ = from_network('QQQ')\n",
    "SPY = from_network('SPY')\n",
    "XLK = from_network('XLK')\n",
    "XLF = from_network('XLF')\n",
    "XLE = from_network('XLE')\n",
    "XLP = from_network('XLP')\n",
    "XLV = from_network('XLV')\n",
    "XLY = from_network('XLY')\n",
    "XLI = from_network('XLI')\n",
    "XLU = from_network('XLU')\n",
    "\n",
    "#dataset = pd.concat([QQQ,SPY,XLK,XLF,XLE,XLP,XLV,XLY,XLI,XLU]).sort_values(by=['Date'],ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepped_data = map(split_data, [\n",
    "    'QQQ',\n",
    "    'SPY',\n",
    "#     'XLK',\n",
    "#     'XLF',\n",
    "#     'XLE',\n",
    "#     'XLP',\n",
    "#     'XLV',\n",
    "#     'XLY',\n",
    "#     'XLI',\n",
    "#     'XLU',\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQQ\n",
      "SPY\n",
      "0\n",
      "3934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(prepped_data)):\n",
    "    print prepped_data[i]['symbol']\n",
    "    \n",
    "def combine_all(accum, prep):\n",
    "    \n",
    "    print len(accum['training'][0])\n",
    "    return {   \n",
    "        'prediction':[\n",
    "            accum['prediction'][0] + prep['prediction'][0],\n",
    "            accum['prediction'][1] + prep['prediction'][1],\n",
    "        ],\n",
    "        'validation':[\n",
    "            accum['validation'][0] + prep['validation'][0],\n",
    "            accum['validation'][1] + prep['validation'][1],\n",
    "        ],\n",
    "        'training':[\n",
    "            accum['training'][0] + prep['training'][0],\n",
    "            accum['training'][1] + prep['training'][1],\n",
    "        ],\n",
    "    }\n",
    "combined = reduce(combine_all, prepped_data,{\n",
    "    'prediction':[[],[]],\n",
    "    'validation':[[],[]],\n",
    "    'training':[[],[]],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9410"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "len(combined['training'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9410\n",
      "1990\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#len(converted_feature_set[0][0])\n",
    "print len(combined['training'][0])\n",
    "train_data = np.array(combined['training'][0])\n",
    "train_labels = np.array(combined['training'][1])\n",
    "\n",
    "print len(combined['validation'][0])\n",
    "test_data = np.array(combined['validation'][0])\n",
    "test_labels = np.array(combined['validation'][1])\n",
    "\n",
    "print len(combined['prediction'][0])\n",
    "prediction_data = np.array(combined['prediction'][0])\n",
    "prediction_labels = np.array(combined['prediction'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.         -0.022394    0.00019814 -0.02150216]\n",
      "  [-0.00515264 -0.01625049 -0.00178359 -0.01575509]\n",
      "  [-0.03190647 -0.03378918 -0.0266548  -0.03220373]\n",
      "  ...\n",
      "  [ 0.01099882  0.00792703  0.01803408  0.01060245]\n",
      "  [ 0.00733252  0.00713438  0.01823223  0.01753861]\n",
      "  [ 0.02209667  0.02031307  0.02873565  0.02467299]]\n",
      "\n",
      " [[ 0.         -0.01104096  0.00335177 -0.0105481 ]\n",
      "  [-0.02661669 -0.02848974 -0.02139194 -0.02691242]\n",
      "  [-0.02779968 -0.03095425 -0.02247633 -0.0257295 ]\n",
      "  ...\n",
      "  [ 0.01242116  0.01222403  0.02326499  0.02257493]\n",
      "  [ 0.02710962  0.02533517  0.03371457  0.02967273]\n",
      "  [ 0.02779968  0.02710962  0.0374606   0.03361597]]\n",
      "\n",
      " [[ 0.         -0.00182449  0.00508929 -0.00028806]\n",
      "  [-0.00115232 -0.00422511  0.00403301  0.00086419]\n",
      "  [ 0.00105628  0.          0.02400615  0.01507586]\n",
      "  ...\n",
      "  [ 0.05233337  0.05060492  0.05876707  0.05483003]\n",
      "  [ 0.05300554  0.05233337  0.06241598  0.05867103]\n",
      "  [ 0.06068753  0.0555022   0.06692915  0.06529675]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.         -0.00350497  0.00280309  0.00280309]\n",
      "  [ 0.00280309 -0.00070188  0.00560617  0.00420463]\n",
      "  [ 0.00630583  0.00630583  0.01541698  0.01121235]\n",
      "  ...\n",
      "  [-0.00840926 -0.01051269 -0.00280309 -0.00840926]\n",
      "  [-0.00911114 -0.01121235  0.00280309 -0.00840926]\n",
      "  [-0.00490651 -0.00560617  0.00490429  0.00420463]]\n",
      "\n",
      " [[ 0.         -0.00351482  0.00281097  0.00140548]\n",
      "  [ 0.00351259  0.00351259  0.01264935  0.0084329 ]\n",
      "  [ 0.00983838  0.00421645  0.00983838  0.00772904]\n",
      "  ...\n",
      "  [-0.01194772 -0.01405483  0.         -0.01124386]\n",
      "  [-0.00773127 -0.0084329   0.00210711  0.00140548]\n",
      "  [ 0.00281097  0.00210711  0.00772904  0.00562193]]\n",
      "\n",
      " [[ 0.          0.          0.00916896  0.00493765]\n",
      "  [ 0.00634809  0.00070634  0.00634809  0.00423131]\n",
      "  [ 0.00775852  0.00775852  0.01551481  0.00987306]\n",
      "  ...\n",
      "  [-0.0112835  -0.0119876  -0.00141044 -0.00211454]\n",
      "  [-0.0007041  -0.00141044  0.00423131  0.00211677]\n",
      "  [ 0.00141044  0.00141044  0.00775852  0.00775852]]]\n",
      "[[1.0096]\n",
      " [0.9949]\n",
      " [0.9741]\n",
      " ...\n",
      " [0.9965]\n",
      " [1.0028]\n",
      " [1.0035]]\n"
     ]
    }
   ],
   "source": [
    "print train_data\n",
    "print train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 60, 64)            1344      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                245824    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 48)                3120      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 24)                888       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 253,253\n",
      "Trainable params: 253,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    #layers.Dense(64, activation=tf.nn.relu, input_shape=[NUM_INPUT_NEURONS]),\n",
    "    layers.Conv1D(64, kernel_size=(5), strides=(1),\n",
    "        activation='relu',\n",
    "        input_shape=(64,4)),\n",
    "#    layers.MaxPooling1D(3),\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dense(48, activation=tf.nn.relu),\n",
    "    layers.Dense(36, activation=tf.nn.relu),\n",
    "    layers.Dense(24, activation=tf.nn.relu),\n",
    "    layers.Dense(12, activation=tf.nn.relu),\n",
    "\n",
    "      \n",
    "#     layers.Dense(64, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(48, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(36, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(24, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "#     layers.Dense(12, kernel_regularizer=keras.regularizers.l1_l2(l1=0.000001, l2=0.000001), activation=tf.nn.relu),\n",
    "    layers.Dense(NUM_OUTPUT_NEURONS)\n",
    "  ])\n",
    "\n",
    "  model.compile(\n",
    "#       loss='mean_squared_logarithmic_error',\n",
    "#       loss='mean_squared_error',\n",
    "     loss='logcosh',\n",
    "#       loss='mean_absolute_error',\n",
    "                optimizer='sgd',\n",
    "#                 metrics=[\n",
    "#                     'mae',\n",
    "#                 ]\n",
    "               )\n",
    "  return model\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7528 samples, validate on 1882 samples\n",
      "Epoch 1/5\n",
      "7528/7528 [==============================] - 1s 148us/step - loss: 0.0533 - val_loss: 5.7737e-05\n",
      "Epoch 2/5\n",
      "7528/7528 [==============================] - 1s 114us/step - loss: 1.4550e-04 - val_loss: 5.7622e-05\n",
      "Epoch 3/5\n",
      "7528/7528 [==============================] - 1s 114us/step - loss: 1.4354e-04 - val_loss: 5.7159e-05\n",
      "Epoch 4/5\n",
      "7528/7528 [==============================] - 1s 117us/step - loss: 1.4204e-04 - val_loss: 5.7022e-05\n",
      "Epoch 5/5\n",
      "7528/7528 [==============================] - 1s 114us/step - loss: 1.4081e-04 - val_loss: 5.6488e-05\n",
      "Train on 7528 samples, validate on 1882 samples\n",
      "Epoch 1/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3988e-04 - val_loss: 5.6390e-05\n",
      "Epoch 2/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3892e-04 - val_loss: 5.6217e-05\n",
      "Epoch 3/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3833e-04 - val_loss: 5.6157e-05\n",
      "Epoch 4/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3776e-04 - val_loss: 5.6050e-05\n",
      "Epoch 5/60\n",
      "7528/7528 [==============================] - 1s 115us/step - loss: 1.3723e-04 - val_loss: 5.5972e-05\n",
      "Epoch 6/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3685e-04 - val_loss: 5.5933e-05\n",
      "Epoch 7/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3653e-04 - val_loss: 5.5885e-05\n",
      "Epoch 8/60\n",
      "7528/7528 [==============================] - 1s 114us/step - loss: 1.3613e-04 - val_loss: 5.5889e-05\n",
      "Epoch 9/60\n",
      "7528/7528 [==============================] - 1s 115us/step - loss: 1.3590e-04 - val_loss: 5.6044e-05\n",
      "Epoch 10/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3569e-04 - val_loss: 5.5770e-05\n",
      "Epoch 11/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3544e-04 - val_loss: 5.5791e-05\n",
      "Epoch 12/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3530e-04 - val_loss: 5.5725e-05\n",
      "Epoch 13/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3511e-04 - val_loss: 5.5690e-05\n",
      "Epoch 14/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3495e-04 - val_loss: 5.5828e-05\n",
      "Epoch 15/60\n",
      "7528/7528 [==============================] - 1s 111us/step - loss: 1.3479e-04 - val_loss: 5.5795e-05\n",
      "Epoch 16/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3468e-04 - val_loss: 5.5721e-05\n",
      "Epoch 17/60\n",
      "7528/7528 [==============================] - 1s 112us/step - loss: 1.3448e-04 - val_loss: 5.5752e-05\n",
      "Epoch 18/60\n",
      "7528/7528 [==============================] - 1s 113us/step - loss: 1.3449e-04 - val_loss: 5.5625e-05\n",
      "Epoch 19/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3428e-04 - val_loss: 5.5582e-05\n",
      "Epoch 20/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3425e-04 - val_loss: 5.5871e-05\n",
      "Epoch 21/60\n",
      "7528/7528 [==============================] - 1s 111us/step - loss: 1.3428e-04 - val_loss: 5.5729e-05\n",
      "Epoch 22/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3413e-04 - val_loss: 5.5772e-05\n",
      "Epoch 23/60\n",
      "7528/7528 [==============================] - 1s 113us/step - loss: 1.3411e-04 - val_loss: 5.5512e-05\n",
      "Epoch 24/60\n",
      "7528/7528 [==============================] - 1s 115us/step - loss: 1.3401e-04 - val_loss: 5.5549e-05\n",
      "Epoch 25/60\n",
      "7528/7528 [==============================] - 1s 111us/step - loss: 1.3395e-04 - val_loss: 5.5554e-05\n",
      "Epoch 26/60\n",
      "7528/7528 [==============================] - 1s 145us/step - loss: 1.3396e-04 - val_loss: 5.5529e-05\n",
      "Epoch 27/60\n",
      "7528/7528 [==============================] - 1s 128us/step - loss: 1.3385e-04 - val_loss: 5.5457e-05\n",
      "Epoch 28/60\n",
      "7528/7528 [==============================] - 1s 122us/step - loss: 1.3387e-04 - val_loss: 5.5563e-05\n",
      "Epoch 29/60\n",
      "7528/7528 [==============================] - 1s 114us/step - loss: 1.3378e-04 - val_loss: 5.5477e-05\n",
      "Epoch 30/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3375e-04 - val_loss: 5.5431e-05\n",
      "Epoch 31/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3368e-04 - val_loss: 5.5445e-05\n",
      "Epoch 32/60\n",
      "7528/7528 [==============================] - 1s 107us/step - loss: 1.3367e-04 - val_loss: 5.5363e-05\n",
      "Epoch 33/60\n",
      "7528/7528 [==============================] - 1s 125us/step - loss: 1.3361e-04 - val_loss: 5.5347e-05\n",
      "Epoch 34/60\n",
      "7528/7528 [==============================] - 1s 119us/step - loss: 1.3359e-04 - val_loss: 5.5389e-05\n",
      "Epoch 35/60\n",
      "7528/7528 [==============================] - 1s 114us/step - loss: 1.3359e-04 - val_loss: 5.5680e-05\n",
      "Epoch 36/60\n",
      "7528/7528 [==============================] - 1s 127us/step - loss: 1.3354e-04 - val_loss: 5.5488e-05\n",
      "Epoch 37/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3351e-04 - val_loss: 5.5410e-05\n",
      "Epoch 38/60\n",
      "7528/7528 [==============================] - 1s 126us/step - loss: 1.3354e-04 - val_loss: 5.5400e-05\n",
      "Epoch 39/60\n",
      "7528/7528 [==============================] - 1s 146us/step - loss: 1.3342e-04 - val_loss: 5.5761e-05\n",
      "Epoch 40/60\n",
      "7528/7528 [==============================] - 1s 134us/step - loss: 1.3342e-04 - val_loss: 5.5283e-05\n",
      "Epoch 41/60\n",
      "7528/7528 [==============================] - 1s 134us/step - loss: 1.3340e-04 - val_loss: 5.5340e-05\n",
      "Epoch 42/60\n",
      "7528/7528 [==============================] - 1s 131us/step - loss: 1.3332e-04 - val_loss: 5.5261e-05\n",
      "Epoch 43/60\n",
      "7528/7528 [==============================] - 1s 132us/step - loss: 1.3337e-04 - val_loss: 5.5347e-05\n",
      "Epoch 44/60\n",
      "7528/7528 [==============================] - 1s 124us/step - loss: 1.3330e-04 - val_loss: 5.5301e-05\n",
      "Epoch 45/60\n",
      "7528/7528 [==============================] - 1s 111us/step - loss: 1.3327e-04 - val_loss: 5.5245e-05\n",
      "Epoch 46/60\n",
      "7528/7528 [==============================] - 1s 111us/step - loss: 1.3320e-04 - val_loss: 5.5468e-05\n",
      "Epoch 47/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3321e-04 - val_loss: 5.5228e-05\n",
      "Epoch 48/60\n",
      "7528/7528 [==============================] - 1s 110us/step - loss: 1.3318e-04 - val_loss: 5.5258e-05\n",
      "Epoch 49/60\n",
      "7528/7528 [==============================] - 1s 109us/step - loss: 1.3308e-04 - val_loss: 5.5868e-05\n",
      "Epoch 50/60\n",
      "7528/7528 [==============================] - 1s 120us/step - loss: 1.3312e-04 - val_loss: 5.5447e-05\n",
      "Epoch 51/60\n",
      "7528/7528 [==============================] - 1s 126us/step - loss: 1.3309e-04 - val_loss: 5.5233e-05\n",
      "Epoch 52/60\n",
      "7528/7528 [==============================] - 1s 111us/step - loss: 1.3309e-04 - val_loss: 5.5201e-05\n",
      "Epoch 53/60\n",
      "7528/7528 [==============================] - 1s 114us/step - loss: 1.3303e-04 - val_loss: 5.5232e-05\n",
      "Epoch 54/60\n",
      "7528/7528 [==============================] - 1s 139us/step - loss: 1.3306e-04 - val_loss: 5.5188e-05\n",
      "Epoch 55/60\n",
      "7528/7528 [==============================] - 1s 129us/step - loss: 1.3303e-04 - val_loss: 5.5250e-05\n",
      "Epoch 56/60\n",
      "7528/7528 [==============================] - 1s 152us/step - loss: 1.3298e-04 - val_loss: 5.5325e-05\n",
      "Epoch 57/60\n",
      "7528/7528 [==============================] - 1s 141us/step - loss: 1.3296e-04 - val_loss: 5.5234e-05\n",
      "Epoch 58/60\n",
      "7528/7528 [==============================] - 1s 134us/step - loss: 1.3291e-04 - val_loss: 5.5517e-05\n",
      "Epoch 59/60\n",
      "7528/7528 [==============================] - 1s 117us/step - loss: 1.3290e-04 - val_loss: 5.5346e-05\n",
      "Epoch 60/60\n",
      "7528/7528 [==============================] - 1s 176us/step - loss: 1.3288e-04 - val_loss: 5.5172e-05\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=5, validation_split = 0.2, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=60, validation_split = 0.2, verbose=1,\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.00055801  0.01599704  0.01041662]\n",
      " [ 0.00285213 -0.00148813  0.0112227   0.00706845]\n",
      " [ 0.01091266  0.00725445  0.02529763  0.01078872]\n",
      " [ 0.01977928  0.01500495  0.03205604  0.02889387]\n",
      " [ 0.03131202  0.02653769  0.05915174  0.05543156]\n",
      " [ 0.0710565   0.04972721  0.07310264  0.0538194 ]\n",
      " [ 0.0396825   0.03428819  0.06448409  0.06380204]\n",
      " [ 0.04352681  0.03906252  0.05313735  0.04265876]\n",
      " [ 0.05152528  0.03528027  0.05927578  0.04420886]\n",
      " [ 0.05102924  0.05022325  0.08804562  0.06764635]\n",
      " [ 0.05468745  0.0541915   0.10658484  0.10044641]\n",
      " [ 0.11024305  0.0824653   0.11049102  0.09424601]\n",
      " [ 0.08500739  0.04458087  0.09027776  0.05102924]\n",
      " [ 0.0557416   0.03354417  0.06752232  0.04420886]\n",
      " [ 0.04185268  0.00347221  0.04923117  0.01922127]\n",
      " [ 0.01773314  0.00824654  0.02628972  0.01630707]\n",
      " [ 0.02387157 -0.00241815  0.03168403  0.00551835]\n",
      " [ 0.00124006 -0.015687    0.00359624 -0.01196682]\n",
      " [-0.02368556 -0.03435025 -0.01705109 -0.03007196]\n",
      " [-0.02337552 -0.03918655 -0.02281751 -0.02926588]\n",
      " [-0.01444694 -0.02783982 -0.00589036 -0.02759175]\n",
      " [-0.01109876 -0.01550099  0.01159471  0.00124006]\n",
      " [-0.00062008 -0.0362103   0.00260416 -0.03025797]\n",
      " [-0.03478423 -0.03490826 -0.00303823 -0.00682047]\n",
      " [-0.02752978 -0.06591025 -0.02628972 -0.06293399]\n",
      " [-0.0685144  -0.07459077 -0.05102924 -0.07335071]\n",
      " [-0.05016119 -0.05078127 -0.03881445 -0.04402286]\n",
      " [-0.0425967  -0.04947914 -0.03435025 -0.0416047 ]\n",
      " [-0.04600693 -0.04600693 -0.01357888 -0.0207093 ]\n",
      " [-0.01339288 -0.01593507  0.00062008 -0.00446429]\n",
      " [-0.00998264 -0.01047869  0.00322423 -0.00099209]\n",
      " [ 0.01283477  0.00272819  0.01357888  0.01357888]\n",
      " [ 0.00564238 -0.00427829  0.01271083 -0.00043407]\n",
      " [ 0.01314481 -0.00062008  0.02573161  0.01804318]\n",
      " [-0.0048363  -0.0348462  -0.00161207 -0.03360614]\n",
      " [-0.03856648 -0.04358878 -0.02790179 -0.03205604]\n",
      " [-0.04222469 -0.04501485 -0.01351691 -0.02244541]\n",
      " [-0.02430554 -0.04600693 -0.01841519 -0.04241069]\n",
      " [-0.03218007 -0.05084324 -0.02926588 -0.03614833]\n",
      " [-0.03131202 -0.05803572 -0.03038191 -0.05605164]\n",
      " [-0.0634921  -0.07421876 -0.05505955 -0.0716146 ]\n",
      " [-0.08172128 -0.08692959 -0.07657494 -0.08358132]\n",
      " [-0.08866569 -0.08866569 -0.06566218 -0.06820437]\n",
      " [-0.0555556  -0.06144596 -0.04613097 -0.04768107]\n",
      " [-0.0476191  -0.05195936 -0.03596232 -0.05158735]\n",
      " [-0.05022325 -0.0698785  -0.04303077 -0.06355407]\n",
      " [-0.06684027 -0.06795639 -0.04650298 -0.05450154]\n",
      " [-0.05295144 -0.06181796 -0.04501485 -0.04551089]\n",
      " [-0.02876984 -0.02951386 -0.00452626 -0.00911459]\n",
      " [-0.01209075 -0.05319942  0.00737849 -0.0491692 ]\n",
      " [-0.03335817 -0.05381949 -0.01829115 -0.02430554]\n",
      " [-0.06063987 -0.06696431 -0.03645836 -0.04110866]\n",
      " [-0.0251736  -0.07490081 -0.02331346 -0.07310273]\n",
      " [-0.07434279 -0.07967514 -0.04675105 -0.05803572]\n",
      " [-0.07831104 -0.08661955 -0.07012647 -0.07924106]\n",
      " [-0.07279269 -0.09170392 -0.06888641 -0.0824653 ]\n",
      " [-0.07378468 -0.09548607 -0.06919645 -0.09511407]\n",
      " [-0.09926832 -0.10528271 -0.08798365 -0.10416669]\n",
      " [-0.09883434 -0.10150047 -0.07700892 -0.07899309]\n",
      " [-0.06777039 -0.07911703 -0.06374007 -0.07707098]\n",
      " [-0.08085323 -0.0842014  -0.05983379 -0.07992311]\n",
      " [-0.05158735 -0.07514878 -0.04048858 -0.0603919 ]\n",
      " [-0.06479413 -0.10751486 -0.06336806 -0.10683281]\n",
      " [-0.11377732 -0.12239587 -0.10832094 -0.11098707]]\n",
      "[0.9963]\n",
      "[1.0002998]\n"
     ]
    }
   ],
   "source": [
    "print test_data[0]\n",
    "print test_labels[0]\n",
    "print outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primitive policy temporarily in place of a RL policy network\n",
    "\n",
    "# signal step for our policy network\n",
    "signals = map(lambda arr: sum(arr) / len(arr),outputs)\n",
    "# signals = map(lambda arr: arr[0],outputs)\n",
    "trades = map(lambda signal: 1 if round(signal,4) > 1 else 0,signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one day return\n",
    "actual = map(lambda arr: arr[0],test_labels)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'signal':signals,\n",
    "    'actual': actual,\n",
    "    'trade':trades,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['entry_success'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['entry_failure'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 1) else 0,axis=1)\n",
    "df['avoid_success'] = df.apply (lambda row: 1 if (row['actual'] < 1.00 and row['trade'] == 0) else 0,axis=1)\n",
    "df['avoid_failure'] = df.apply (lambda row: 1 if (row['actual'] > 1.00 and row['trade'] == 0) else 0,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>signal</th>\n",
       "      <th>trade</th>\n",
       "      <th>entry_success</th>\n",
       "      <th>entry_failure</th>\n",
       "      <th>avoid_success</th>\n",
       "      <th>avoid_failure</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9963</td>\n",
       "      <td>1.000300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0029</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0081</td>\n",
       "      <td>1.000543</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0090</td>\n",
       "      <td>1.000109</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0119</td>\n",
       "      <td>1.000945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0428</td>\n",
       "      <td>1.001056</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0040</td>\n",
       "      <td>1.000060</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0084</td>\n",
       "      <td>1.002193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9995</td>\n",
       "      <td>1.002752</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0039</td>\n",
       "      <td>1.003264</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0624</td>\n",
       "      <td>1.002841</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.9724</td>\n",
       "      <td>1.000846</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.9690</td>\n",
       "      <td>1.000007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.000537</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9754</td>\n",
       "      <td>1.000058</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0063</td>\n",
       "      <td>1.001048</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9773</td>\n",
       "      <td>1.000672</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0003</td>\n",
       "      <td>1.000829</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0088</td>\n",
       "      <td>1.000889</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0033</td>\n",
       "      <td>1.001193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0105</td>\n",
       "      <td>1.001825</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.9670</td>\n",
       "      <td>1.000118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0071</td>\n",
       "      <td>1.000748</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9616</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0175</td>\n",
       "      <td>0.999433</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0073</td>\n",
       "      <td>1.000345</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.9967</td>\n",
       "      <td>1.000108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0322</td>\n",
       "      <td>1.000798</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.999566</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>1.0127</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>0.9977</td>\n",
       "      <td>1.000035</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.999437</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>1.0041</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>1.0011</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>0.9958</td>\n",
       "      <td>1.000131</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>0.9959</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>1.0063</td>\n",
       "      <td>0.999650</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>0.9966</td>\n",
       "      <td>1.000178</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.999628</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>1.0028</td>\n",
       "      <td>1.000190</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>0.9999</td>\n",
       "      <td>1.000216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>1.0060</td>\n",
       "      <td>1.000659</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>0.9993</td>\n",
       "      <td>1.000302</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>1.0001</td>\n",
       "      <td>0.999686</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>1.0016</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>1.0041</td>\n",
       "      <td>0.999468</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1.0096</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>1.0006</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>1.0107</td>\n",
       "      <td>1.000013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>0.9955</td>\n",
       "      <td>1.000091</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>0.9972</td>\n",
       "      <td>1.000549</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>1.0101</td>\n",
       "      <td>1.000392</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>0.9962</td>\n",
       "      <td>1.000853</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>1.0145</td>\n",
       "      <td>1.000028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>1.0124</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0.9874</td>\n",
       "      <td>0.999191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1990 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual    signal  trade  entry_success  entry_failure  avoid_success  \\\n",
       "0     0.9963  1.000300      1              0              1              0   \n",
       "1     1.0029  0.999719      0              0              0              0   \n",
       "2     1.0081  1.000543      1              1              0              0   \n",
       "3     1.0090  1.000109      1              1              0              0   \n",
       "4     1.0119  1.000945      1              1              0              0   \n",
       "5     1.0428  1.001056      1              1              0              0   \n",
       "6     0.9673  0.999887      0              0              0              1   \n",
       "7     1.0040  1.000060      1              1              0              0   \n",
       "8     1.0084  1.002193      1              1              0              0   \n",
       "9     0.9995  1.002752      1              0              1              0   \n",
       "10    1.0039  1.003264      1              1              0              0   \n",
       "11    1.0624  1.002841      1              1              0              0   \n",
       "12    0.9724  1.000846      1              0              1              0   \n",
       "13    0.9690  1.000007      0              0              0              1   \n",
       "14    0.9855  1.000537      1              0              1              0   \n",
       "15    0.9754  1.000058      1              0              1              0   \n",
       "16    1.0063  1.001048      1              1              0              0   \n",
       "17    0.9773  1.000672      1              0              1              0   \n",
       "18    0.9757  0.999948      0              0              0              1   \n",
       "19    1.0003  1.000829      1              1              0              0   \n",
       "20    1.0088  1.000889      1              1              0              0   \n",
       "21    1.0033  1.001193      1              1              0              0   \n",
       "22    1.0105  1.001825      1              1              0              0   \n",
       "23    0.9670  1.000118      1              0              1              0   \n",
       "24    1.0071  1.000748      1              1              0              0   \n",
       "25    0.9616  0.999520      0              0              0              1   \n",
       "26    1.0175  0.999433      0              0              0              0   \n",
       "27    1.0073  1.000345      1              1              0              0   \n",
       "28    0.9967  1.000108      1              0              1              0   \n",
       "29    1.0322  1.000798      1              1              0              0   \n",
       "...      ...       ...    ...            ...            ...            ...   \n",
       "1960  0.9939  0.999566      0              0              0              1   \n",
       "1961  1.0127  0.999735      0              0              0              0   \n",
       "1962  0.9977  1.000035      0              0              0              1   \n",
       "1963  0.9838  0.999437      0              0              0              1   \n",
       "1964  1.0041  0.999341      0              0              0              0   \n",
       "1965  0.9859  0.999784      0              0              0              1   \n",
       "1966  1.0011  0.999705      0              0              0              0   \n",
       "1967  0.9958  1.000131      1              0              1              0   \n",
       "1968  0.9959  0.999816      0              0              0              1   \n",
       "1969  1.0063  0.999650      0              0              0              0   \n",
       "1970  0.9966  1.000178      1              0              1              0   \n",
       "1971  0.9988  0.999628      0              0              0              1   \n",
       "1972  0.9992  0.999916      0              0              0              1   \n",
       "1973  1.0028  1.000190      1              1              0              0   \n",
       "1974  0.9999  1.000216      1              0              1              0   \n",
       "1975  1.0060  1.000659      1              1              0              0   \n",
       "1976  0.9993  1.000302      1              0              1              0   \n",
       "1977  1.0001  0.999686      0              0              0              0   \n",
       "1978  1.0016  0.999521      0              0              0              0   \n",
       "1979  1.0041  0.999468      0              0              0              0   \n",
       "1980  1.0096  0.999869      0              0              0              0   \n",
       "1981  1.0006  0.999778      0              0              0              0   \n",
       "1982  1.0107  1.000013      0              0              0              0   \n",
       "1983  0.9955  1.000091      1              0              1              0   \n",
       "1984  0.9972  1.000549      1              0              1              0   \n",
       "1985  1.0101  1.000392      1              1              0              0   \n",
       "1986  0.9962  1.000853      1              0              1              0   \n",
       "1987  1.0145  1.000028      0              0              0              0   \n",
       "1988  1.0124  0.999384      0              0              0              0   \n",
       "1989  0.9874  0.999191      0              0              0              1   \n",
       "\n",
       "      avoid_failure  success  \n",
       "0                 0        0  \n",
       "1                 1        1  \n",
       "2                 0        1  \n",
       "3                 0        1  \n",
       "4                 0        1  \n",
       "5                 0        1  \n",
       "6                 0        1  \n",
       "7                 0        1  \n",
       "8                 0        1  \n",
       "9                 0        0  \n",
       "10                0        1  \n",
       "11                0        1  \n",
       "12                0        0  \n",
       "13                0        1  \n",
       "14                0        0  \n",
       "15                0        0  \n",
       "16                0        1  \n",
       "17                0        0  \n",
       "18                0        1  \n",
       "19                0        1  \n",
       "20                0        1  \n",
       "21                0        1  \n",
       "22                0        1  \n",
       "23                0        0  \n",
       "24                0        1  \n",
       "25                0        1  \n",
       "26                1        1  \n",
       "27                0        1  \n",
       "28                0        0  \n",
       "29                0        1  \n",
       "...             ...      ...  \n",
       "1960              0        1  \n",
       "1961              1        1  \n",
       "1962              0        1  \n",
       "1963              0        1  \n",
       "1964              1        1  \n",
       "1965              0        1  \n",
       "1966              1        1  \n",
       "1967              0        0  \n",
       "1968              0        1  \n",
       "1969              1        1  \n",
       "1970              0        0  \n",
       "1971              0        1  \n",
       "1972              0        1  \n",
       "1973              0        1  \n",
       "1974              0        0  \n",
       "1975              0        1  \n",
       "1976              0        0  \n",
       "1977              1        1  \n",
       "1978              1        1  \n",
       "1979              1        1  \n",
       "1980              1        1  \n",
       "1981              1        1  \n",
       "1982              1        1  \n",
       "1983              0        0  \n",
       "1984              0        0  \n",
       "1985              0        1  \n",
       "1986              0        0  \n",
       "1987              1        1  \n",
       "1988              1        1  \n",
       "1989              0        1  \n",
       "\n",
       "[1990 rows x 8 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primitive policy - replace with a policy network which maximizes reward\n",
    "def label_success (row):\n",
    "    return 0 if (row['entry_failure'] == 1 or row['entry_failure'] == 1) else 1\n",
    "\n",
    "success = df.apply (lambda row: label_success (row),axis=1)\n",
    "df['success'] = success;\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009140122235402\n",
      "0.004438019357029727\n",
      "0.4471830274316815\n",
      "0.4648115898011365\n"
     ]
    }
   ],
   "source": [
    "print df['actual'].corr(df['signal'])\n",
    "print df['actual'].corr(df['trade'])\n",
    "print df['actual'].corr(df['entry_success'])\n",
    "print df['actual'].corr(df['success'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1990.000000\n",
       "mean        1.000404\n",
       "std         0.009946\n",
       "min         0.954200\n",
       "25%         0.996600\n",
       "50%         1.000600\n",
       "75%         1.005175\n",
       "max         1.062400\n",
       "Name: actual, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['actual'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1990.000000\n",
       "mean        1.000470\n",
       "std         0.000890\n",
       "min         0.997219\n",
       "25%         0.999882\n",
       "50%         1.000428\n",
       "75%         1.001083\n",
       "max         1.003569\n",
       "Name: signal, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['signal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n",
      "\n",
      "Precision\n",
      "0.549925484352\n",
      "\n",
      "Recall\n",
      "0.686511627907\n",
      "\n",
      "Accuracy\n",
      "0.370854271357\n",
      "\n",
      "Non-loss events\n",
      "1386\n",
      "0.69648241206\n",
      "\n",
      "Lose trades\n",
      "604\n",
      "0.30351758794\n",
      "\n",
      "Win trades\n",
      "738\n",
      "0.370854271357\n",
      "\n",
      "Missed opportunities\n",
      "337\n",
      "0.169346733668\n",
      "\n",
      "Bullets dodged\n",
      "294\n",
      "0.147738693467\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST_SAMPLES = len(test_data)\n",
    "print NUM_TEST_SAMPLES\n",
    "\n",
    "print '\\nPrecision' # optimize for this since we can increase discovery, so long as we find enough trades\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['entry_failure'])) \n",
    "\n",
    "print '\\nRecall'\n",
    "print sum(df['entry_success']) * 1.00 / (sum(df['entry_success']) + sum(df['avoid_failure']))\n",
    "\n",
    "print '\\nAccuracy'\n",
    "print sum(df['entry_success']) * 1.00 / (NUM_TEST_SAMPLES)\n",
    "\n",
    "print '\\nNon-loss events'\n",
    "print sum(df['success'])\n",
    "print sum(df['success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nLose trades'\n",
    "print sum(df['entry_failure'])\n",
    "print sum(df['entry_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nWin trades'\n",
    "print sum(df['entry_success'])\n",
    "print sum(df['entry_success']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nMissed opportunities'\n",
    "print sum(df['avoid_failure'])\n",
    "print sum(df['avoid_failure']) / (NUM_TEST_SAMPLES * 1.00)\n",
    "\n",
    "print '\\nBullets dodged'\n",
    "print sum(df['avoid_success'])\n",
    "print sum(df['avoid_success']) / (NUM_TEST_SAMPLES * 1.00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        , -0.00071011,  0.00426743, -0.00071011],\n",
       "        [-0.00711238, -0.00711238, -0.00071011, -0.00071011],\n",
       "        [-0.00924496, -0.00995733, -0.00426743, -0.00640001],\n",
       "        [-0.01991465, -0.02062476, -0.00995733, -0.01066743],\n",
       "        [-0.02418208, -0.02631466, -0.01208991, -0.02346971],\n",
       "        [-0.02346971, -0.02560455, -0.01777981, -0.02346971],\n",
       "        [-0.02346971, -0.02702703, -0.02204724, -0.02346971],\n",
       "        [-0.01635734, -0.01991465, -0.01422475, -0.01991465],\n",
       "        [-0.01777981, -0.01849218, -0.01351238, -0.01635734],\n",
       "        [-0.0227596 , -0.02702703, -0.01920229, -0.01920229],\n",
       "        [-0.01493486, -0.02133713, -0.01493486, -0.02133713],\n",
       "        [ 0.01066969, -0.01208991,  0.01209217, -0.01208991],\n",
       "        [ 0.0113798 ,  0.00924722,  0.01493712,  0.01066969],\n",
       "        [ 0.01209217,  0.        ,  0.02560455,  0.        ],\n",
       "        [ 0.00853485,  0.00853485,  0.01351464,  0.01209217],\n",
       "        [ 0.00497979,  0.00355732,  0.00853485,  0.0056899 ],\n",
       "        [ 0.0056899 ,  0.00142248,  0.01066969,  0.00213484],\n",
       "        [-0.00711238, -0.00711238,  0.00497979,  0.00497979],\n",
       "        [-0.00924496, -0.00995733, -0.00426743, -0.00640001],\n",
       "        [-0.01066743, -0.0113798 , -0.0056899 , -0.0113798 ],\n",
       "        [-0.00782248, -0.01422475, -0.00640001, -0.01422475],\n",
       "        [-0.0227596 , -0.0227596 , -0.00711238, -0.00853485],\n",
       "        [-0.02702703, -0.02773714, -0.0227596 , -0.02418208],\n",
       "        [-0.02133713, -0.0284495 , -0.02133713, -0.0284495 ],\n",
       "        [-0.01849218, -0.02702703, -0.01777981, -0.0227596 ],\n",
       "        [-0.04125178, -0.04125178, -0.02062476, -0.02062476],\n",
       "        [-0.03769446, -0.0398293 , -0.03556188, -0.03911694],\n",
       "        [-0.0398293 , -0.0398293 , -0.03342704, -0.03769446],\n",
       "        [-0.03698435, -0.04338436, -0.03556188, -0.04053941],\n",
       "        [-0.02631466, -0.02915961, -0.01991465, -0.0284495 ],\n",
       "        [-0.03129445, -0.03129445, -0.02560455, -0.02560455],\n",
       "        [-0.03129445, -0.0341394 , -0.03129445, -0.03129445],\n",
       "        [-0.02489219, -0.02987198, -0.02346971, -0.02987198],\n",
       "        [-0.03129445, -0.03556188, -0.02915961, -0.02915961],\n",
       "        [-0.02489219, -0.03058209, -0.02489219, -0.03058209],\n",
       "        [-0.01920229, -0.02133713, -0.01422475, -0.01493486],\n",
       "        [-0.02133713, -0.0227596 , -0.01991465, -0.02204724],\n",
       "        [-0.02133713, -0.02560455, -0.01493486, -0.01991465],\n",
       "        [-0.02773714, -0.02987198, -0.02062476, -0.02204724],\n",
       "        [-0.02204724, -0.02773714, -0.02133713, -0.02702703],\n",
       "        [-0.02631466, -0.03129445, -0.0227596 , -0.0227596 ],\n",
       "        [-0.02915961, -0.02915961, -0.02631466, -0.02773714],\n",
       "        [-0.0284495 , -0.03484951, -0.0284495 , -0.03200456],\n",
       "        [-0.02489219, -0.02987198, -0.0227596 , -0.02987198],\n",
       "        [-0.00355506, -0.01635734, -0.00355506, -0.01635734],\n",
       "        [-0.00853485, -0.0113798 , -0.0056899 , -0.0113798 ],\n",
       "        [-0.0056899 , -0.01066743, -0.00284495, -0.01066743],\n",
       "        [-0.00924496, -0.00924496, -0.00497753, -0.00711238],\n",
       "        [-0.00782248, -0.01351238, -0.00355506, -0.01351238],\n",
       "        [-0.02204724, -0.0227596 , -0.01635734, -0.01777981],\n",
       "        [-0.02418208, -0.02773714, -0.02062476, -0.02133713],\n",
       "        [-0.0227596 , -0.02560455, -0.02204724, -0.02489219],\n",
       "        [-0.0227596 , -0.02489219, -0.01849218, -0.02204724],\n",
       "        [-0.0227596 , -0.02489219, -0.02133713, -0.02346971],\n",
       "        [-0.01849218, -0.02560455, -0.01777981, -0.0227596 ],\n",
       "        [-0.01351238, -0.01849218, -0.00711238, -0.0170697 ],\n",
       "        [-0.01280228, -0.01564723, -0.00995733, -0.01564723],\n",
       "        [ 0.        , -0.0170697 ,  0.        , -0.00853485],\n",
       "        [ 0.00426743, -0.00071011,  0.0056899 ,  0.00213484],\n",
       "        [ 0.01209217,  0.        ,  0.01493712,  0.00355732],\n",
       "        [ 0.00142248,  0.00142248,  0.01351464,  0.01351464],\n",
       "        [ 0.00355732,  0.00071237,  0.00497979,  0.00284495],\n",
       "        [-0.00071011, -0.00071011,  0.00711238,  0.00142248],\n",
       "        [-0.00213258, -0.00782248, -0.00213258, -0.00426743]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_data = np.array(convert_to_train(SPY.copy(), 0)[0][:1])\n",
    "today_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9994432]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_prediction = model.predict(today_data)\n",
    "future_prediction # [1 day prediction, 5 day prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
