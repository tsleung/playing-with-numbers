{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest multiple models\n",
    "Loads valuation model(s) and applies error handling to rank sectors and trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "import itertools\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_INPUT_NEURONS = 64\n",
    "NUM_OUTPUT_NEURONS = 1\n",
    "DAY_OFFSET = 5\n",
    "\n",
    "def from_network(symbol):\n",
    "    return pdr.get_data_yahoo(symbols=symbol, start=datetime(1900, 1, 1)).sort_values(by=['Date'],ascending=False)\n",
    "\n",
    "# Create features (only close price for now)\n",
    "def convert_to_percentage(old, new):\n",
    "    return (old - new) / old\n",
    "\n",
    "def convert_labels_to_category(labels): \n",
    "    # Simplification - If positive return, 1, else 0\n",
    "    # return map(lambda arr: 1 if arr[0] > 1 else 0, labels)\n",
    "    # rounding simpliciation\n",
    "    return map(lambda arr: map(lambda val: round(val,4),arr), labels)\n",
    "\n",
    "def convert_to_train(raw_dataset, offset=5):\n",
    "    dataset = raw_dataset.copy()\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(offset, len(dataset) - NUM_INPUT_NEURONS):\n",
    "\n",
    "        feature_dataset = dataset[i:i+NUM_INPUT_NEURONS].copy()\n",
    "        latest_close = feature_dataset['Close'].iloc[0]\n",
    "        \n",
    "        features.append(\n",
    "            feature_dataset['Close']\n",
    "                .map(lambda current: convert_to_percentage(latest_close, current))\n",
    "                .tolist()\n",
    "        )\n",
    "        labels.append([\n",
    "            dataset['Close'].iloc[i-1] / latest_close, # 1 day trade\n",
    "        ])\n",
    "        \n",
    "    # Without converting labels the precision is hard to determine accuracy. \n",
    "    # Rather than crude 0/1, maybe this can be more sophisticated\n",
    "    labels = convert_labels_to_category(labels)\n",
    "    \n",
    "    return [features,labels]\n",
    "\n",
    "def split_data(symbol):\n",
    "    fetched = from_network(symbol)\n",
    "    converted = convert_to_train(fetched)\n",
    "    features = converted[0]\n",
    "    labels = converted[1]\n",
    "    training = [ # since we decide a model, use all data for training\n",
    "        features[1:],\n",
    "        labels[1:]\n",
    "    ]\n",
    "    validation = [\n",
    "        features[5:1000],\n",
    "        labels[5:1000]\n",
    "    ]\n",
    "    prediction = [\n",
    "        features[:5],\n",
    "        labels[:5],\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'prediction': prediction,\n",
    "        'validation': validation,\n",
    "        'training': training,\n",
    "    }\n",
    "\n",
    "    \n",
    "def combine_all(accum, prep):\n",
    "    return {   \n",
    "        'prediction':[\n",
    "            accum['prediction'][0] + prep['prediction'][0],\n",
    "            accum['prediction'][1] + prep['prediction'][1],\n",
    "        ],\n",
    "        'validation':[\n",
    "            accum['validation'][0] + prep['validation'][0],\n",
    "            accum['validation'][1] + prep['validation'][1],\n",
    "        ],\n",
    "        'training':[\n",
    "            accum['training'][0] + prep['training'][0],\n",
    "            accum['training'][1] + prep['training'][1],\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "    error_model = load_model('sector_model_error.h5')\n",
    "    model = load_model('sector_model.h5')\n",
    "    model_0 = load_model('sector_model_0.h5')\n",
    "    model_1 = load_model('sector_model_1.h5')\n",
    "    model_2 = load_model('sector_model_2.h5')\n",
    "    model_3 = load_model('sector_model_3.h5')\n",
    "    model_4 = load_model('sector_model_4.h5')\n",
    "    model_5 = load_model('sector_model_5.h5')\n",
    "    model_6 = load_model('sector_model_6.h5')\n",
    "    model_7 = load_model('sector_model_7.h5')\n",
    "    model_8 = load_model('sector_model_8.h5')\n",
    "    model_9 = load_model('sector_model_9.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QQQ = from_network('QQQ')\n",
    "SPY = from_network('SPY')\n",
    "XLK = from_network('XLK')\n",
    "XLF = from_network('XLF')\n",
    "XLE = from_network('XLE')\n",
    "XLP = from_network('XLP')\n",
    "XLV = from_network('XLV')\n",
    "XLY = from_network('XLY')\n",
    "XLI = from_network('XLI')\n",
    "XLU = from_network('XLU')\n",
    "DIA = from_network('DIA')\n",
    "IWM = from_network('IWM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "securities_to_predict = [\n",
    "    ['SPY', np.array(convert_to_train(SPY.copy(), 0)[0][:1])],\n",
    "#     ['QQQ', np.array(convert_to_train(QQQ.copy(), 0)[0][:1])],\n",
    "    ['XLE', np.array(convert_to_train(XLE.copy(), 0)[0][:1])],\n",
    "    ['XLF', np.array(convert_to_train(XLF.copy(), 0)[0][:1])],\n",
    "    ['XLK', np.array(convert_to_train(XLK.copy(), 0)[0][:1])],\n",
    "    ['XLP', np.array(convert_to_train(XLP.copy(), 0)[0][:1])],\n",
    "    ['XLV', np.array(convert_to_train(XLV.copy(), 0)[0][:1])],\n",
    "    ['XLU', np.array(convert_to_train(XLU.copy(), 0)[0][:1])],\n",
    "    ['XLY', np.array(convert_to_train(XLY.copy(), 0)[0][:1])],\n",
    "    ['XLI', np.array(convert_to_train(XLI.copy(), 0)[0][:1])],\n",
    "#     ['IWM', np.array(convert_to_train(IWM.copy(), 0)[0][:1])],\n",
    "#     ['DIA', np.array(convert_to_train(DIA.copy(), 0)[0][:1])],\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XLK', 'XLE', 'XLP', 'XLY', 'SPY', 'XLV', 'XLF', 'XLI', 'XLU']\n",
      "['XLK', 'XLI', 'XLF', 'XLY', 'SPY', 'XLU', 'XLE', 'XLV', 'XLP']\n",
      "['XLE', 'XLK', 'XLU', 'XLP', 'XLI', 'XLV', 'SPY', 'XLF', 'XLY']\n",
      "['XLI', 'XLY', 'XLV', 'XLE', 'XLP', 'SPY', 'XLU', 'XLK', 'XLF']\n",
      "['XLE', 'XLY', 'XLK', 'XLI', 'XLU', 'XLV', 'SPY', 'XLP', 'XLF']\n",
      "['XLU', 'XLP', 'XLV', 'XLI', 'SPY', 'XLF', 'XLK', 'XLY', 'XLE']\n",
      "['XLE', 'XLK', 'XLP', 'XLF', 'XLV', 'XLU', 'XLY', 'XLI', 'SPY']\n",
      "['XLY', 'XLP', 'XLI', 'SPY', 'XLU', 'XLF', 'XLV', 'XLK', 'XLE']\n",
      "['SPY', 'XLI', 'XLF', 'XLK', 'XLY', 'XLV', 'XLP', 'XLE', 'XLU']\n",
      "['XLE', 'XLP', 'XLK', 'XLV', 'XLY', 'SPY', 'XLU', 'XLF', 'XLI']\n",
      "['XLK', 'XLI', 'XLF', 'XLY', 'SPY', 'XLP', 'XLU', 'XLE', 'XLV']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(generate_model_predictions,[model, model_1, model_2, model_3, model_4,model_5,model_6,model_7,model_8,model_9,model_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPY_TRAIN = convert_to_train(SPY.copy(), 0)\n",
    "XLE_TRAIN = convert_to_train(XLE.copy(), 0)\n",
    "XLF_TRAIN = convert_to_train(XLF.copy(), 0)\n",
    "XLK_TRAIN = convert_to_train(XLK.copy(), 0)\n",
    "XLP_TRAIN = convert_to_train(XLP.copy(), 0)\n",
    "XLV_TRAIN = convert_to_train(XLV.copy(), 0)\n",
    "XLU_TRAIN = convert_to_train(XLU.copy(), 0)\n",
    "XLY_TRAIN = convert_to_train(XLY.copy(), 0)\n",
    "XLI_TRAIN = convert_to_train(XLI.copy(), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Perhaps creating a policy which takes the aggregate score 0-1 to create a normalized ranking for an item and trading it as a softmax of the portfolio?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Account 0.9965166666666667\n"
     ]
    }
   ],
   "source": [
    "account = 1\n",
    "for x, y in zip(range(0,60), itertools.count()):\n",
    "    securities_to_predict = [\n",
    "#         ['SPY', np.array(SPY_TRAIN[0][x:x+1]),np.array(SPY_TRAIN[1][x:x+1])],\n",
    "        ['XLE', np.array(XLE_TRAIN[0][x:x+1]),np.array(XLE_TRAIN[1][x:x+1])],\n",
    "        ['XLF', np.array(XLF_TRAIN[0][x:x+1]),np.array(XLF_TRAIN[1][x:x+1])],\n",
    "        ['XLK', np.array(XLK_TRAIN[0][x:x+1]),np.array(XLK_TRAIN[1][x:x+1])],\n",
    "        ['XLP', np.array(XLP_TRAIN[0][x:x+1]),np.array(XLP_TRAIN[1][x:x+1])],\n",
    "        ['XLV', np.array(XLV_TRAIN[0][x:x+1]),np.array(XLV_TRAIN[1][x:x+1])],\n",
    "        ['XLU', np.array(XLU_TRAIN[0][x:x+1]),np.array(XLU_TRAIN[1][x:x+1])],\n",
    "        ['XLY', np.array(XLY_TRAIN[0][x:x+1]),np.array(XLY_TRAIN[1][x:x+1])],\n",
    "        ['XLI', np.array(XLI_TRAIN[0][x:x+1]),np.array(XLI_TRAIN[1][x:x+1])],\n",
    "    ]\n",
    "\n",
    "    def print_arg(arr):\n",
    "        print '{} {}'.format(arr[0],str(arr[1]))\n",
    "\n",
    "\n",
    "    def predict_and_correct(model, inputs):\n",
    "        prediction = model.predict(inputs)\n",
    "        return prediction\n",
    "\n",
    "    def generate_model_predictions(model):\n",
    "        predictions = map(lambda arr: [arr[0], predict_and_correct(model,arr[1])], securities_to_predict)\n",
    "        predictions.sort(key=lambda x: x[1],reverse=True)\n",
    "#         print map(print_arg, predictions)\n",
    "#         print map(lambda arr: arr[0],predictions)\n",
    "        return predictions\n",
    "    \n",
    "    all_model_predictions = map(generate_model_predictions,[model, model_1, model_2, model_3, model_4,model_5,model_6,model_7,model_8,model_9,model_0])\n",
    "\n",
    "    def ranking_for_symbol(symbol):\n",
    "        return map(lambda predictions: map(lambda arr: arr[0],predictions).index(symbol),all_model_predictions)\n",
    "\n",
    "    securities_to_predict.sort(key=lambda x: np.mean(ranking_for_symbol(x[0])))\n",
    "\n",
    "#     print 'result ranking'\n",
    "#     print map(lambda x: x[0],securities_to_predict)\n",
    "#     for security in securities_to_predict:\n",
    "#         print security[0],security[2]\n",
    "    buy = map(lambda x: x[2],securities_to_predict[:3]) # long\n",
    "    sell = map(lambda x: x[2],securities_to_predict[5:]) # short\n",
    "#     print 'Long',np.mean(buy)\n",
    "#     print 'Short',1+(1-np.mean(sell))\n",
    "    change = (np.mean(buy) + 1+(1-np.mean(sell))) / 2\n",
    "#     print 'Change', change\n",
    "    account = 1 * change\n",
    "#     print 'Account', account\n",
    "print 'End Account', account\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
